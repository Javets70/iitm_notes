{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 : Write a function compute_GridSearchCV which accepts the Kernel and regularization parameters as inputs and returns the Mean cross-validated score of the best_estimator, denoted with best_score_ of the models with the below-mentioned hyperparameters:\n",
    "Split the Iris dataset into train and test sets with 70:30 ratio \\\n",
    "Import svm.SVC as 'model' \\\n",
    "kernels = ['linear' , 'rbf'] \\\n",
    "Regularization = [1,15,25] \\\n",
    "gamma = 'auto' \\\n",
    "Cross Validation = 4 \\\n",
    "random_state=0 \\\n",
    "Note: Mark the closest option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_328352/3332160253.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing  import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9807692307692308"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X , y = load_iris(return_X_y=True, as_frame=True)\n",
    "params = {\n",
    "    'kernel' : ['linear' , 'rbf'] ,\n",
    "    'C' : [1,15,25] ,\n",
    "    'gamma' : ['auto'] ,\n",
    "\n",
    "}\n",
    "def compute_GridSearchCV(X,y, params):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "    grid_search = GridSearchCV(SVC(random_state=0),param_grid=params, cv=4)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    return grid_search.best_score_\n",
    "\n",
    "compute_GridSearchCV(X,y , params )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Read the instructions given below to answer the two questions given below.\n",
    "\n",
    "Split the Social_Network_Ads dataset \\\n",
    "(https://drive.google.com/file/d/1qUa1GlG4X4ZY_4E0e7jPR-z7AG7NIDbE/view?usp=sharing) into training and test set in 75:25 ratio. \\\n",
    "Fit and transform the train and test set of the feature matrix by applying StandardScaler transformer. \\\n",
    "Fit a linear SVM (with random_state = 0and linear kernel) to training data. \\\n",
    "\n",
    "\n",
    "#### Question 2 : The predicted data returns an accuracy_score on test data. Which of the following option represents the calculated accuracy_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Social_Network_Ads.csv')\n",
    "X,  y = data.iloc[:, :-1], data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "stdslr = StandardScaler()\n",
    "X_train = stdslr.fit_transform(X_train)\n",
    "X_test = stdslr.transform(X_test)\n",
    "\n",
    "\n",
    "svc = SVC(kernel='linear', random_state=0)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "accuracy_score(y_test, svc.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 : Calculate the confusion matrix obtained from the above-predicted data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[66,  2],\n",
       "       [ 8, 24]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, svc.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  From the MNIST dataset, consider the first 20,000 data samples as training data and the next 5,000 data samples as test data. Fit a pipeline with MinMaxScaler and a classifier with SVC, linear kernel, one vs rest decision_function_shape and class_weight=None to this dataset and answer the following questions(Q.no 4 and Q.no 5).\n",
    "##### Question 4: What is the sum of the main diagonal elements of the confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X ,y =  fetch_openml('mnist_784',return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:20000]\n",
    "X_test = X[20000:25000]\n",
    "y_train = y[:20000]\n",
    "y_test = y[20000:25000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4623"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(MinMaxScaler(), SVC(kernel='linear',class_weight=None,decision_function_shape='ovr' ))\n",
    "pipe.fit(X_train, y_train)\n",
    "np.trace(confusion_matrix(y_test, pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       478\n",
      "           1       0.95      0.98      0.97       568\n",
      "           2       0.92      0.92      0.92       521\n",
      "           3       0.91      0.90      0.90       516\n",
      "           4       0.91      0.94      0.92       500\n",
      "           5       0.88      0.88      0.88       460\n",
      "           6       0.97      0.96      0.96       491\n",
      "           7       0.91      0.96      0.94       504\n",
      "           8       0.93      0.85      0.89       466\n",
      "           9       0.92      0.88      0.90       496\n",
      "\n",
      "    accuracy                           0.92      5000\n",
      "   macro avg       0.92      0.92      0.92      5000\n",
      "weighted avg       0.92      0.92      0.92      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 6 : Consider the MNIST dataset, split it into training and test set in 50:50 ratio with random_state = 42. Fit a SVM model using pipeline with StandardScalar, SVM classifier kernel='poly' and degree = 3, decision_function_shape='ovr'and class_weight='balanced', C=10. Train the model on training data, and make predictions for test data. Generate the Classification report and choose the correct value for weighted avg of f1_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.99      0.98      0.99      3463\\n           1       0.99      0.99      0.99      3927\\n           2       0.96      0.97      0.96      3520\\n           3       0.98      0.96      0.97      3551\\n           4       0.96      0.98      0.97      3333\\n           5       0.97      0.97      0.97      3144\\n           6       0.98      0.98      0.98      3490\\n           7       0.98      0.97      0.97      3718\\n           8       0.96      0.96      0.96      3344\\n           9       0.96      0.96      0.96      3510\\n\\n    accuracy                           0.97     35000\\n   macro avg       0.97      0.97      0.97     35000\\nweighted avg       0.97      0.97      0.97     35000\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X, y , test_size=0.5 , random_state=42) \n",
    "pipe = make_pipeline(StandardScaler(), SVC(kernel='poly', degree=3, class_weight='balanced' , decision_function_shape='ovr',C=10))\n",
    "pipe.fit(X_train, y_train)\n",
    "print(classification_report(y_test , pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 7 : Write the function compute_score(X_train, y_train, X_test, y_test) to do the following on the Iris dataset-\n",
    "\n",
    "Write your code keeping in mind: \\\n",
    "Split the Iris dataset into train and test set with 70:30 ratio (Take random state value as 42) \\\n",
    "Import svm.SVC as 'model' \\\n",
    "kernel as 'poly', regularization parameter as 10 and gamma as 'auto' \\\n",
    "Train the model and mark the computed accuracy score for test data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X ,y  =  load_iris(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.3, random_state=42)\n",
    "svc = SVC(C=10, kernel='poly', gamma='auto')\n",
    "svc.fit(X_train, y_train)\n",
    "accuracy_score(y_test, svc.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8 : Write the function compute_score(X_train, y_train, X_test, y_test) to do the following on the Iris dataset-\n",
    "\n",
    "Write your code keeping in mind the following: \\\n",
    "Split the Iris dataset into train and test sets with 70:30 ratio(Take random state value as 42) \\\n",
    "Import svm.SVC as 'model' \\\n",
    "kernel as 'sigmoid', regularization parameter as 25, and gamma as 'auto' \\\n",
    "Train the 'model' and mark the computed 'accuracy score' for the test data.' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28888888888888886"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(C=25, kernel='sigmoid', gamma='auto')\n",
    "svc.fit(X_train, y_train)\n",
    "accuracy_score(y_test, svc.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question9 : Import the iris dataset and drop the rows where class=Iris-setosa. Apply a pipeline containing a MinMaxScaler()function calledScaler and a svm.svc() called classifier. Split the iris dataset into 75:25 ratio with random_state=0. Mark the correct precision score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.25, random_state=0)\n",
    "pipe = make_pipeline(MinMaxScaler(),SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train , y_train)\n",
    "\n",
    "print(classification_report(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
