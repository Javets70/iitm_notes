{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"about/","title":"Iram et nec","text":""},{"location":"about/#iovis-egreditur-frigoris-heros-notissima-undas-momordit","title":"Iovis egreditur frigoris heros notissima undas momordit","text":"<p>Lorem markdownum aut regem, cumulusque: spiritus gemellam; Titania etiam ire meruisse, sorores! Virus thalamumque Aesaris naturae Iuno, inbutam est iram, temptat sanctasque idem sed ambae autumnos quasque. Vestigia excipit auctor Cinyras oscula.</p>"},{"location":"about/#vestigia-animaeque-cornum","title":"Vestigia animaeque cornum","text":"<p>Suspirat et lynca passim, temperiem in Aeneas thyrso finita, hic Arcade iunguntur, solis: ait. Est tenuitque ne atque mutare cum, finxit, internodia vim poenaeque venti vero. Fervet quis puer, more nec Phocaico lumen parenti nullaque Aenean, nec pater super quoniam: lucem mirabile Ligurum. Fuge dum manus insolida musco; illa virgaque triplicis venatibus protinus inque alis se saxa.</p>"},{"location":"about/#pavet-habuistis-occupat-potentior-ubicumque-pars","title":"Pavet habuistis occupat potentior ubicumque pars","text":"<p>Habuit o animam facies, vel turba clausa tamen finemque. Pars maius cumque plangente et vita amor cautus; Iapygis ingeniis ab nunc serpunt litusque.</p> <ul> <li>Mente virum</li> <li>Melanchaetes genus solus nunc desine errare de</li> <li>Quaque quibus patris venenis vixisti nostro pulmone</li> <li>Quod ante surgere mandat pallidiora mors est</li> <li>Colebat filius</li> <li>Aetas tori dextra Achille</li> </ul>"},{"location":"about/#undis-phoebus-praevius-bracchia-bracchia-foret","title":"Undis Phoebus praevius bracchia bracchia foret","text":"<p>Nec bonis, desuetudine, placet ara virgo excipit. A est dedecus quid nempe et soror ingens tua quae mora corpus notas, tempore. A et felix peragentem et ossibus parabat est quae arcus! Quae mihi parte. Regis meum illa melle bracchia; de Iove tu vivit graves, geniti aliquis mariti.</p> <ul> <li>Mea semper tenaci</li> <li>Vacuae baculum mea de et nunc</li> <li>Equos ipsaque dulcia smaragdis montibus tectis ita</li> <li>Per puer nisi cuncta et in maerens</li> <li>Adunca et parvis timide deae constabat ideoque</li> </ul>"},{"location":"about/#modumque-quoque-acoetes-ad-iam-equos-poenas","title":"Modumque quoque Acoetes ad iam equos poenas","text":"<p>Lumina sub miratur est foret Aegyptia tamen, et petam amplexus quoque. Et dixit alios; tua saepe quod Aetnaeo seque: ebur sed exspectatum.</p> <ul> <li>Virgo humilesque et silicem guttas circumdata quae</li> <li>Et puerum</li> <li>Seu profatur madent</li> <li>Tamen saepe grandaevumque fontes mortis tantum aquas</li> </ul> <p>Urbes gravius ponite. Per arcus humilesque credere tangi non.</p>"},{"location":"MATHS2/1.01%20-%20Vectors/","title":"Vectors","text":""},{"location":"MATHS2/1.01%20-%20Vectors/#a-vector-can-be-thought-of-as-a-list","title":"A vector can be thought of as a list.","text":"<ul> <li>Vectors can be used to perform arithmatic operations on lists such as table rows or columns.</li> </ul>"},{"location":"MATHS2/1.02%20-%20Matricies/","title":"Matrix","text":""},{"location":"MATHS2/1.02%20-%20Matricies/#video-link","title":"Video Link","text":"<ul> <li>Matrix is a rectangular array of number arranged in rows and columns. (Pural: Matrices)</li> <li>Example:</li> <li>\\(\\begin{bmatrix}       1 &amp; 2 &amp; 3 \\\\       4 &amp; 5 &amp; 6 \\\\       \\end{bmatrix}\\)</li> <li>This is a 2x3 matrix.</li> <li>A matrix has \\(m\\) x \\(n\\) dimensions.</li> </ul>"},{"location":"MATHS2/1.02%20-%20Matricies/#square-matrix","title":"Square Matrix","text":"<ul> <li> <p>A square matrix is a matrix in which the number of rows is the same as the number of columns.</p> </li> <li> <p>Example:</p> </li> <li>\\(\\begin{bmatrix}       1 &amp; 2 &amp; 3 \\\\       4 &amp; 5 &amp; 6 \\\\       7 &amp; 8 &amp; 9 \\\\       \\end{bmatrix}\\)</li> <li>This is a 3x3 matrix.</li> <li>The (2,3) element is 6.</li> <li>The \\(i\\)-th diagonal element is \\(a_{ii}\\).</li> </ul>"},{"location":"MATHS2/1.02%20-%20Matricies/#diagonal-matrix","title":"Diagonal Matrix","text":"<ul> <li>A diagonal matrix is a square matrix in which all the elements outside the main diagonal are zero.<ul> <li>Example:</li> <li>\\(\\begin{bmatrix}     1 &amp; 0 &amp; 0 \\\\     0 &amp; 2 &amp; 0 \\\\     0 &amp; 0 &amp; 3 \\\\     \\end{bmatrix}\\)</li> <li>This is a 3x3 diagonal matrix.</li> <li>The (2,3) element is 0.</li> </ul> </li> </ul>"},{"location":"MATHS2/1.02%20-%20Matricies/#scalar-matrix","title":"Scalar Matrix","text":"<ul> <li>A scalar matrix is a square matrix in which all the diagonal elements are the same.<ul> <li>Example:</li> <li>\\(\\begin{bmatrix}     2 &amp; 0 &amp; 0 \\\\     0 &amp; 2 &amp; 0 \\\\     0 &amp; 0 &amp; 2 \\\\     \\end{bmatrix}\\)</li> <li>This is a 3x3 scalar matrix.</li> <li>The (2,3) element is 0.</li> </ul> </li> </ul>"},{"location":"MATHS2/1.02%20-%20Matricies/#identity-matrix","title":"Identity Matrix","text":"<ul> <li>The identity matrix is a diagonal matrix in which all the diagonal elements are 1.</li> <li>Example:</li> <li>\\(\\begin{bmatrix}           1 &amp; 0 &amp; 0 \\\\           0 &amp; 1 &amp; 0 \\\\           0 &amp; 0 &amp; 1 \\\\           \\end{bmatrix}\\)</li> <li>This is a 3x3 identity matrix.</li> <li>The (2,3) element is 0.</li> </ul>"},{"location":"MATHS2/1.02%20-%20Matricies/#linear-equations-and-matrices","title":"Linear Equations and Matrices","text":"<ul> <li>A linear equation is an equation that can be written in the form \\(Ax = b\\) where \\(A\\) is a matrix, \\(x\\) is a vector and \\(b\\) is a vector.</li> <li>Example:</li> <li>\\(3x + 2y = 5\\)</li> <li>\\(4x + 6y = 7\\)</li> <li>\\(A = \\begin{bmatrix}           3 &amp; 2 | &amp; 5 \\\\           4 &amp; 6 | &amp; 7\\\\           \\end{bmatrix}\\)</li> </ul>"},{"location":"MATHS2/1.02%20-%20Matricies/#matrix-arithmetic","title":"Matrix Arithmetic","text":"<ul> <li>Lets take two matricies and perform arithmetic operations on them</li> <li>\\(A = \\begin{bmatrix}         1 &amp; 2 &amp; 3 \\\\         4 &amp; 5 &amp; 6 \\\\         \\end{bmatrix}\\)</li> <li>\\(B = \\begin{bmatrix}         4 &amp; 5 &amp; 6 \\\\         7 &amp; 8 &amp; 9 \\\\         \\end{bmatrix}\\)</li> </ul>"},{"location":"MATHS2/1.02%20-%20Matricies/#addition","title":"Addition","text":"<ul> <li>The sum of two matricies is the matrix obtained by adding corresponding elements of the two matricies.</li> <li>Example:</li> <li>\\(A + B = \\begin{bmatrix}               1 + 4 &amp; 2 + 5 &amp; 3 + 6 \\\\               4 + 7 &amp; 5 + 8 &amp; 6 + 9 \\\\               \\end{bmatrix}\\)</li> <li>\\(A + B = \\begin{bmatrix}               5 &amp; 7 &amp; 9 \\\\               11 &amp; 13 &amp; 15 \\\\               \\end{bmatrix}\\)</li> </ul>"},{"location":"MATHS2/1.02%20-%20Matricies/#subtraction","title":"Subtraction","text":"<ul> <li>Subtration is the same as addition but with subtraction.</li> <li>Example:</li> <li>\\(A - B = \\begin{bmatrix}               1 - 4 &amp; 2 - 5 &amp; 3 - 6 \\\\               4 - 7 &amp; 5 - 8 &amp; 6 - 9 \\\\               \\end{bmatrix}\\)</li> <li>\\(A - B = \\begin{bmatrix}               -3 &amp; -3 &amp; -3 \\\\               -3 &amp; -3 &amp; -3 \\\\               \\end{bmatrix}\\)</li> </ul>"},{"location":"MATHS2/1.02%20-%20Matricies/#in-addition-and-subtration-the-matricies-must-have-the-same-dimensions","title":"In addition and subtration the matricies must have the same dimensions.","text":""},{"location":"MATHS2/1.02%20-%20Matricies/#scalar-multiplication","title":"Scalar Multiplication","text":"<ul> <li> <p>The product of a matrix \\(A\\) with a number \\(c\\) is denoted by \\(cA\\) and the (i,j)-th entry of \\(cA\\) is product of (i,j)-th entry of \\(A\\) with the number \\(c\\).</p> </li> <li> <p>Example:</p> </li> <li>\\(2A = \\begin{bmatrix}             2 &amp; 4 &amp; 6 \\\\             8 &amp; 10 &amp; 12 \\\\             \\end{bmatrix}\\)</li> </ul>"},{"location":"MATHS2/1.02%20-%20Matricies/#matrix-multiplication","title":"Matrix Multiplication","text":"<ul> <li>The product of two matricies \\(A\\) and \\(B\\) is denoted by \\(AB\\) and is defined only if the number of columns of \\(A\\) is equal to the number of rows of \\(B\\).</li> </ul>"},{"location":"MATHS2/1.02%20-%20Matricies/#formula","title":"Formula:","text":"\\[(AB)_{ij} = \\sum_{k=1}^nA_{ij}\\cdot B_{kj}\\]"},{"location":"MATHS2/1.02%20-%20Matricies/#example","title":"Example:","text":"<ul> <li>\\(A = \\begin{bmatrix}           1 &amp; 2 &amp; 3 \\\\           4 &amp; 5 &amp; 6 \\\\           \\end{bmatrix}\\)</li> <li>\\(B = \\begin{bmatrix}           4 &amp; 5 &amp; 6 \\\\           7 &amp; 8 &amp; 9 \\\\           \\end{bmatrix}\\)</li> <li>\\(AB = \\begin{bmatrix}             1 &amp; 2 &amp; 3 \\\\             4 &amp; 5 &amp; 6 \\\\             \\end{bmatrix} \\begin{bmatrix}             4 &amp; 5 &amp; 6 \\\\             7 &amp; 8 &amp; 9 \\\\             \\end{bmatrix}\\)</li> <li>\\(AB = \\begin{bmatrix}             1 \\times 4 + 2 \\times 7 &amp; 1 \\times 5 + 2 \\times 8 &amp; 1 \\times 6 + 2 \\times 9 \\\\             4 \\times 4 + 5 \\times 7 &amp; 4 \\times 5 + 5 \\times 8 &amp; 4 \\times 6 + 5 \\times 9 \\\\             \\end{bmatrix}\\)</li> <li>\\(AB = \\begin{bmatrix}             18 &amp; 21 &amp; 24 \\\\             45 &amp; 54 &amp; 63 \\\\             \\end{bmatrix}\\)</li> </ul>"},{"location":"MATHS2/1.02%20-%20Matricies/#about-matrix-multiplication","title":"About Matrix Multiplication:","text":"<ul> <li>In matrix multiplication the number of columns of the first matrix must be equal to the number of rows of the second matrix.</li> <li>The dimensions of the product matrix are the number of rows of the first matrix by the number of columns of the second matrix.</li> <li>\\(\\text{Scalar Multiplication by } c = \\text{Mulltiplication by scalar matrix } cI\\)</li> </ul>"},{"location":"MATHS2/1.02%20-%20Matricies/#properties-of-matrix-arithmetic","title":"Properties of Matrix Arithmetic","text":"<ul> <li>\\(A + B = B + A\\)</li> <li>\\((AB)C = A(BC)\\)</li> <li>\\(AB \\neq BA\\)</li> <li>\\(\\lambda(A + B) = \\lambda A + \\lambda B\\)</li> </ul>"},{"location":"MATHS2/1.03%20-%20Determinants/","title":"Determinants","text":""},{"location":"MATHS2/1.03%20-%20Determinants/#video-link","title":"Video Link","text":""},{"location":"MATHS2/1.03%20-%20Determinants/#definition","title":"Definition","text":"<ul> <li>Every square matrix A has an associated number, called its determinant and denoted by \\(\\det(A)\\) or \\(|A|\\).  It is used in : <ul> <li>Solving systems of linear equations</li> <li>Finding the inverse of a matrix</li> <li>Calculus and More</li> </ul> </li> </ul>"},{"location":"MATHS2/1.03%20-%20Determinants/#calculating-determinants","title":"Calculating Determinants","text":"<ul> <li> <p>Determinant of a 1 x 1 matrix : \\(\\det(A) = a_{11}\\)</p> </li> <li> <p>Determinant of a 2 x 2 matrix :</p> <ul> <li>\\(A = \\begin{bmatrix}         a &amp; b \\\\         c &amp; d \\\\         \\end{bmatrix}\\)</li> <li>\\(det(A) = ad - bc\\)</li> </ul> </li> <li> <p>Determinant of a 3 x 3 matrix :</p> <ul> <li>\\(A = \\begin{bmatrix}         a_{11} &amp; a_{12} &amp; a_{13} \\\\         a_{21} &amp; a_{22} &amp; a_{23} \\\\         a_{31} &amp; a_{32} &amp; a_{33} \\\\         \\end{bmatrix}\\)</li> <li>We will the get the determinant by using the first row.</li> <li>\\(det(A) = a_{11} \\begin{vmatrix}         a_{22} &amp; a_{23} \\\\         a_{32} &amp; a_{33} \\\\         \\end{vmatrix} - a_{12} \\begin{vmatrix}         a_{21} &amp; a_{23} \\\\         a_{31} &amp; a_{33} \\\\         \\end{vmatrix} + a_{13} \\begin{vmatrix}         a_{21} &amp; a_{22} \\\\         a_{31} &amp; a_{32} \\\\         \\end{vmatrix}\\)</li> <li>\\(det(A) = a_{11} (a_{22}a_{33} - a_{23}a_{32}) - a_{12} (a_{21}a_{33} - a_{23}a_{31}) + a_{13} (a_{21}a_{32} - a_{22}a_{31})\\)</li> </ul> </li> </ul> <p>Example</p> <p>\\(A = \\begin{bmatrix}         2 &amp; 4 &amp; 1 \\\\         3 &amp; 8 &amp; 7 \\\\         5 &amp; 6 &amp; 9 \\\\         \\end{bmatrix}\\)</p> <p>\\(\\begin{align}         \\det(A) &amp;= 2 \\begin{vmatrix}         8 &amp; 7 \\\\         6 &amp; 9 \\\\         \\end{vmatrix} - 4 \\begin{vmatrix}         3 &amp; 7 \\\\         5 &amp; 9 \\\\         \\end{vmatrix} + 1 \\begin{vmatrix}         3 &amp; 8 \\\\         5 &amp; 6 \\\\         \\end{vmatrix} \\\\         &amp;= 2 (8 \\times 9 - 7 \\times 6) - 4 (3 \\times 9 - 7 \\times 5) + 1 (3 \\times 6 - 8 \\times 5) \\\\         &amp;= 2 (72 - 42) - 4 (27 - 35) + 1 (18 - 40) \\\\         &amp;= 2 (30) - 4 (-8) + 1 (-22) \\\\         &amp;= 60 - (-32) + 22 \\\\         &amp;= 94 \\end{align}\\)</p>"},{"location":"MATHS2/1.03%20-%20Determinants/#determinant-of-identity-matrix","title":"Determinant of Identity Matrix","text":"<ul> <li>Determinant of a 1 x 1 identity matrix : \\(\\det(I) = 1\\)</li> <li>Determinant of a 2 x 2 identity matrix : \\(\\det(I) = (1 \\times 1) - (0 \\times 0) = 1\\)</li> <li>Determinant of a 3 x 3 identity matrix : \\(\\det(I) = 1 \\times 1 \\times 1 - 0 \\times 0 \\times 0 = 1\\)</li> </ul> <p>Properties of Determinant</p> <ul> <li>Determinant of Product of Matrices :  \\(\\det(AB) = \\det(A) \\times \\det(B)\\)</li> <li>Determinant of Inverse of Matrix : \\(\\det(A^{-1}) = \\frac{1}{\\det(A)}\\)</li> <li>Switching Rows : \\(\\det(A') = -\\det(A)\\) ( \\(A'\\) is the new matrix with switched rows.)</li> <li>Switching Columns : \\(\\det(A') = -det(A)\\) (\\(A'\\) is the new matrix with switched columns.)</li> <li> <p>Adding multiples of rows: (Same proof for multiple of columns)</p> <ul> <li>\\(A = \\begin{bmatrix}       a &amp; b \\\\       c &amp; d \\\\       \\end{bmatrix}\\)</li> <li> <p>\\(A' = \\begin{bmatrix}       a + tc &amp; b + td \\\\       c  &amp; d  \\\\       \\end{bmatrix}\\)</p> <p>\\(\\begin{align}  \\det(A') &amp;= (a + tc)(d) - (b + td)(c) \\\\ &amp;= ad + tcd - bc - tcd \\\\ &amp;= ad - bc \\\\ &amp;= \\det(A) \\\\ \\end{align}\\)</p> </li> </ul> </li> <li> <p>Scalar multiplication of a row/column of matrix \\(A\\) with a constant \\(t\\) : \\(det(A') = t \\cdot det(A)\\)</p> </li> </ul>"},{"location":"MATHS2/1.03%20-%20Determinants/#minors","title":"Minors","text":"<ul> <li>Minor of a matrix is the determinant of a submatrix obtained by deleting a row and a column from the matrix.</li> <li>Minors are denoted by \\(M_{ij}\\).</li> </ul> <p>Example</p> <p>\\(A = \\begin{bmatrix}         a_{11} &amp; a_{12} &amp; a_{13} \\\\         a_{21} &amp; a_{22} &amp; a_{23} \\\\         a_{31} &amp; a_{32} &amp; a_{33} \\\\         \\end{bmatrix}\\)</p> <p>\\(M_{11} = \\begin{vmatrix}         a_{22} &amp; a_{23} \\\\         a_{32} &amp; a_{33} \\\\         \\end{vmatrix}\\)</p> <p>\\(M_{11} = a_{22}a_{33} - a_{23}a_{32}\\)</p>"},{"location":"MATHS2/1.03%20-%20Determinants/#cofactors","title":"Cofactors","text":"<ul> <li>Cofactor of a matrix is the determinant of a submatrix obtained by deleting a row and a column from the matrix.</li> <li> <p>Cofactors are used to calculate the inverse of a matrix.</p> </li> <li> <p>We can use Minors and Cofactors to calculate the determinant of a matrix.</p> </li> </ul>"},{"location":"MATHS2/1.03%20-%20Determinants/#formula","title":"Formula","text":"\\[\\sum_{j=1}^n (-1)^{1+i} \\cdot a_{1i} \\cdot M_{1j} = \\sum_{j=1}^n (-1)^{1+i} \\cdot a_{1i} \\cdot C_{1j}\\]"},{"location":"MATHS2/1.03%20-%20Determinants/#expansion-along-any-row-or-column","title":"Expansion along any row or column","text":"\\[det(A) = \\sum_{j=1}^n (-1)^{1+i} \\cdot a_{ii} \\cdot C_{ij} \\text{ (i is fixed)}\\] \\[\\text{OR}\\] \\[det(A) = \\sum_{j=1}^n (-1)^{1+i} \\cdot a_{ii} \\cdot C_{ij} \\text{ (j is fixed)}\\] <p>More Properties of Determinants</p> <ul> <li>\\(\\det(A^n) = \\det(A)^n\\)</li> <li>\\(\\det(A^{-1}) = \\frac{1}{\\det(A)}\\)</li> <li>\\(\\det(P^{-1}AP) = \\det(A)\\)</li> <li>\\(\\det(AB) = \\det(BA)\\)</li> <li>\\(\\det(A^T) = \\det(A)\\)</li> <li>\\(\\det(tA_{n\\times n}) = t^n \\cdot \\det(A)\\)</li> </ul> <p>Tips for Calculating Determinant</p> <ul> <li>The determinant of a matrix with a row or column of zeros is \\(0\\).</li> <li>The determinant of a matrix in which one row (or column) is a linear combination of other rows (resp. columns) is \\(0\\).</li> <li>Scalar multiplication of a row by a constant t multiplies the     determinant by \\(t\\).</li> <li>While computing the determinant, you can choose to compute it using expansion along a suitable row or column.</li> </ul>"},{"location":"MATHS2/10.01%20-%20Directional%20ascent%20%26%20descent/","title":"Direction of Ascent and Descent","text":"<ul> <li>Let \\(f(x_1,x_2, \\dots, x_n)\\) be a function defined on a domain in \\(D\\) in \\(\\mathbb{R}^n\\) containing some open ball around the point \\(a\\).</li> <li>Suppose \\(\\nabla f\\) exists and is continuous on some open ball around the point \\(a\\).</li> <li>\\(f_u = \\nabla f \\cdot u\\) is the directional derivative of \\(f\\) in the direction of \\(u\\).</li> <li>\\(f_u(a) = ||\\nabla f(a)|| ||u||\\cos \\theta\\)</li> <li>It is minimized when \\(\\theta = \\pi\\) and maximized when \\(\\theta = 0\\).</li> <li>It is maximised when \\(u\\) is in the same direction as \\(\\nabla f(a)\\) or \\(u = \\frac{\\nabla f(a)}{||\\nabla f(a)||}\\).</li> <li>It is minimized when \\(u\\) is in the opposite direction as \\(\\nabla f(a)\\) or \\(u = -\\frac{\\nabla f(a)}{||\\nabla f(a)||}\\).</li> <li>It remains unchanged when \\(f_u = 0\\) or \\(\\theta = \\frac{\\pi}{2}\\). It is orthogonal or perpendicular to \\(\\nabla f(a)\\).</li> </ul>"},{"location":"MATHS2/10.01%20-%20Directional%20ascent%20%26%20descent/#example","title":"Example","text":"<ul> <li> <p>\\(f(x,y) = \\sin(xy)\\)</p> </li> <li> <p>\\(\\nabla f(a) = (y\\cos(xy),x\\cos(xy))\\)</p> </li> <li>At \\((\\pi,1)\\)</li> <li>\\(\\nabla f(\\pi,1) = (1 \\cos(\\pi), \\pi \\cos(\\pi)) = (-1,\\pi)\\)</li> <li> <p>\\(u = \\frac{\\nabla f(\\pi,1)}{||\\nabla f(\\pi,1)||} = \\frac{-1,\\pi}{\\sqrt{1^2 + \\pi^2}} = \\frac{1}{\\sqrt{\\pi^2 + 1}} \\cdot (1,\\pi)\\)</p> </li> <li> <p>\\(f(x,y,z) = x^2 + y^2 +z^2\\)</p> </li> <li>\\(\\nabla f(a) = (2x,2y,2z)\\)</li> <li>At $(1,1,1) what is the direction in which the function increases fastest?</li> <li>\\(\\nabla f(1,1,1) = (2,2,2)\\)</li> <li>\\(u = \\frac{(2,2,2)}{\\sqrt{2^2 + 2^2 + 2^2}} = \\frac{1}{\\sqrt{12}}(2,2,2)\\)</li> </ul>"},{"location":"MATHS2/10.02%20-%20Tangets%20of%20scalared-valued%20multivariable%20functions/","title":"Tangents for \\(f(x,y)\\)","text":"<ul> <li>Let \\(f(x,y)\\) be a function defined on a domain in \\(D\\) in \\(\\mathbb{R}^2\\) containing some open ball around the point \\(a\\).</li> <li>Consider a line \\(L\\) in \\(D\\) passing through \\(a\\) and restrict \\(f\\) to \\(L\\).</li> <li>Sicne it now a function of one variable, we can consider the tanget to \\(f\\) at \\(a\\) over \\(L\\).</li> <li>If the line \\(L\\) is in the direction of the unit vector \\(u\\), then the tangent (if it exists) is given by \\(f_u(a)\\) passing through the point \\((a,f(a))\\).</li> </ul>"},{"location":"MATHS2/10.02%20-%20Tangets%20of%20scalared-valued%20multivariable%20functions/#equations-of-the-tangent-line","title":"Equations of the tangent line","text":"<ul> <li>\\(u = (u_1,u_2)\\) is a unit vector in the direction of the tangent line.</li> <li>\\(a = (a,b)\\) is the point on the tangent line.</li> <li>\\(f_u(a)\\) is the directional derivative at \\(a\\) in the direction of \\(u\\).</li> <li>\\(L : z = 0, u_1(y-b) = u_2(x-a)\\)</li> <li>\\(P : u_1(y-b) = u_2(x-a)\\)</li> <li>\\(x(t) = a + tu_1\\)</li> <li>\\(y(t) = b + tu_2\\)</li> <li>\\(z(t) = 0\\)</li> <li>\\((x(t),y(t),z(t)) = (a + tu_1, b + tu_2, 0)\\)</li> <li>Parametric equation of the tangent line is \\(P(t) = (a + tu_1, b + tu_2, f(a,b) + tf_u(a,b))\\)</li> </ul>"},{"location":"MATHS2/10.02%20-%20Tangets%20of%20scalared-valued%20multivariable%20functions/#example","title":"Example","text":"<ul> <li>\\(f(x,y) = x + y;\\) tangent at \\((1,1)\\) in the direction of \\((1,0)\\)</li> <li>\\(f_u(a) = \\nabla f \\cdot u = 1\\)</li> <li>\\((x(t),y(t),z(t)) = (1 + t, 1, 1 + tf_u(1,1)) = (1 + t, 1, 2 + t)\\)</li> <li>\\(f(x,y) = xy;\\) tangent at \\((1,1)\\) in the direction of \\((3,4)\\)</li> <li>\\(u = (\\frac{3}{5},\\frac{4}{5})\\)</li> <li>\\(f_u(1,1) = 1 \\cdot \\frac{3}{5} + 1 \\cdot \\frac{4}{5} = \\frac{7}{5}\\)</li> <li>\\((x(t),y(t),z(t)) = (1 + \\frac{3}{5}t, 1 + \\frac{4}{5}t, 1 + \\frac{7}{5}t)\\)</li> </ul>"},{"location":"MATHS2/10.02%20-%20Tangets%20of%20scalared-valued%20multivariable%20functions/#tangents-for-scalared-valued-multivariable-functions","title":"Tangents for scalared-valued multivariable functions","text":"<ul> <li>Let \\(f(x_1,x_2, \\dots, x_n)\\) be a function defined on a domain in \\(D\\) in \\(\\mathbb{R}^n\\) containing some open ball around the point \\(a\\).</li> <li>Consider a line \\(L\\) in \\(D\\) passing through \\(a\\) and restrict \\(f\\) to \\(L\\).</li> <li>Computing this is same as computing it for \\(R^2\\).</li> </ul>"},{"location":"MATHS2/10.03%20-%20Finding%20the%20tangent%20Hyper%28plane%29/","title":"Collection of all the tangents","text":"<ul> <li>Let \\(f(x,y)\\) be a fucntion defined on a domain \\(D\\) in \\(\\mathbb{R}^2\\) containing some open ball around the point \\((a,b)\\).</li> <li>Suppose \\(\\nabla f\\) exists and is continuous on some open ball around the point \\((a,b)\\).</li> <li>Then all the tangent line at the point \\((a,b)\\) exists and we can rewrite the equation of a tangent line the direction of the unit vector \\(u\\) as:   \\(\\(\\begin{align} f_u(a,b) &amp;= \\nabla f \\cdot u \\\\ &amp;= u_1 \\frac{\\partial f}{\\partial x} + u_2 \\frac{\\partial f}{\\partial y} \\end{align}\\)\\)</li> </ul>"},{"location":"MATHS2/10.03%20-%20Finding%20the%20tangent%20Hyper%28plane%29/#tangents-in-terms-of-linear-algebra-for-fxy","title":"Tangents in terms of linear algebra for \\(f(x,y)\\)","text":"<ul> <li>\\((x(t),y(t),z(t)) = (a + tu_1, b + tu_2, f(a,b) + tf_u(a,b))\\)</li> <li>Tangent line to \\(f\\) at \\((a,b)\\) in the direction of \\(u\\) is given by \\((a,b,f(a,b)) + W\\) \\(\\(z = f(a,b) + \\frac{\\partial f}{\\partial x}(a,b)(x-a) + \\frac{\\partial f}{\\partial y}(a,b)(y-b)\\)\\)</li> </ul>"},{"location":"MATHS2/10.03%20-%20Finding%20the%20tangent%20Hyper%28plane%29/#the-tangent-hyperplane","title":"The tangent hyperplane","text":"<ul> <li>Let \\(f(x_1,x_2, \\dots, x_n)\\) be a function defined on a domain \\(D\\) in \\(\\mathbb{R}^n\\) containing some open ball around the point \\(a\\).</li> <li>Suppose \\(\\nabla f\\) exists and is continuous on some open ball around the point \\(a\\).</li> </ul>"},{"location":"MATHS2/10.03%20-%20Finding%20the%20tangent%20Hyper%28plane%29/#linear-approximation","title":"Linear Approximation","text":"<ul> <li>Let \\(f(x_1,x_2, \\dots, x_n)\\) be a function defined on a domain \\(D\\) in \\(\\mathbb{R}^n\\) containing some open ball around the point \\(a\\).</li> <li>Suppose \\(\\nabla f\\) exists and is continuous on some open ball around the point \\(a\\).</li> <li>Then the function:   \\(\\(L_f(x) = f(a) + \\nabla f \\cdot (x-a)\\)\\)</li> </ul>"},{"location":"MATHS2/10.04%20-%20Points%20of%20local%20extrema%20for%20multivariable%20functions/","title":"Points of local extrema for multivariable functions","text":"<ul> <li>Let \\(f(x)\\) be a function defined on a domain \\(D\\) in \\(\\mathbb{R}^n\\) containing some open ball around the point \\(a\\).</li> <li>The point \\(a\\) is the local maximum (or point of local maximum) of \\(f\\) if for some some open ball \\(B\\) containing \\(a\\), \\(f(x) \\leq f(a)\\) whenever \\(x \\in B \\cap D\\).</li> <li>The point \\(a\\) is the local minimum (or point of local minimum) of \\(f\\) if for some some open ball \\(B\\) containing \\(a\\), \\(f(x) \\geq f(a)\\) whenever \\(x \\in B \\cap D\\).</li> <li>A local extremum (or point of local extremum) of \\(f\\) is a point \\(a\\) such that \\(f\\) is either a local maximum or a local minimum at \\(a\\).</li> </ul>"},{"location":"MATHS2/10.04%20-%20Points%20of%20local%20extrema%20for%20multivariable%20functions/#the-gradient-vector-at-points-of-local-extrema","title":"The gradient vector at points of local extrema","text":"<ul> <li>Let \\(f(x)\\) be a function defined on a domain \\(D\\) in \\(\\mathbb{R}^n\\) containing some open ball around the point \\(a\\) of local extremum.</li> <li>Restrict \\(f\\) to a a line \\(L\\) passing throught \\(a\\) and view it as a fucntion of one variable on \\(L\\).</li> <li>Then \\(a\\) is a local extremum for the restricted function on \\(L\\) and hence the directional derivative of \\(f\\) in the direction of the \\(L\\)(if it exists) at \\(a\\) is zero.</li> <li>In particular, those partial derivatives which exist at \\(a\\) are zero.</li> </ul>"},{"location":"MATHS2/10.04%20-%20Points%20of%20local%20extrema%20for%20multivariable%20functions/#critical-points","title":"Critical points","text":"<ul> <li>A point \\(a\\) is called a critical point of a function \\(f(x)\\) if either \\(\\nabla f(a)\\) does not exist or \\(\\nabla f(a) = 0\\).</li> </ul>"},{"location":"MATHS2/10.04%20-%20Points%20of%20local%20extrema%20for%20multivariable%20functions/#example","title":"Example","text":"<ul> <li>Critical Points of \\(f(x,y) = x^2 + 6xy + 4y^2 + 2x - 4y\\)</li> <li>\\(\\frac{\\partial f}{\\partial x} = 2x + 6y + 2 = 0\\)</li> <li>\\(\\frac{\\partial f}{\\partial y} = 6x + 8y - 4 = 0\\)</li> <li>\\(\\nabla f(a) = (0,0)\\)</li> <li>\\(\\nabla f(x,y) = (2x + 6y + 2, 6x + 8y - 4) = (0,0)\\)</li> <li>Using Gaussian elimination, we get \\(x = 2\\) and \\(y = -1\\).</li> <li>Therefore, \\((2,-1)\\) is a critical point of \\(f(x,y)\\).</li> </ul>"},{"location":"MATHS2/11.01%20-%20Higher%20order%20partial%20derivatives%20and%20Hessian%20matrix/","title":"Second order partial derivatives for \\(f(x,y)\\)","text":"<ul> <li>Let \\(f(x,y)\\) be a function of two variables.</li> <li>Then the second order partial derivatives for \\(f\\) are the partial derivatives of the first order partial derivatives.</li> <li> \\[   \\begin{align} f_{xx} &amp;=(fx)x &amp;= \\frac{\\partial^2 f}{\\partial x^2}   \\\\ f_{xy} &amp;=(fx)y &amp;= \\frac{\\partial^2 f}{\\partial y \\partial x}   \\\\ f_{yx} &amp;=(fy)x &amp;= \\frac{\\partial^2 f}{\\partial x \\partial y}   \\\\ f_{yy} &amp;=(fy)y &amp;= \\frac{\\partial^2 f}{\\partial y^2} \\end{align}   \\] </li> </ul>"},{"location":"MATHS2/11.01%20-%20Higher%20order%20partial%20derivatives%20and%20Hessian%20matrix/#example","title":"Example","text":"<ul> <li>\\(f(x,y) = x+y\\)</li> <li>\\(\\frac{\\partial f}{\\partial x} = 1\\)</li> <li>\\(\\frac{\\partial f}{\\partial y} = 1\\)</li> <li>\\(f_{xx} = \\frac{\\partial^2 f}{\\partial x^2} = 0\\)</li> <li>\\(f_{xy} = \\frac{\\partial^2 f}{\\partial y \\partial x} = 0\\)</li> <li>\\(f(x,y) = sin(xy)\\)</li> <li>\\(\\frac{\\partial f}{\\partial x} = y\\cos(xy)\\)</li> <li>\\(\\frac{\\partial f}{\\partial y} = x\\cos(xy)\\)</li> <li>\\(f_{xx} = \\frac{\\partial^2 f}{\\partial x^2} = -y^2\\sin(xy)\\)</li> <li>\\(f_{xy} = \\frac{\\partial^2 f}{\\partial y \\partial x} = -xy\\sin(xy)\\)</li> <li>\\(f_{yx} = \\frac{\\partial^2 f}{\\partial x \\partial y} = -yx\\sin(xy)\\)</li> <li>\\(f_{yy} = \\frac{\\partial^2 f}{\\partial y^2} = -x^2\\sin(xy)\\)</li> </ul>"},{"location":"MATHS2/11.01%20-%20Higher%20order%20partial%20derivatives%20and%20Hessian%20matrix/#clairauts-theorem","title":"Clairaut's Theorem","text":"<ul> <li>Let \\(f(x,y)\\) be a function defined on a domain in \\(D\\) in \\(\\mathbb{R}^2\\) containing some open ball around the point \\(a\\).</li> <li>If the second order partial derivatives \\(f_{xy}\\) or \\(f_{yx}\\) are continuous on some open ball around the point \\(a\\) then \\(f_{xy} = f_{yx}\\).</li> </ul>"},{"location":"MATHS2/11.01%20-%20Higher%20order%20partial%20derivatives%20and%20Hessian%20matrix/#higher-order-partial-derivatives","title":"Higher order partial derivatives","text":"<ul> <li>Let \\(f(x_1,x_2, \\dots, x_n)\\) be a function defined on a domain D in \\(\\mathbb{R}^n\\). Then the higher order partial derivatives of \\(f\\) are defined by taking successive partial derivatives of \\(f\\).   $$ f_{x_1x_2 \\dots x_n} = \\frac{\\partial^4 f}{\\partial x_1 \\partial x_2 \\dots \\partial x_n} $$</li> <li>An appropriately modified statement of Clairaut's theorem holds.</li> </ul>"},{"location":"MATHS2/11.01%20-%20Higher%20order%20partial%20derivatives%20and%20Hessian%20matrix/#hessian-matrix","title":"Hessian matrix","text":"<ul> <li>Let \\(f(x_1,x_2, \\dots, x_n)\\) be a function defined on a domain D in \\(\\mathbb{R}^n\\).</li> <li>Then the Hessian matrix of \\(f\\) is defined has   \\(\\(\\begin{bmatrix} f_{xx} &amp; f_{xy} &amp; \\dots &amp; f_{x_n} \\\\ f_{yx} &amp; f_{yy} &amp; \\dots &amp; f_{y_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ f_{n_1x} &amp; f_{n_1y} &amp; \\dots &amp; f_{n_1n} \\end{bmatrix}\\)\\)</li> </ul>"},{"location":"MATHS2/11.01%20-%20Higher%20order%20partial%20derivatives%20and%20Hessian%20matrix/#example_1","title":"Example","text":"<ul> <li>\\(f (x,y,z) = xy + yz + xz\\)</li> <li>\\(\\begin{bmatrix} f_{xx} &amp; f_{xy} &amp; f_{xz} \\\\ f_{yx} &amp; f_{yy} &amp; f_{yz} \\\\ f_{zx} &amp; f_{zy} &amp; f_{zz} \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 \\end{bmatrix}\\)</li> </ul>"},{"location":"MATHS2/11.02%20-%20Hessian%20matrix%20and%20local%20extrema/","title":"Hessian Test : Classifying Critical Points for \\(f(x,y)\\)","text":"<ul> <li>Let \\(f(x,y)\\) be a function defined on a domain \\(D\\) in \\(\\mathbb{R}^2\\).</li> <li>Let \\(a\\) be a critical point of \\(f\\) such that the first and second order partial derivatives are continuous in an open ball around \\(a\\).</li> <li> <p>Then the Hessian test can be applied to check the nature of the critical point \\(a\\).</p> </li> <li> <p>If \\(det(Hf(a)) &gt; 0\\) and \\(f_{xx}(a) &gt; 0\\) then \\(a\\) is a local minimum.</p> </li> <li>If \\(det(Hf(a)) &gt; 0\\) and \\(f_{xx}(a) &lt; 0\\) then \\(a\\) is a local maximum.</li> <li>If \\(det(Hf(a)) &lt; 0\\) then \\(a\\) is a saddle point.</li> <li>If \\(det(Hf(a)) = 0\\) then \\(a\\) then the test is inconclusive.</li> </ul>"},{"location":"MATHS2/11.02%20-%20Hessian%20matrix%20and%20local%20extrema/#hessian-test-classifying-critical-points-for-fxyz","title":"Hessian Test : Classifying Critical Points for \\(f(x,y,z)\\)","text":"<ul> <li>Let \\(f(x,y,z)\\) be a function defined on a domain \\(D\\) in \\(\\mathbb{R}^3\\).</li> <li>Let \\(a\\) be a critical point of \\(f\\) such that the first and second order partial derivatives are continuous in an open ball around \\(a\\).</li> <li>Then the Hessian test can be applied to check the nature of the critical point \\(a\\).</li> <li>If \\(f_{xx}(a) &gt; 0, (f_{xx}f_{yy}-f_{xy}^2)(a) &gt; 0\\) and \\(det(Hf(a)) &gt; 0\\) then \\(a\\) is a local minimum.</li> <li>If \\(f_{xx}(a) &lt; 0, (f_{xx}f_{yy}-f_{xy}^2)(a) &gt; 0\\) and \\(det(Hf(a)) &lt; 0\\) then \\(a\\) is a local maximum.</li> <li>For all other cases where the determinant of the Hessian \\(\\neq 0\\) then it is a saddle point.</li> <li>If \\(det(Hf(a)) = 0\\) then \\(a\\) then the test is inconclusive.</li> </ul>"},{"location":"MATHS2/11.04%20-%20Differentiablity%20for%20Multvariable%20Functions/","title":"Differenability for Scalar-Value Multivariable Functions","text":"<ul> <li>Let \\(f\\) be a scalar-valued multivariable function defined on a domain \\(D\\) in \\(\\mathbb{R}^n\\) containing some open ball around the point \\(a\\).</li> <li>Then \\(f\\) is differentiable at \\(a\\) if:   \\(\\(\\lim_{h \\to 0} \\frac{f(a+h) - f(a)-h\\cdot\\nabla f(a)}{||h||}=0   \\tag{1}\\)\\)</li> <li>If \\(f\\) is differentiable at \\(a\\), then</li> <li>Tangent Hyperplane to \\(f\\) at \\(a\\) exists</li> <li>Best linear approximation of \\(f\\) at \\(a\\) is given by the tangent hyperplane</li> <li>It is continuous at \\(a\\)</li> </ul>"},{"location":"MATHS2/2.01%20-%20Cramer%27s%20Rule/","title":"Cramer's Rule","text":"<ul> <li>Cramer's rule is a formula for solving systems of linear equations using determinants. It expresses the solution for a particular variable in terms of the determinants of matrices derived from the coefficients of the equations and their corresponding constants.</li> </ul>"},{"location":"MATHS2/2.01%20-%20Cramer%27s%20Rule/#forumla","title":"Forumla","text":""},{"location":"MATHS2/2.01%20-%20Cramer%27s%20Rule/#cramers-rule-for-2x2-systems","title":"Cramer's Rule for 2x2 Systems","text":"<ul> <li> <p>The formula for Cramer's rule is:</p> </li> <li> <p>\\(A = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}\\)</p> </li> <li>\\(b = \\begin{bmatrix} e  \\\\ f  \\end{bmatrix}\\)</li> <li>\\(A_{x1} = \\begin{bmatrix} e &amp; b \\\\ f &amp; d \\end{bmatrix}\\)</li> <li> <p>\\(A_{x2} = \\begin{bmatrix} a &amp; e \\\\ c &amp; f \\end{bmatrix}\\) \\(\\(x_1 = det(A_{x1})/det(A)\\)\\) \\(\\(x_2 = det(A_{x2})/det(A)\\)\\)</p> </li> <li> <p>This is how we find the solution to a system of linear equations using Cramer's rule.</p> </li> </ul>"},{"location":"MATHS2/2.01%20-%20Cramer%27s%20Rule/#cramers-rule-for-3x3-systems","title":"Cramer's Rule for 3x3 Systems","text":"<ul> <li> <p>The formula for Cramer's rule is:</p> </li> <li> <p>\\(A = \\begin{bmatrix} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ g &amp; h &amp; i \\end{bmatrix}\\)</p> </li> <li>\\(b = \\begin{bmatrix} j  \\\\ k  \\\\ l \\end{bmatrix}\\)</li> <li>\\(A_{x1} = \\begin{bmatrix} j &amp; b &amp; c \\\\ k &amp; e &amp; f \\\\ l &amp; h &amp; i \\end{bmatrix}\\)</li> <li>\\(A_{x2} = \\begin{bmatrix} a &amp; j &amp; c \\\\ d &amp; k &amp; f \\\\ g &amp; l &amp; i \\end{bmatrix}\\)</li> <li>\\(A_{x3} = \\begin{bmatrix} a &amp; b &amp; j \\\\ d &amp; e &amp; k \\\\ g &amp; h &amp; l \\end{bmatrix}\\) \\(\\(x_1 = det(A_{x1})/det(A)\\)\\) \\(\\(x_2 = det(A_{x2})/det(A)\\)\\) \\(\\(x_3 = det(A_{x3})/det(A)\\)\\)</li> </ul>"},{"location":"MATHS2/2.01%20-%20Cramer%27s%20Rule/#cramers-rule-for-3x3-invertible-matrix","title":"Cramer's Rule for 3x3 invertible matrix","text":"<ul> <li>A 3x3 matrix is invertible if the determinant is not equal to 0.</li> <li>If the determinant is not equal to 0, then the inverse of the matrix is equal to the adjugate divided by the determinant.</li> </ul>"},{"location":"MATHS2/2.02%20-%20Solutions%20to%20a%20system%20of%20linear%20equations%20with%20invertible%20coefficient/","title":"Invertible Coefficient Matrix","text":"<ul> <li>Let \\(A\\) be a \\(n \\times n\\) matrix with \\(det(A) \\neq 0\\). The Inverse of \\(A\\) is \\(B\\) such that \\(AB = BA = I\\).</li> </ul>"},{"location":"MATHS2/2.02%20-%20Solutions%20to%20a%20system%20of%20linear%20equations%20with%20invertible%20coefficient/#example","title":"Example","text":"<ul> <li>\\(A = \\begin{bmatrix} 4 &amp; 7 \\\\ 2 &amp; 6 \\end{bmatrix}\\)</li> <li>\\(B = \\begin{bmatrix} 0.6 &amp; -0.7 \\\\ -0.2 &amp; 4 \\end{bmatrix}\\)</li> <li>\\(A \\times B = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} = B \\times A\\)</li> </ul>"},{"location":"MATHS2/2.02%20-%20Solutions%20to%20a%20system%20of%20linear%20equations%20with%20invertible%20coefficient/#the-adjugate-of-a-matrix","title":"The Adjugate of a matrix","text":"<ul> <li>The adjugate of a matrix is the transpose of the cofactor matrix.</li> </ul>"},{"location":"MATHS2/2.02%20-%20Solutions%20to%20a%20system%20of%20linear%20equations%20with%20invertible%20coefficient/#adjugate-of-a-2-times-2-matrix","title":"Adjugate of a \\(2 \\times 2\\) matrix","text":"<ul> <li>\\(A = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix}\\)</li> <li>Calculate the cofactor matrix of \\(A\\).</li> <li>\\(C = \\begin{bmatrix} 4 &amp; -3 \\\\ -2 &amp; 1 \\end{bmatrix}\\)</li> <li>Now, calculate the transpose of \\(C\\).</li> <li>\\(C^T = \\begin{bmatrix} 4 &amp; -2 \\\\ -3 &amp; 1 \\end{bmatrix}\\)</li> <li>The final step is to divide the transpose of the cofactor matrix by the determinant of the original matrix.</li> <li>\\(A^* = \\frac{1}{det(A)} \\times C^T \\times A\\)</li> <li>\\(A^* = \\frac{1}{-2} \\times \\begin{bmatrix} 4 &amp; -2 \\\\ -3 &amp; 1 \\end{bmatrix} \\times \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix}\\)</li> <li>\\(A^* = \\frac{1}{-2} \\times \\begin{bmatrix} 4 &amp; -2 \\\\ -3 &amp; 1 \\end{bmatrix}\\)</li> <li>\\(A^* = \\begin{bmatrix} -2 &amp; 1 \\\\ 1.5 &amp; -0.5 \\end{bmatrix}\\)</li> </ul>"},{"location":"MATHS2/2.02%20-%20Solutions%20to%20a%20system%20of%20linear%20equations%20with%20invertible%20coefficient/#adjugate-of-a-3-times-3-matrix","title":"Adjugate of a \\(3 \\times 3\\) matrix","text":"<ul> <li>\\(A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 2 &amp; 8 \\\\ 5 &amp; 6 &amp; 0 \\end{bmatrix}\\)</li> <li>Calculate the cofactor matrix of \\(A\\).</li> <li>\\(C = \\begin{bmatrix} -48 &amp; 40 &amp; -10 \\\\ 18 &amp; -15 &amp; 4 \\\\ 10 &amp; -8 &amp; 2 \\end{bmatrix}\\)</li> <li>Now, calculate the transpose of \\(C\\).</li> <li>\\(C^T = \\begin{bmatrix} -48 &amp; 18 &amp; 10 \\\\ 40 &amp; -15 &amp; -8 \\\\ -10 &amp; 4 &amp; 2 \\end{bmatrix}\\)</li> <li>The final step is to divide the transpose of the cofactor matrix by the determinant of the original matrix.</li> <li> <p>Formula for the adjugate of a matrix:   \\(\\(A^* = \\frac{1}{det(A)} \\times C^T \\times A\\)\\)</p> </li> <li> <p>\\(A^* = \\frac{1}{2} \\times \\begin{bmatrix} -48 &amp; 18 &amp; 10 \\\\ 40 &amp; -15 &amp; -8 \\\\ -10 &amp; 4 &amp; 2 \\end{bmatrix} \\times \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 2 &amp; 8 \\\\ 5 &amp; 6 &amp; 0 \\end{bmatrix}\\)</p> </li> <li>\\(A^* = \\begin{bmatrix} -24 &amp; 9 &amp; 5 \\\\ 20 &amp; -7.5 &amp; -4 \\\\ -5 &amp; 2 &amp; 1 \\end{bmatrix} \\times \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 2 &amp; 8 \\\\ 5 &amp; 6 &amp; 0 \\end{bmatrix}\\)</li> <li>\\(A^* = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</li> </ul>"},{"location":"MATHS2/2.02%20-%20Solutions%20to%20a%20system%20of%20linear%20equations%20with%20invertible%20coefficient/#formula-for-the-adjugate-of-a-matrix","title":"Formula for the Adjugate of a matrix","text":"\\[ \\frac{1}{det(A)} \\times \\sum^{n}_{j\\ = \\ 1} a_{ij} \\times C\\_{kj}\\]"},{"location":"MATHS2/2.02%20-%20Solutions%20to%20a%20system%20of%20linear%20equations%20with%20invertible%20coefficient/#properties-of-the-adjugate-of-a-matrix","title":"Properties of the Adjugate of a matrix","text":"<ul> <li>\\(adj(AB) = adj(B) \\times adj(A)\\)</li> <li>\\(adj(AB) = adj(B) \\times adj(A)\\)</li> <li>\\(adj(A+B) = adj(A) + adj(B)\\)</li> <li>\\(adj(A^T) = adj(A)^T\\)</li> <li>\\(adj(A^{-1}) = adj(A)^{-1}\\)</li> </ul>"},{"location":"MATHS2/2.02%20-%20Solutions%20to%20a%20system%20of%20linear%20equations%20with%20invertible%20coefficient/#solution-of-a-system-of-linear-equations-with-invertible-coefficient-matrix","title":"Solution of a system of linear equations with invertible coefficient matrix","text":"<ul> <li>Consider the system of linear equations \\(Ax = b\\) where the coefficient matrix \\(A\\) is an invertible matrix.</li> <li>Multiplying both sides by we obtain \\(A^{-1}\\) we obtain:</li> <li>\\(Ax = b\\)</li> <li>\\(A^{-1}Ax = A^{-1}b\\)</li> <li>\\(I_n x = A^{-1}b\\)</li> <li>\\(x = A^{-1}b\\)</li> </ul>"},{"location":"MATHS2/2.02%20-%20Solutions%20to%20a%20system%20of%20linear%20equations%20with%20invertible%20coefficient/#properties-of-systems-of-linear-equations","title":"Properties of systems of linear equations","text":"<ul> <li>A system of linear equations \\(Ax = b\\) is called a non-homogeneous system of linear equations if \\(b \\neq 0.\\)</li> <li>If \\(v\\) is a solution of the system of linear equations \\(Ax = b\\), then \\(\\frac{1}{c}v\\) is a solutions of linear equation \\(cAx = b\\). where \\(c \\neq 0\\).</li> <li>Let \\(Ax = b\\) be a system of linear equations. If \\(A\\) is invertible, then \\(adj(A)x = b\\) also has a solution.</li> </ul>"},{"location":"MATHS2/2.02%20-%20Solutions%20to%20a%20system%20of%20linear%20equations%20with%20invertible%20coefficient/#the-inverse-of-a-matrix","title":"The Inverse of a matrix","text":"<ul> <li>The inverse of a matrix \\(A\\) is a matrix \\(B\\) such that \\(AB = BA = I\\).</li> <li>The inverse of a matrix \\(A\\) is denoted by \\(A^{-1}\\).</li> <li>We can find the inverse of a matrix by using the adjugate of the matrix.</li> <li>\\(A^{-1} = \\frac{1}{det(A)} \\times adj(A)\\)</li> </ul>"},{"location":"MATHS2/2.03%20-%20The%20Echelon%20form/","title":"Echelon Form","text":""},{"location":"MATHS2/2.03%20-%20The%20Echelon%20form/#definition","title":"Definition","text":"<ul> <li>A matrix is in echelon form if it is in row echelon form and has all leading entries in the first column, second column, and so on.</li> </ul>"},{"location":"MATHS2/2.03%20-%20The%20Echelon%20form/#row-echelon-form","title":"Row Echelon Form","text":"<ul> <li>The first non-zero element in each row, called the leading entry, is \\(1\\).</li> <li>Each leading entry is in a column to the right of the leading entry in the previous row.</li> <li>Rows with zero elements, if any, are below rows having a non-zero element.</li> <li>For a non-zero row, the leading entry in the row is the only non-zero entry in the column.</li> <li>Suppose for some \\(i\\), $i^{th} row of \\(A\\) is a zero row but \\(b_i \\neq 0\\).</li> <li>The system of equations \\(Ax=b\\) has no solution.</li> <li>Reason: This means if we write the corresponding system of linear equations, the \\(i^{th}\\) equation is<ul> <li>\\(0x_1 + 0x_2 + \\cdots + 0x_n = b_i\\).</li> <li>This solution is not possible.</li> </ul> </li> </ul>"},{"location":"MATHS2/2.03%20-%20The%20Echelon%20form/#example","title":"Example","text":"<ul> <li>\\(A_{ref} = \\begin{bmatrix}       1 &amp; 2 &amp; 3 &amp; 4 \\\\       0 &amp; 0 &amp; 1 &amp; 3 \\\\       0 &amp; 0 &amp; 0 &amp; 1 \\\\       \\end{bmatrix}\\)</li> <li>\\(A\\) is in row echelon form.</li> <li>Converting \\(A\\) to reduced row echelon form:</li> <li>\\(A_{rref} = \\begin{bmatrix}       1 &amp; 2 &amp; 0 &amp; 0 \\\\       0 &amp; 0 &amp; 1 &amp; 0 \\\\       0 &amp; 0 &amp; 0 &amp; 1 \\\\       \\end{bmatrix}\\)</li> <li>\\(A\\) is in reduced row echelon form.</li> <li>Soluting the system of equations:</li> <li>Let \\(Ax=b\\) where \\(A\\) is in the form of \\(A_{rref}\\).</li> <li>Suppose for some \\(i\\), \\(a_{ii} = 0\\).</li> </ul>"},{"location":"MATHS2/2.03%20-%20The%20Echelon%20form/#dependency","title":"Dependency","text":"<ul> <li>Let \\(Ax=b\\) where \\(A\\) is in the form of \\(A_{rref}\\).</li> <li>Assume that for every zero row of \\(A\\), \\(b_i = 0\\).</li> <li>If the \\(i^{th}\\) column has the leading entry of some row, we call \\(x_i\\) a dependent variable.</li> <li>If the \\(i^{th}\\) column has no leading entry, we call \\(x_i\\) an independent variable.</li> </ul>"},{"location":"MATHS2/2.03%20-%20The%20Echelon%20form/#example_1","title":"Example","text":"<ul> <li>\\(A = \\begin{bmatrix}       1 &amp; 2 &amp; 3 &amp; 4 \\\\       0 &amp; 0 &amp; 1 &amp; 3 \\\\       0 &amp; 0 &amp; 0 &amp; 1 \\\\       \\end{bmatrix}\\)</li> <li>\\(A\\) is in reduced row echelon form.</li> <li>In this case, \\(x_1\\), \\(x_3\\) and \\(x_4\\) are dependent variables.</li> <li>and \\(x_2\\) is an independent variable.</li> </ul>"},{"location":"MATHS2/2.04%20-%20Row%20Reduction/","title":"Elementry Row Operations","text":""},{"location":"MATHS2/2.04%20-%20Row%20Reduction/#operations","title":"Operations","text":"TYPE ACTION EXAMPLE AND NOTATION Description 1 Interchange two rows \\(R_1 \\leftrightarrow R_2\\) Interchanging \\(R_1\\) and \\(R_2\\) 2 Scalar multiplication of a row by constant \\(t\\) \\(kR_1 \\rightarrow R_1\\) Multiplying Row 1 with constant \\(k\\) 3 Add a multiple of one row to another row \\(R_1 (+/-) kR_2\\) Adding or subtracting \\(k \\times R_2\\) from \\(R_1\\)"},{"location":"MATHS2/2.04%20-%20Row%20Reduction/#what-are-these-operations-used-for","title":"What are these operations used for?","text":""},{"location":"MATHS2/2.04%20-%20Row%20Reduction/#row-reduction-row-echelon-form","title":"Row Reduction: Row Echelon Form","text":"<ul> <li>Row reduction is a sequence of elementry row operations that transforms a matrix into row echelon form.</li> <li>\\(A = \\begin{bmatrix}       3 &amp; 2 &amp; 1 &amp; 1 \\\\       1 &amp; 1 &amp; 0 &amp; 0 \\\\       0 &amp; 7 &amp; 1 &amp; 1 \\\\       \\end{bmatrix}\\)</li> <li>To get the \\(rref\\) of the matrix \\(A\\) we need to reduce the first element to \\(1\\). This can be done by multiplying the first row by \\(1/3\\).</li> <li>So the first operation is \\(R_1/3\\).</li> <li>\\(A = \\begin{bmatrix}       1 &amp; 2/3 &amp; 1/3 &amp; 1/3 \\\\       1 &amp; 1 &amp; 0 &amp; 0 \\\\       0 &amp; 7 &amp; 1 &amp; 1 \\\\       \\end{bmatrix}\\)</li> <li>Now we need to make the second element of the second row \\(0\\). This can be done by subtracting \\(1\\) times the first row from the second row.</li> <li>So the second operation is \\(R_2 - R_1\\).</li> <li>\\(A = \\begin{bmatrix}       1 &amp; 2/3 &amp; 1/3 &amp; 1/3 \\\\       0 &amp; 1/3 &amp; -1/3 &amp; -1/3 \\\\       0 &amp; 7 &amp; 1 &amp; 1 \\\\       \\end{bmatrix}\\)</li> <li>Now we want to normalize the second row. This can be done by multiplying the second row by \\(3\\).</li> <li>So the third operation is \\(3R_2\\).</li> <li>\\(A = \\begin{bmatrix}       1 &amp; 2/3 &amp; 1/3 &amp; 1/3 \\\\       0 &amp; 1 &amp; -1 &amp; -1 \\\\       0 &amp; 7 &amp; 1 &amp; 1 \\\\       \\end{bmatrix}\\)</li> <li>Now we want to make the third element of the third row \\(0\\). This can be done by subtracting \\(7\\) times the second row from the third row.</li> <li>So the fourth operation is \\(R_3 - 7R_2\\).</li> <li>\\(A = \\begin{bmatrix}       1 &amp; 2/3 &amp; 1/3 &amp; 1/3 \\\\       0 &amp; 1 &amp; -1 &amp; -1 \\\\       0 &amp; 0 &amp; 8 &amp; 8 \\\\       \\end{bmatrix}\\)</li> <li>Now we want to normalize the third row. This can be done by multiplying the third row by \\(1/8\\).</li> <li>So the fifth operation is \\(R_3/8\\).</li> <li>\\(A = \\begin{bmatrix}       1 &amp; 2/3 &amp; 1/3 &amp; 1/3 \\\\       0 &amp; 1 &amp; -1 &amp; -1 \\\\       0 &amp; 0 &amp; 1 &amp; 1 \\\\       \\end{bmatrix}\\)</li> <li>Now we want to set all the values above the leading one to \\(0\\). This can be done by subtracting \\(1\\) times the third row from the first row.</li> <li>So the sixth operation is \\(R_2 - R_3 \\text{ and } R_1 - \\frac{1}{3}R_3\\).</li> <li>\\(A = \\begin{bmatrix}       1 &amp; 2/3 &amp; 0 &amp; 0 \\\\       0 &amp; 1 &amp; 0 &amp; 0 \\\\       0 &amp; 0 &amp; 1 &amp; 1 \\\\       \\end{bmatrix}\\)</li> <li>Final Step is remove the leading zeros. This can be done by subtracting \\(2/3\\) times the second row from the first row.</li> <li>So the seventh operation is \\(R_1 - \\frac{2}{3}R_2\\).</li> <li>\\(A = \\begin{bmatrix}       1 &amp; 0 &amp; 0 &amp; 0 \\\\       0 &amp; 1 &amp; 0 &amp; 0 \\\\       0 &amp; 0 &amp; 1 &amp; 1 \\\\       \\end{bmatrix}\\)</li> <li>So we used the Row Operations to get the \\(rref\\) of the matrix \\(A\\).</li> </ul>"},{"location":"MATHS2/2.04%20-%20Row%20Reduction/#computing-the-determinant-of-a-matrix","title":"Computing the determinant of a matrix","text":"<ul> <li>\\(A = \\begin{bmatrix}       2 &amp; 4 &amp; 1  \\\\       3 &amp; 8 &amp; 7  \\\\       5 &amp; 6 &amp; 9  \\\\       \\end{bmatrix}\\)</li> <li>\\(R_1/2\\)</li> <li>\\(A_1 = \\begin{bmatrix}       1 &amp; 2 &amp; 1/2  \\\\       3 &amp; 8 &amp; 7  \\\\       5 &amp; 6 &amp; 9  \\\\       \\end{bmatrix}\\)</li> <li>\\(R_2 - 3R_1, R_2 - 5R_1\\)</li> <li>\\(A = \\begin{bmatrix}       1 &amp; 2 &amp; 1/2  \\\\       0 &amp; 2 &amp; 11/2  \\\\       0 &amp; -4 &amp; 13/2  \\\\       \\end{bmatrix}\\)</li> <li>\\(R_2/2\\)</li> <li>\\(A = \\begin{bmatrix}       1 &amp; 2 &amp; 1/2  \\\\       0 &amp; 1 &amp; 11/4  \\\\       0 &amp; -4 &amp; 13/2  \\\\       \\end{bmatrix}\\)</li> <li>\\(R_3 - 4R_2\\)</li> <li>\\(A = \\begin{bmatrix}       1 &amp; 2 &amp; 1/2  \\\\       0 &amp; 1 &amp; 11/4  \\\\       0 &amp; 0 &amp; 1  \\\\       \\end{bmatrix}\\)</li> </ul>"},{"location":"MATHS2/2.05%20-%20Gaussian%20elemination/","title":"Augmented Matrix","text":"<ul> <li>Let \\(Ax = b\\) be a system of linear equations where \\(A\\) is an \\(m \\times n\\) matrix and \\(b\\) is an \\(m \\times 1\\) matrix.</li> <li>The augmented matrix of \\(A\\) and \\(b\\) is the \\(m \\times (n+1)\\) matrix \\([A|b]\\).</li> </ul>"},{"location":"MATHS2/2.05%20-%20Gaussian%20elemination/#example","title":"Example","text":"<ul> <li>System of linear equations:</li> <li>\\(3x_1 + 2x_2 + x_3 + x_4 = 6\\)</li> <li>\\(x_1 +x_2 = 2\\)</li> <li>7x_2 + x_3 + x_4 = 8$</li> <li>Matrix \\(A\\):<ul> <li>\\(A = \\begin{bmatrix}       3 &amp; 2 &amp; 1 &amp; 1 \\\\       1 &amp; 1 &amp; 0 &amp; 0 \\\\       0 &amp; 7 &amp; 1 &amp; 1 \\\\       \\end{bmatrix}\\)</li> </ul> </li> <li>Matrix \\(b\\):</li> <li>\\(b = \\begin{bmatrix}         6 \\\\         2 \\\\         8 \\\\         \\end{bmatrix}\\)</li> <li>Augmented matrix \\([A|b]\\):</li> <li>\\([A|b] = \\begin{bmatrix}         3 &amp; 2 &amp; 1 &amp; 1| &amp; 6 \\\\         1 &amp; 1 &amp; 0 &amp; 0| &amp; 2 \\\\         0 &amp; 7 &amp; 1 &amp; 1|&amp; 8 \\\\         \\end{bmatrix}\\)</li> </ul>"},{"location":"MATHS2/2.05%20-%20Gaussian%20elemination/#gaussian-elimination","title":"Gaussian Elimination","text":"<ul> <li>Gaussian elimination is a method for solving a system of linear equations.</li> </ul>"},{"location":"MATHS2/2.05%20-%20Gaussian%20elemination/#algorithm","title":"Algorithm","text":"<ul> <li>Form the augmented matrix \\([A|b]\\).</li> <li>Perform the following steps until the augmented matrix is in reduced row echelon form.</li> <li>Apply elementary row operations on both sides of the augmented matrix.</li> <li>It is ok if the \\(b\\) column is not in reduced row echelon form.</li> <li>Let \\(R\\) be the submatrix of the obtained matrix of the frist \\(n\\) columns and \\(c\\) be the submatrix of the obtained matrix consisting of the last column.</li> <li>We write the obtained matrix as \\([R|c]\\).</li> <li>The solutions of \\(Ax=b\\) are precisely the values of solutions of \\(Rx=c\\).</li> </ul>"},{"location":"MATHS2/2.05%20-%20Gaussian%20elemination/#notes","title":"Notes","text":"<ul> <li>If there are zero rows in the augmented matrix, then the system has no solutions.</li> </ul>"},{"location":"MATHS2/2.05%20-%20Gaussian%20elemination/#example_1","title":"Example","text":"<ul> <li>\\(A = \\begin{bmatrix}       3 &amp; 2 &amp; 1 &amp; 1 &amp; |6 \\\\       1 &amp; 1 &amp; 0 &amp; 0 &amp; |2 \\\\       0 &amp; 7 &amp; 1 &amp; 1&amp; |8 \\\\       \\end{bmatrix}\\)</li> <li>\\(A\\) is not in reduced row echelon form.</li> <li>\\(R_1/3\\):</li> <li>\\(A = \\begin{bmatrix}         1 &amp; \\frac{2}{3} &amp; \\frac{1}{3} &amp; \\frac{1}{3} &amp; |2 \\\\         1 &amp; 1 &amp; 0 &amp; 0 &amp; |2 \\\\         0 &amp; 7 &amp; 1 &amp; 1 &amp; |8 \\\\         \\end{bmatrix}\\)</li> <li> <p>\\(R_2 - R_1\\):</p> </li> <li> <p>$A = \\begin{bmatrix}     1 &amp; \\frac{2}{3} &amp; \\frac{1}{3} &amp; \\frac{1}{3} &amp;</p> <pre><code>|2 \\\\\n0 &amp; \\frac{1}{3} &amp; -\\frac{1}{3} &amp; -\\frac{1}{3} &amp; |0 \\\\\n0 &amp; 7 &amp; 1 &amp; 1&amp; |8 \\\\\n\\end{bmatrix}$\n</code></pre> </li> <li> <p>\\(3R_2\\):</p> </li> <li>\\(A = \\begin{bmatrix}         1 &amp; \\frac{2}{3} &amp; \\frac{1}{3} &amp; \\frac{1}{3} &amp; |2 \\\\         0 &amp; 1 &amp; -1 &amp; -1 &amp; |0 \\\\         0 &amp; 7 &amp; 1 &amp; 1&amp; |8 \\\\         \\end{bmatrix}\\)</li> <li>\\(R_3 - 7R_2\\):</li> <li>\\(A = \\begin{bmatrix}         1 &amp; \\frac{2}{3} &amp; \\frac{1}{3} &amp; \\frac{1}{3} &amp; |2 \\\\         0 &amp; 1 &amp; -1 &amp; -1 &amp; |0 \\\\         0 &amp; 0 &amp; 8 &amp; 8 &amp; |8 \\\\         \\end{bmatrix}\\)</li> <li>\\(R_3/8\\):</li> <li>\\(A = \\begin{bmatrix}         1 &amp; \\frac{2}{3} &amp; \\frac{1}{3} &amp; \\frac{1}{3} &amp; |2 \\\\         0 &amp; 1 &amp; -1 &amp; -1 &amp; |0 \\\\         0 &amp; 0 &amp; 1 &amp; 1 &amp; |1 \\\\         \\end{bmatrix}\\)</li> <li>\\(R_2 + R_3, R_1 - R_3/3\\)</li> <li>\\(A = \\begin{bmatrix}         1 &amp; \\frac{2}{3} &amp; 0 &amp; 0 &amp; |\\frac{5}{3} \\\\         0 &amp; 1 &amp; 0 &amp; 0 &amp; |1 \\\\         0 &amp; 0 &amp; 1 &amp; 1 &amp; |1 \\\\         \\end{bmatrix}\\)</li> <li>\\(R_1 - 2/3R_2:\\)</li> <li>\\(A = \\begin{bmatrix}         1 &amp; 0 &amp; 0 &amp; 0 &amp; |1 \\\\         0 &amp; 1 &amp; 0 &amp; 0 &amp; |1 \\\\         0 &amp; 0 &amp; 1 &amp; 1 &amp; |1 \\\\         \\end{bmatrix}\\)</li> </ul>"},{"location":"MATHS2/2.05%20-%20Gaussian%20elemination/#homogeneous-system-of-linear-equations","title":"Homogeneous System of linear equations","text":"<ul> <li>0 is always a solution of a homogenous system of linear equations.</li> <li>\\(Ax = 0\\) is called a trivial solution.</li> <li>For a homogenous system, there are always \\(2\\) types of possible outcomes:</li> <li>0 is the unqiue solution.</li> <li>There are infinitely many solutions other than 0.</li> <li>In a homogenous system, if there are more variables than equations, then it is guaranteed to have non-trivial solutions.</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/","title":"Vectors","text":"<ul> <li>Consider two vectors \\((x_1, x_2, \\cdots , x_n)\\) and \\((y_1, y_2, \\cdots , y_n)\\) in \\(\\mathbb{R}^n\\) and \\(c \\in \\mathbb{R}\\).</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#properties-of-vectors","title":"Properties of Vectors","text":"<p>Let \\(v, w\\) and \\(v'\\) be vectors in \\(\\mathbb{R}^n\\).</p> <ul> <li>\\(v + w = w + v\\)</li> <li>\\((v + w) + v' = v + (w + v')\\)</li> <li>The \\(0\\) vector satisfies that \\(v + 0 = 0 + v = v\\).</li> <li>The vector \\(-v\\) satisfies that \\(v + (-v) = (-v) + v = 0\\).</li> <li>\\(1 \\cdot v = v\\).</li> <li>\\((ab)v = a(bv)\\).</li> <li>\\(a(v + w) = av + aw\\).</li> <li>\\((a + b)v = av + bv\\).</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#vector-spaces","title":"Vector Spaces","text":"<ul> <li>A vector space is a set with two operations (called addition and scalar multiplication with the first and last properties mentioned above).</li> <li> <p>A vector space \\(V\\) over \\(\\mathbb{R}\\) is a set along with two functions   \\(\\(+ : V \\times V \\rightarrow V \\text{ and } \\cdot : \\mathbb{R} \\times V \\rightarrow V\\)\\)</p> </li> <li> <p>It is standard to suppress the \\(\\cdot\\)</p> </li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#defination","title":"Defination","text":"<ul> <li>\\(v_1+ v_2\\) for all \\(v_1, v_2 \\in V\\).</li> <li>\\((v_1 + v_2) + v_3 = v_1 + (v_2 + v_3)\\) for all \\(v_1, v_2, v_3 \\in V\\).</li> <li>There exists a vector \\(0 \\in V\\) such that \\(v + 0 = 0 + v = v\\) for all \\(v \\in V\\).</li> <li>For each element \\(v \\in V\\) there exists an element in \\(v' \\in V\\) such that \\(v + v' = v' + v = 0\\).</li> <li>For each ement in \\(v \\in V\\) , \\(1 \\cdot v = v\\).</li> <li>For each pair of elements \\(a, b \\in \\mathbb{R}\\) and \\(v \\in V\\), \\((ab)v = a(bv)\\).</li> <li>For each element \\(a \\in \\mathbb{R}\\) and each pair of ements \\(v_1\\) and \\(v_2\\) a(\\(v_1 + v_2) = av_1 + av_2\\).</li> <li>For each pair of elements \\(a,b \\in \\mathbb{R}\\) and each element \\(v \\in V\\), \\((a + b)v = av + bv\\).</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#examples-matricies","title":"Examples : Matricies","text":"<ul> <li>Let \\(M_{m \\times n}(\\mathbb{R})\\) be the set of all \\(m \\times n\\) matrices with entries in \\(\\mathbb{R}\\).</li> <li>Addition and scalar multiplication are defined as usual:</li> <li>\\((A + B)_{ij} = A_{ij} + B_{ij}\\)</li> <li>\\((cA)_{ij} = cA_{ij}\\)</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#examples-solutions-of-homogeneous-linear-equations","title":"Examples : Solutions of homogeneous linear equations","text":"<ul> <li>Consider the set of solutions \\(V\\) of a homogenous linear equation \\(Ax = 0\\) where \\(A \\in M_{m \\times n}(\\mathbb{R})\\)</li> <li>Nothat that if \\(v, w \\in V\\) then   $$A(v+w) = Av + Aw = 0 + 0 = 0 $$</li> <li>and if \\(c \\in \\mathbb{R}\\) then   \\(\\(A(c \\cdot v) = c(A \\cdot v) = c(0) = 0\\)\\)</li> </ul> <ul> <li>So addition and scalar multiplication on restricts to the solution set. Hence it is a vector space.</li> </ul> <p>This is an example of a subs ace of a vector space.</p>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#non-examples","title":"Non - Examples","text":"<ul> <li>\\((x_1, x_2) + (y_1, y_2) = (x_1 + y_1, x_2 + y_2)\\) is not a vector space. It is not closed under scalar multiplication.</li> <li>\\(c(x_1, x_2) = (cx_1, 0)\\) is not a vector space. It is not closed under addition. it fails under the \\(3^{rd}\\), \\(4^{th}\\) and \\(5^{th}\\) properties.</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#properties-of-vector-spaces","title":"Properties of Vector Spaces","text":"<ul> <li>\\(V\\) is a vector space if and only if it is closed under addition and scalar multiplication.</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#cancelation-law-of-addition","title":"Cancelation Law of Addition","text":"<ul> <li>If \\(v_1, v_2, v_3 \\in V\\) and \\(v_1 + v_3 = v_2 + v_3\\) then \\(v_1 = v_2\\). This is called the cancelation law of addition.</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#linear-dependence","title":"Linear Dependence","text":"<ul> <li>A set of vectors VI, v2, , Vn from a vector space \\(V\\) linearly dependent if there exists scalars \\(a_1, a_2, \\dots , a_n,\\) such that is said to be not all zero.   \\(\\(a_1v_1 + a_2v_2 + \\cdots + a_nv_n = 0\\)\\)</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#linear-independence","title":"Linear Independence","text":"<ul> <li>A set of vectors \\(v_1, v_2, \\dots , v_n\\) from a vector space \\(V\\) is said to be linearly independent if no scalar \\(a_1, a_2, \\dots , a_n\\) can be found such that all are zero.</li> </ul> \\[a_1v_1 + a_2v_2 + \\cdots + a_nv_n = 0\\]"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#example","title":"Example","text":"<ul> <li>Consider the two vectors \\((-1,3)\\) and (2,0) in \\(\\mathbb{R}^2\\).</li> <li>Consider the following equation:</li> <li>\\(a(-1,3) + b(2,0) = 0\\)</li> <li>Hence the following linear equation is satisfied:</li> <li>\\(a(-1) + b(2) = 0\\)</li> <li>\\(a(3) + b(0) = 0\\)</li> <li>Hence \\(a = 0\\) and \\(b = 0\\) is the only</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#the-0-vector","title":"The 0 Vector","text":"<ul> <li>Let \\(v_1, v_2, \\dots , v_n\\) be a set of vectors containing the zero vector \\(0\\).</li> <li>Suppose \\(v_i = 0\\). Then we can choose \\(a_i = 1\\) and \\(a_j = 0\\) for \\(j \\neq i\\).</li> <li>Then the linear combination \\(a_1v_1 + a_2v_2 + \\cdots + a_nv_n = 0\\) is satisfied.</li> <li>Hence, a set of vectors \\(v_1, v_2, \\dots , v_n\\) containing the \\(0\\) vector is always linearly dependent.   1</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#when-are-two-non-zero-vectors-linearly-independent","title":"When are two non-zero vectors linearly independent?","text":"<ul> <li>Let \\(v_1, v_2\\) be two non-zero vectors in \\(\\mathbb{R}^n\\).</li> <li>Suppose \\(v_1\\) and \\(v_2\\) are linear dependent.</li> <li>Then \\(a_1v_1 + a_2v_2 = 0\\) for some \\(a_1, a_2 \\in \\mathbb{R}\\). and atleast one of \\(a_1, a_2\\) is not zero.</li> <li>Dividing by \\(a_1\\) and putting \\(c = -\\frac{a_2}{a_1}\\) we get   \\(\\(v_1 + cv_2 = 0\\)\\)</li> <li>we get \\(v_1 = cv_2\\).</li> <li>Hence \\(v_1\\) is a scalar multiple of \\(v_2\\).</li> <li> <p>We can reverse the implications above and conclude that if \\(v_1\\) and \\(v_2\\) are multiples of each other then they are linearly dependent.</p> </li> <li> <p>If \\(v_1\\) and \\(v_2\\) are linearly independent when \\(v_1\\) and \\(v_2\\) are not multiples of each other.</p> </li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#linear-independence-of-three-vectors","title":"Linear Independence of three Vectors","text":"<ul> <li>Let \\(v_1, v_2, v_3\\) be a set of vectors in \\(\\mathbb{R}^n\\).</li> <li>Then \\(a_1v_1 + a_2v_2 + a_3v_3 = 0\\) for some \\(a_1, a_2, a_3 \\in \\mathbb{R}\\) and atleast one of \\(a_1, a_2, a_3\\) is not zero.</li> <li>If \\(a_1 = 0\\) then \\(v_1 = b_2v_2 + b_3v_3\\), where. \\(b_2 = -\\frac{a_2}{a_1}\\) and \\(b_3 = -\\frac{a_3}{a_1}\\). Hence \\(v_1\\) is a linear combination of \\(v_2\\) and \\(v_3\\).</li> <li>Similarly if \\(a_2 \\neq 0\\)</li> <li>Since the implication is true in both directions, we conclude that if \\(v_1, v_2, v_3\\) are linearly independent then \\(v_1\\) is not a linear combination of \\(v_2\\) and \\(v_3\\).</li> <li>Concusion: If \\(v_1, v_2, v_3\\) are linearly independent then non of these vectors is a linear combination of the other two.</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#example_1","title":"Example","text":"<ul> <li>Let us consider the vectors \\(v_1 = (1,1,2)\\), \\(v_2 = (1,2,0)\\) and \\(v_3 = (0,2,1)\\) in \\(\\mathbb{R}^3\\).</li> </ul> <p>\\(\\(a(1,1,2) + b(1,2,0) + c(0,2,1) = 0\\)\\)</p> <ul> <li> <p>We get the following equations:</p> </li> <li> <p>\\(a + b = 0\\)</p> </li> <li>\\(a + 2b + 2c= 0\\)</li> <li> <p>\\(2a + c = 0\\)</p> </li> <li> <p>We get \\(a = 0\\), \\(b = 0\\) and \\(c = 0\\).</p> </li> <li>Hence \\(v_1, v_2, v_3\\) are linearly dependent.</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#linear-independence-of-n-vectors","title":"Linear Independence of n Vectors","text":"<ul> <li>Let \\(v_1, v_2, \\dots , v_n\\) be a set of vectors in \\(\\mathbb{R}^m\\).</li> <li>In terms of coordinates, let \\(v_j = (v_{j1}, v_{j2}, \\dots , v_{jm})\\).</li> <li>Let us write the linear combination of these vectors with arbitary coefficients \\(a_1, a_2, \\dots , a_n\\) as:   \\(\\(a_1v_1 + a_2v_2 + \\cdots + a_nv_n = 0\\)\\)</li> <li>We get the following equations:</li> <li>\\(a_1v_{11} + a_2v_{12} + \\cdots + a_nv_{1n} = 0\\)</li> <li>\\(a_1v_{21} + a_2v_{22} + \\cdots + a_nv_{2n} = 0\\)</li> <li>\\(\\vdots\\)</li> <li>\\(a_1v_{n1} + a_2v_{n2} + \\cdots + a_nv_{nm} = 0\\)</li> <li>For linear independence, we need to find a set of coefficients \\(a_1, a_2, \\dots , a_n\\) such that all the equations are satisfied.</li> <li>We need to check if the only choice of \\(a_i\\)'s satisfying the above identities is \\(a_i = 0\\) for all \\(i\\).</li> <li>Concusion: If \\(v_1, v_2, \\dots , v_n \\in \\mathbb{R}^m\\) are linearly independent, we have to check that the homogeneous system of linear eqautions \\(V_x=0\\) has only trivial solution, where \\(j^{th}\\) column of \\(V\\) is \\(v_j\\).</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#example_2","title":"Example","text":""},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#2x2-matrix","title":"2x2 Matrix","text":"<ul> <li>Consider the two vectors \\((5,2) \\text{ and } (1,3)\\) in \\(\\mathbb{R}^2\\). Write the linear combination of these vectors with arbitary coefficients \\(a_1\\) and \\(a_2\\) as:   \\(\\(a_1(5,2) + a_2(1,3) = 0\\)\\)</li> <li>We get the following equations:</li> <li>\\(5x + y = 0\\)</li> <li>\\(2x + 3y = 0\\)</li> <li>Since the corresponding matrix is:   \\(\\begin{bmatrix} 5 &amp; 1 \\\\ 2 &amp; 3 \\end{bmatrix}\\) is invertible, the system of linear equations has only trivial solution.</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#3x2-matrix","title":"3x2 Matrix","text":"<ul> <li>Consider the two vectors \\((1,2,0) \\text{ and } (3,3,5)\\) in \\(\\mathbb{R}^3\\). Write the linear combination of these vectors with arbitary coefficients \\(a_1\\) and \\(a_2\\) as:   \\(\\(a_1(1,2,0) + a_2(3,3,5) = 0\\)\\)</li> <li>We get the following equations:</li> <li>\\(x + 3y = 0\\)</li> <li>\\(2x + 3y = 0\\)</li> <li>\\(5y = 0\\)</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#2x3-matrix","title":"2x3 Matrix","text":"<ul> <li>Consider the three vectors \\((1,2), (1,3) \\text{ and } (3,4)\\). Equate the linear combination of these vectors with arbitary coefficients \\(a_1, a_2, a_3\\) as:   \\(\\(a_1(1,2) + a_2(1,3) + a_3(3,4) = 0\\)\\)</li> <li>We get the following equations:</li> <li>\\(1x + 1y + 3z = 0\\)</li> <li>2x + 3y + 4z = 0$</li> <li>We can find the values using gaussian elimination.</li> <li>\\(\\begin{bmatrix} 1 &amp; 1 &amp; 3 \\\\ 2 &amp; 3 &amp; 4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} 1 &amp; 1 &amp; 3 \\\\ 0 &amp; 1 &amp; -2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} 1 &amp; 1 &amp; 3 \\\\ 0 &amp; 1 &amp; -2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)</li> <li>We get infinite solutions. Hence the vectors are linearly dependent.</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#3x3-matrix","title":"3x3 Matrix","text":"<ul> <li>Consider the three vectors \\((1,2,0), (0,2,4) \\text{ and } (3,0,0)\\). Equate the linear combination of these vectors with arbitary coefficients \\(x, y, z\\) as:   \\(\\(x(1,2,0) + y(0,2,4) + z(3,0,0) = 0\\)\\)</li> <li>We get the following equations:</li> <li>\\(x + 2y = 0\\)</li> <li>\\(2y + 4z = 0\\)</li> <li>\\(3x = 0\\)</li> <li>Since the matrix is \\(\\begin{bmatrix} 1 &amp; 0 &amp; 3 \\\\ 2 &amp; 2 &amp; 0 \\\\ 0 &amp; 4 &amp; 0 \\end{bmatrix}\\) is invertible, the system of linear equations has only trivial solution.</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#more-than-2-vectors-in-mathbbr2","title":"More than 2 vectors in \\(\\mathbb{R}^2\\)","text":"<ul> <li>Suppose we have \\(n\\) vectors in \\(\\mathbb{R}^2\\), where \\(n \\geq 3\\). To check linear independece, we have to check wheater the corresponding homogeneous linear system \\(V_x = 0\\) has only trivial solution.</li> <li>Since \\(n \\geq 3 &gt; 2\\)</li> <li>We also know that gaussian elimination can yeild infinite solutions.</li> <li>Hence, any set of \\(n\\) vectors in \\(\\mathbb{R}^2\\) with \\(n \\geq 3\\) are linearly dependent.</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#relation-between-linear-independence-and-determinant","title":"Relation between Linear Independence and Determinant","text":"<ul> <li>To Check wheater a set of \\(n\\) vectors in \\(\\mathbb{R}^n\\) are linearly independent, we can check wheater the corresponding homogeneous linear system \\(V_x = 0\\) has only trivial solution. Where \\(V\\) is an \\(n \\times n\\) matrix obtained by arranging the vectors as columns.</li> <li>Since \\(V\\) is a square matrix, it has unique solution \\(x=0\\) if and only if \\(V\\) is invertible and it has its \\(det(V) \\neq 0\\).</li> <li>If \\(A\\) is invertible then there exists \\(A^{-1}\\) such that \\(A^{-1}A = I\\). Hence \\(det(A^{-1}A) = det(I) \\neq 0\\).</li> </ul>"},{"location":"MATHS2/3.01%20-%20Vector%20Spaces/#example_3","title":"Example","text":"<ul> <li>Let us consider the following matrix \\((1,4,2), (0,4,3) \\text{ and } (1,1,0)\\) in \\(\\mathbb{R}^3\\).</li> <li>Let the matrix be: \\(V = \\begin{bmatrix} 1 &amp; 0 &amp; 1 \\\\ 4 &amp; 4 &amp; 1 \\\\ 2 &amp; 3 &amp; 0 \\end{bmatrix}\\)</li> <li>Since the \\(det(V) = 1\\) and \\(\\neq 0\\), the system of linear equations has only trivial solution. Hence the vectors are linearly independent.</li> </ul>"},{"location":"MATHS2/4.01%20-%20Vector%20Basis/","title":"Span of a set of vectors","text":"<ul> <li>The span of a set \\(S\\) (of vectors) is defined as the set of all finite linear   combinations of elements(vectors) of \\(S\\), and denoted by \\(Span(S)\\).   \\(\\(Span(S) = \\{ \\sum_{i=1}^n a_i v_i  \\in V|a_1, a_2, \\cdots, a_n \\in \\mathbb{R}\\}\\)\\)</li> </ul>"},{"location":"MATHS2/4.01%20-%20Vector%20Basis/#example","title":"Example","text":"<ul> <li> <p>Let \\(S = \\{(1,0)\\} \\in \\mathbb{R}^2\\)</p> </li> <li> <p>\\(Span(S) = \\{a(1,0) | a \\in \\mathbb{R}\\} = \\{(a,0) | a \\in \\mathbb{R}\\}\\)</p> </li> <li> <p>Let \\(S = \\{(1,1)\\} \\in \\mathbb{R}^2\\)</p> </li> <li> <p>\\(Span(S) = \\{a(1,1) | a \\in \\mathbb{R}\\} = \\{(a,a) | a \\in \\mathbb{R}\\}\\)</p> </li> <li> <p>Let \\(S = \\{(1,0,0), (0,1,0)\\} \\in \\mathbb{R}^3\\)</p> </li> <li>\\(Span(S) = \\{a(1,0,0) + b(0,1,0) | a,b \\in \\mathbb{R}\\} = \\{(a, b, 0) | a,b \\in \\mathbb{R}\\}\\)</li> </ul>"},{"location":"MATHS2/4.01%20-%20Vector%20Basis/#spaning-set-for-a-vector-space","title":"Spaning set for a vector space","text":"<ul> <li>Let \\(V\\) be a vector space. A set \\(S\\) of vectors in \\(V\\) is called a spanning set for \\(V\\) if \\(V = Span(S)\\).</li> </ul>"},{"location":"MATHS2/4.01%20-%20Vector%20Basis/#example_1","title":"Example","text":"<ul> <li>If \\(S = \\{(1,0), (0,1)\\}\\), then \\(Span(S) = \\mathbb{R}^2\\).</li> <li>If \\(S = \\{(1,1), (0,1)\\}\\), then \\(Span(S) = \\mathbb{R}^2\\).</li> <li>If \\(S = \\{(1,0,0), (0,1,0), (0,0,1)\\}\\), then \\(Span(S) = \\mathbb{R}^3\\).</li> </ul>"},{"location":"MATHS2/4.01%20-%20Vector%20Basis/#example-adding-vectors-to-obtain-a-spanning-set-for-mathbbr3","title":"Example: Adding vectors to obtain a spanning set for \\(\\mathbb{R}^3\\)","text":"<ul> <li>Start with \\(S_0\\) to be the empty set \\(\\emptyset\\).</li> <li>Thus \\(Span(S_0) = \\{(0,0,0)\\}\\).</li> <li>We will add the vector \\((3,0,0)\\) to \\(S_0\\) to obtain \\(S_1\\).</li> <li>Now \\(Span(S_1) = \\{(3,0,0)\\}\\).</li> <li>Now we will add the vector \\((2,2,1)\\) to \\(S_1\\) to obtain \\(S_2\\).</li> <li>This does not cover the entire vector space \\(\\mathbb{R}^3\\).</li> <li>So we add the vector \\((1,3,3)\\) to \\(S_2\\) to obtain \\(S_3\\).</li> <li>So \\(Span(S_3) = \\mathbb{R}^3\\).</li> <li>\\((x,y,z) = \\frac{3x-5y+4z}{9}(3,0,0) + (y-z)(2,2,1) + \\frac{2z-y}{3}(1,3,3)\\)</li> </ul>"},{"location":"MATHS2/4.01%20-%20Vector%20Basis/#basis-of-a-vector-space","title":"Basis of a vector space","text":"<ul> <li>A basis \\(B\\) of a vector space \\(V\\) is linearly independent subset of \\(B\\) that spans \\(V\\).</li> </ul>"},{"location":"MATHS2/4.01%20-%20Vector%20Basis/#conditions-for-a-set-to-be-a-basis","title":"Conditions for a set to be a basis","text":"<ul> <li>The set \\(B\\) is linearly independent. \\(Span(B) = V\\).</li> <li>\\(B\\) is a maximal linearly independent set.</li> <li>\\(B\\) is a minimal spanning set.</li> </ul>"},{"location":"MATHS2/4.01%20-%20Vector%20Basis/#finding-a-basis-for-a-vector-space","title":"Finding a basis for a vector space","text":""},{"location":"MATHS2/4.01%20-%20Vector%20Basis/#method-1","title":"Method 1","text":"<ul> <li>Start with the \\(\u00d8\\) and keep appending vectors which are not in the span of the set thus far obtained, until we obtain a spanning set.</li> </ul>"},{"location":"MATHS2/4.01%20-%20Vector%20Basis/#example_2","title":"Example","text":"<ul> <li>Let \\(V = \\mathbb{R}^2\\).</li> <li>Let us start with the empty set \\(\\emptyset\\) and append a non-zero vector \\((1,2)\\) to it.</li> <li>Now choose another vector which is not in the span of the of the earlier vector like \\((2,3)\\).</li> </ul>"},{"location":"MATHS2/4.01%20-%20Vector%20Basis/#method-2","title":"Method 2","text":"<ul> <li>Take a spanning set and keep deleting vectors which are linear combinations of the other vectors, until the remaining vectors satisfy that they are not a linear combination of the other remaining ones.</li> </ul>"},{"location":"MATHS2/4.01%20-%20Vector%20Basis/#example_3","title":"Example","text":"<ul> <li>Let \\(V = \\mathbb{R}^3\\).</li> <li>Let us start with the spanning set \\(\\{(1,0,0), (1,2,0), (1,0,3), (0,2,3), (0,4,3)\\}\\).</li> </ul>"},{"location":"MATHS2/4.01%20-%20Vector%20Basis/#notes","title":"Notes","text":"<ul> <li>The span of a empty set is the zero vector space \\(\\{0\\}\\).</li> </ul>"},{"location":"MATHS2/4.01%20-%20Vector%20Basis/#example_4","title":"Example","text":"<ul> <li>Let \\(e_i \\in \\mathbb{R}^n\\) be the vector with \\(i^{th}\\) coordinate equal to \\(1\\) and all other coordinates equal to \\(0\\).</li> <li>The set \\(\\epsilon =  \\{e_1, e_2, \\cdots, e_n\\}\\) is a basis for \\(\\mathbb{R}^n\\).<ul> <li>\\(e_1 = (1,0,\\cdots, 0)\\)</li> <li>\\(e_2 = (0,1,\\cdots, 0)\\)</li> <li>\\(\\vdots\\)</li> <li>\\(e_n = (0,0,\\cdots, 1)\\)</li> </ul> </li> <li>\\(Span(\\epsilon) = \\mathbb{R}^n\\)</li> <li>\\(\\sum_{i=1}^n a_i e_i = (a_1, a_2, \\cdots, a_n)\\)</li> </ul>"},{"location":"MATHS2/4.02%20-%20Rank%20or%20Dimension%20of%20vector%20space/","title":"Dimension / Rank of a vector space","text":"<ul> <li>The dimension of a vector space \\(V\\) is the cardinality of a basis for \\(V\\).</li> <li>If \\(B\\) is a basis for \\(V\\), then rank of \\(V\\) is the cardinality of \\(B\\).</li> <li>For every vector space there exists a basis,and all bases of a vector space have the same number of elements (or cardinality); hence, the dimension (or rank) of a vector space (say \\(V\\)) is uniquely defined and denoted by \\(dim(V)\\) / \\(rank(V)\\) respectively.</li> </ul>"},{"location":"MATHS2/5.01%20-%20null%20space%20and%20nullity/","title":"Null Space of a Matrix","text":"<ul> <li>Let \\(A\\) be a \\(m \\times n\\) matrix</li> <li>The subspace \\(W = \\{x \\in \\mathbb{R}^n | Ax = 0\\}\\) of \\(\\mathbb{R}^n\\) is called the Solution Space of the homogenous system \\(Ax = 0\\) or the Null Space of \\(A\\).</li> <li>Note that the null space is the subspace of \\(\\mathbb{R}^n\\). The dimension of the null space is the nullity of \\(A\\).   $$ x,y \\in W \\implies Ax = Ay = 0 \\implies A(x-y) = 0 \\implies Ax + Ay = 0 + 0 = 0$$ $$ \\lambda \\in \\mathbb{R} \\implies A(\\lambda x) = \\lambda (Ax) = 0 \\implies \\lambda 0 = 0 | \\therefore \\ \\lambda \\in W$$</li> </ul>"},{"location":"MATHS2/5.01%20-%20null%20space%20and%20nullity/#finding-the-nullity-and-a-basis-for-the-null-space","title":"Finding the nullity and a basis for the null space","text":"<ul> <li>We have seen how to find the dimension and a basis for the row space of A using row reduction.</li> <li>We will use row reduction to also find the nullity and a basis for the null space of A.</li> <li> <p>First how to find the solution space for a system Ax b i.e. Gaussian elimination.</p> </li> <li> <p>Form the augmented matrix \\([A |b]\\)</p> </li> <li>Applying the elementary row operations to the augmented matrix we reduce the matrix \\(A\\) to its reduced row echelon form.and obtain \\([R | c]\\) where \\(R\\) is the reduced row echelon form of \\(A\\) and \\(c\\) is the reduced row echelon form of \\(b\\).</li> <li>If the \\(i^{th}\\) column has the leading entry of some row, we call \\(x_i\\) a dependent variable.</li> <li> <p>If the \\(i^{th}\\) column has no leading entry, we call \\(x_i\\) an independent variable.     $$ nullity(A) = \\text{number of independent variables}$$</p> </li> <li> <p>Assign arbitrary values \\(t_i\\) to the \\(i^{th}\\) independent variable.</p> </li> <li>Compute the value of each dependent variable in terms of \\(t_is\\) from the unique row it occurs in.</li> <li>Every solution is obtained by letting \\(t_is\\) be any real number.</li> </ul>"},{"location":"MATHS2/5.01%20-%20null%20space%20and%20nullity/#example","title":"Example","text":"<ul> <li>Consider the homogeneous system \\(Ax = 0\\) where \\(A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 2 &amp; 2 \\\\ 3 &amp; 3 &amp; 3 \\end{bmatrix}\\)</li> <li>The augmented matrix is \\([A | 0] = \\begin{bmatrix} 1 &amp; 1 &amp; 1 |&amp; 0 \\\\ 2 &amp; 2 &amp; 2 |&amp; 0 \\\\ 3 &amp; 3 &amp; 3 |&amp; 0 \\end{bmatrix}\\)</li> <li>Row reduce the augmented matrix to obtain \\([R | c] = \\begin{bmatrix} 1 &amp; 1 &amp; 1 |&amp; 0 \\\\ 0 &amp; 0 &amp; 0 |&amp; 0 \\\\ 0 &amp; 0 &amp; 0 |&amp; 0 \\end{bmatrix}\\)</li> <li>Hence the nullity of \\(A\\) is 2 because there are 2 independent variables.</li> <li>Put \\(x_2 = t_1\\) and \\(x_3 = t_2\\) that yeilds \\(x_1 = -t_1 - t_2\\).</li> <li>The null space of \\(A\\) is \\(\\{ (-t_1-t_2, t_1,t_2)\\}\\)</li> <li>The basis vector is \\((-1,-1,0)\\) and \\((-1,0,-1)\\)</li> <li>Consider the matrix \\(A = \\begin{bmatrix} 1 &amp; 2 &amp; 0 &amp; 3 \\\\ 2 &amp; 3 &amp; 0 &amp; 3 \\\\ 1 &amp; 1 &amp; 1 &amp; 2 \\end{bmatrix}\\)</li> <li>Applying row reductions on the matrix \\(A\\) we obtain \\([R | c] = \\begin{bmatrix} 1 &amp; 2 &amp; 0 &amp; 3 |&amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 3 |&amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 2 |&amp; 0 \\end{bmatrix}\\)</li> <li>\\(x_1,x_2 \\text{ and } x_3\\) are dependent variables and \\(x_4\\) are independent variables.</li> <li>Hence the nullity of \\(A\\) is 1. The null space of \\(A\\) is \\(\\{ (3t,-3t,-2t,t)\\}\\)</li> </ul>"},{"location":"MATHS2/5.01%20-%20null%20space%20and%20nullity/#the-rank-nullity-theorem","title":"The Rank-Nullity Theorem","text":"<ul> <li>Let \\(A\\) be a \\(m \\times n\\) matrix.</li> <li>The rank is number of linearly dependent variables in \\(A\\).</li> <li>The nullity is the number of linearly independent variables in \\(A\\).   $$ rank(A) + nullity(A) = n$$</li> </ul>"},{"location":"MATHS2/5.04%20-%20Linear%20Mapping/","title":"Linear Mapping","text":""},{"location":"MATHS2/5.04%20-%20Linear%20Mapping/#defination","title":"Defination","text":"<ul> <li>A linear mapping \\(f\\) from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\) is a function that satisfies the following two conditions:   \\(\\(f(x_1,x_2, \\dots ,x_n) = (\\sum^n_{j=1}a_{1j}x_j, \\sum^n_{j=1}a_{2j}x_j, \\dots , \\sum^n_{j=1}a_{mj}x_j)\\)\\)</li> <li>Here the coefficients \\(a_{ij}\\) are real numbers (scalars). A linear mapping can be thought of as a collection of linear combinations.</li> </ul>"},{"location":"MATHS2/5.04%20-%20Linear%20Mapping/#linearity-of-a-linear-mapping","title":"Linearity of a Linear Mapping","text":"<ul> <li>It follows that a linear mapping satisfies linearity, i.e. for any \\(c \\in \\mathbb{R}\\) (scalar)   \\(\\(f(x_1 + cy_1 , x_2 + cy_2, \\dots , x_n + cy_n) = f(x_1,x_2, \\dots ,x_n) + cf(y_1,y_2, \\dots ,y_n)\\)\\) \\(\\(\\text{or}\\)\\) \\(\\(f(x_1 + cy_1 , x_2 + cy_2, \\dots , x_n + cy_n) = A \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} + c \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\)\\)</li> </ul>"},{"location":"MATHS2/5.05%20-%20Linear%20Transformation/","title":"Linear Mappings","text":""},{"location":"MATHS2/5.05%20-%20Linear%20Transformation/#definition","title":"Definition","text":"<ul> <li>A linear mapping \\(f: V \\rightarrow W\\) between two vector spaces \\(V\\) and \\(W\\) is said to be a linear transformations if for nay two vectors \\(v_1\\) and \\(v_2\\) in the vector space \\(V\\) and for any \\(c \\in \\mathbb{R}\\), the following two conditions are satisfied:</li> <li>\\(f(v_1 + cv_2) = f(v_1) + cf(v_2)\\)</li> <li>\\(f(v_1) + f(cv_2) = f(v_1) + cf(v_2)\\)</li> </ul>"},{"location":"MATHS2/5.05%20-%20Linear%20Transformation/#types-of-linear-mappings","title":"Types of Linear Mappings","text":""},{"location":"MATHS2/5.05%20-%20Linear%20Transformation/#1-1-function","title":"\\(1-1\\) function","text":"<ul> <li>Function \\(f: V \\rightarrow W\\) is said to be a \\(1-1\\) function if for any two vectors \\(v_1\\) and \\(v_2\\) in the vector space \\(V\\), if \\(f(v_1) = f(v_2)\\) then \\(v_1 = v_2\\).</li> <li>For linear transformation, being a \\(1-1\\) is equivalent to \\(f(v) = 0\\) implies \\(v = 0\\).</li> </ul>"},{"location":"MATHS2/5.05%20-%20Linear%20Transformation/#onto-function","title":"Onto function","text":"<ul> <li>Function \\(f: V \\rightarrow W\\) is said to be an onto function if for any vector \\(w\\) in the vector space \\(W\\), there exists a vector \\(v\\) in the vector space \\(V\\) such that \\(f(v) = w\\).</li> </ul>"},{"location":"MATHS2/5.05%20-%20Linear%20Transformation/#isomorphism","title":"Isomorphism","text":"<ul> <li>A linear transformation \\(f: V \\rightarrow W\\) is said to be an isomorphism if it is both a \\(1-1\\) function and an onto function or \\(f\\) is a bijective function.</li> <li>Note that being a bijection is equivalent to: for any \\(v \\in W\\), there exists a \\(v \\in V\\) such that \\(f(v) = w\\).</li> </ul>"},{"location":"MATHS2/5.05%20-%20Linear%20Transformation/#definition_1","title":"Definition","text":"<ul> <li>A linear transformation \\(f: V \\rightarrow W\\) between two vector spaces \\(V\\) and \\(W\\) is said to be an isomorphism if it is a bijection.</li> </ul>"},{"location":"MATHS2/5.05%20-%20Linear%20Transformation/#example","title":"Example","text":"<ul> <li>\\(f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) defined by \\(f(x,y) = (2x,0)\\)</li> <li> <p>There is no pre-image for the vector \\((u,v)\\), where v is non-zero eg. \\((0,1)\\) has no pre-image. So \\(f\\) is not surjective. Also \\(f(x,y) = (0,0)\\) implies \\(x = 0\\). But there is no restriction on \\(y\\), eg. \\(f(0,1) = (0,0)\\). Hence \\(f\\) is not \\(1-1\\).</p> </li> <li> <p>\\(f : \\mathbb{R} \\rightarrow \\mathbb{R}^3 \\ ; \\ f(t) = (t,3t,\\frac{23}{89}t)\\) is \\(1-1\\) but not onto.</p> </li> <li>\\(f : \\mathbb{R}^2 \\rightarrow \\mathbb{R} \\ ; \\ f(x,y) = x\\) is onto but not \\(1-1\\).</li> </ul>"},{"location":"MATHS2/5.05%20-%20Linear%20Transformation/#bases-determine-linear-transformations","title":"Bases determine linear transformations","text":"<ul> <li>Let \\(V\\) be a vector space with basis \\(\\lbrace v_1, v_2, \\dots,v_n\\rbrace\\).</li> <li>Let \\(f : V \\rightarrow W\\) be a linear transformation. Then the ordered vectors \\(f(v_1), f(v_2), \\dots, f(v_n)\\) form a basis for \\(W\\).   \\(\\(f(v) = f(\\sum^n_{i=1}c_iv_i) = \\sum^n_{i=1}c_if(v_i)\\)\\) \\(\\(\\text{ The values are determined by } c_1,\\dots,c_n \\ \\&amp; \\ f(v_1), \\dots f(v_n).\\)\\)</li> </ul>"},{"location":"MATHS2/5.05%20-%20Linear%20Transformation/#example_1","title":"Example","text":"<ul> <li>Let the standard basis for \\(\\mathbb{R}^2\\) be \\(\\lbrace (1,0), (0,1) \\rbrace\\). What linear transformation \\(f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) do we obtain by extending:   $$ f((1,0)) = (2,0) = 2(1,0)$$   $$ f((0,1)) = (0,1) = 1(0,1)$$</li> <li>\\((x,y) = x(1,0) + y(0,1)\\)</li> <li>\\(f(x,y) = 2x(1,0) + y(0,1) = (2x,y)\\)</li> </ul>"},{"location":"MATHS2/6.01%20-%20Linear%20Tranformation%2C%20ordered%20bases%20and%20matricies/","title":"An important property of finite dimensional vector spaces","text":"<ul> <li>Let \\(V\\) be a vector space with dimension \\(n\\). Choose a basis \\(\\lbrace v_1, v_2, \\dots, v_n \\rbrace\\) for \\(V\\).</li> <li>Define \\(f : V \\rightarrow \\mathbb{R}^n\\) by extending the function sending the basis vector \\(v_i\\) to the standard basis vector \\(e_i \\in \\mathbb{R}^n\\) for each in \\(i\\).</li> <li>Then \\(f\\) is an isomorphism.</li> <li>\\(v = \\sum c_iv_i\\)</li> <li>\\(f(v) = \\sum c_ie_i \\rightarrow f(v_i) = e_i\\)</li> <li>Onto : \\((x_1,\\dots,x_n) \\in \\mathbb{R}^n . \\text{ Let } V = \\sum x_iv_i\\)<ul> <li>Then \\(f(V) = (x_1,\\dots,x_n) \\text{ or } \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\\)</li> </ul> </li> <li>\\(1-1\\) : \\(f(v) = 0 \\rightarrow \\sum c_ie_i = (0,\\dots,0)\\)<ul> <li>\\((c_1,\\dots,c_n) = (0,\\dots, 0)\\)</li> </ul> </li> </ul>"},{"location":"MATHS2/6.01%20-%20Linear%20Tranformation%2C%20ordered%20bases%20and%20matricies/#examples","title":"Examples","text":"<ul> <li>We computed the that the basis for the subspace.</li> <li>\\(W = \\lbrace (x,y,z)| x + y + z = 0\\rbrace \\text{ is } (-1,1,0), (-1, 0, 1)\\)</li> <li>Then the homomorphism \\(f\\) obtained by extending \\(f(-1,1,0) = (1,0) \\text{ and } f(-1,0,1) = (0,1)\\) is an isomorphism.</li> <li>Note that \\((x,y,z)\\) can be uniquely expressed as<ul> <li>\\((x,y,z) = y(-1,1,0) + z(-1,0,1)\\)</li> </ul> </li> <li>Hence, \\(f : W \\rightarrow \\mathbb{R}^2 is f(x,y,z) = y(1,0) + z(0,1) = (y,z)\\)</li> <li>\\(Onto : (y,z)^ \\in \\mathbb{R}^2 \\text{ is } x = -y - z\\) and consider \\((x,y,z) \\in W\\)<ul> <li>Then \\(f(W) = (y,z) = (x,-x)\\)</li> </ul> </li> <li>\\(1-1\\) : \\(f(x,y,z) = (0,0) \\rightarrow x = -y - z = 0\\)<ul> <li>Then \\((x,y,z) = (0,0,0)\\)</li> </ul> </li> </ul>"},{"location":"MATHS2/6.01%20-%20Linear%20Tranformation%2C%20ordered%20bases%20and%20matricies/#example-linear-transformation-in-matrix-form","title":"Example : Linear transformation in Matrix form","text":"<ul> <li>Consider the linear tranformation \\(f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2 ; f(x,y) = (2x,y)\\)</li> <li>We can represent this matrix form as \\(f(x,y) = \\begin{bmatrix}  2 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix}\\)</li> <li>If we represent the matrix using the standard basis we can write it as</li> <li>\\(f(1,0) = (2,0) = 2(1,0) + 0(0,1)\\)</li> <li>\\(f(0,1) = (0,1) = 0(1,0) + 1(0,1)\\)</li> </ul>"},{"location":"MATHS2/6.01%20-%20Linear%20Tranformation%2C%20ordered%20bases%20and%20matricies/#matrix-corresponding-to-a-linear-transformation-with-respect-to-ordered-basis","title":"Matrix corresponding to a linear transformation with respect to ordered basis","text":"<ul> <li>Let \\(f : V \\rightarrow W\\) be a linear transformation</li> <li>Let \\(\\beta = v_1,v_2, \\dots, v_n\\) be an ordered basis of \\(V\\) and \\(\\gamma = w_1,w_2, \\dots, w_n\\) be an ordered basis of \\(W\\)</li> <li>Each \\(f(v_1)\\) can be uniquely written as a linear combination of \\(w_js\\), where \\(i  = 1,2, \\dots, n\\) and \\(j = 1,2, \\dots, m\\)   $$f(v_1) = a_{11}w_1 + a_{21}w_2 + \\dots + a_{m1}w_m $$   $$f(v2) = a{12}w1 + a{22}w2 + \\dots + a{m2}wm $$   (\\(\\vdots\\)\\)   $$f(v_n) = a{1n}w1 + a{2n}w2 + \\dots + a{mn}w_m $$</li> <li>The matrix corresponding to the linear transformation \\(f\\) with respect to the ordered bases \\(\\beta \\text{ and } \\gamma\\) are given by the matrix   \\(\\(A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\dots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\dots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\dots &amp; a_{mn} \\end{bmatrix}\\)\\)</li> <li>The matrix \\(A\\) is called the matrix corresponding to the linear transformation \\(f\\) with respect to the ordered bases \\(\\beta\\) and \\(\\gamma\\)</li> <li>Example:<ul> <li>Let \\(V = W = \\mathbb{R}^2, \\beta = \\gamma = (1,0), (1,1) anf f(x,y) = (2x,y)\\)</li> <li>\\(f(1,0) = (2,0) = 2(1,0) + 0(1,1)\\)</li> <li>\\(f(1,1) = (2,1) = 1(1,0) + 1(1,1)\\)</li> <li>Then the matrix corresponding to the linear transformation \\(f\\) with respect to the ordered bases \\(\\beta\\) and \\(\\gamma\\) are given by the matrix   \\(\\(A = \\begin{bmatrix} 2 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix}\\)\\)</li> </ul> </li> </ul>"},{"location":"MATHS2/6.01%20-%20Linear%20Tranformation%2C%20ordered%20bases%20and%20matricies/#recovering-the-linear-transformation-from-the-matrix","title":"Recovering the linear transformation from the matrix","text":"<ul> <li>Let \\(\\beta = v_1, v_2, \\dots, v_n\\) and \\(\\gamma = w_1, w_2, \\dots, w_m\\) be ordered bases for \\(V\\) and \\(W\\) respectively. Suppose \\(A\\) is an \\(m \\times n\\) matrix. What is the linear transformation?</li> <li>Let \\(v \\in V\\). Express \\(v = \\sum_{j=1}^n c_jv_j\\) \\(\\(f(v) = \\sum_{j=1}^n c_j\\sum_{i=1}^ma_{ij}w_i\\)\\)</li> <li>Checking that \\(f\\) is a linear transformation!</li> <li>Letting \\(c_k = 1\\) and \\(c_j = 0\\) for all \\(j \\neq k\\) we get     \\(\\(f(v_k) = A_{1k}w_1 + \\dots + A_{mk}w_m\\)\\)</li> </ul>"},{"location":"MATHS2/6.01%20-%20Linear%20Tranformation%2C%20ordered%20bases%20and%20matricies/#fixed-ordered-bases-linear-transformations-lrarr-matrices","title":"Fixed ordered bases : Linear transformations \\(\\lrarr\\) matrices","text":"<ul> <li>Let \\(\\beta\\) and \\(\\gamma\\) be ordered bases for vector spaces \\(V\\) and \\(W\\) respectively where \\(n = dim(V)\\) and \\(m = dim(W)\\)</li> <li>There is a bijection:</li> <li>\\(\\lbrace \\text{Linear transformations from } V \\text{ to } W \\rbrace \\leftrightarrow \\lbrace m \\times n \\text{ matrices } \\rbrace\\)</li> </ul>"},{"location":"MATHS2/6.01%20-%20Linear%20Tranformation%2C%20ordered%20bases%20and%20matricies/#example","title":"Example","text":"<ul> <li>Let \\(W  = \\lbrace (x,y,z)| x + y + z = 0\\rbrace\\) and \\(V = \\mathbb{R}^2\\)</li> <li>Let \\(\\beta = (-1,1,0), (-1,0,1)\\) and \\(\\gamma = (1,0), (0,1)\\)</li> <li>The isomorphism \\(f\\) is obtained by extending \\(f (\u20141, 1, 0) (1, 0)\\) and \\(f (\u20141, 0, 1) = (0,1)\\) \\(\\(A = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\)\\)</li> </ul>"},{"location":"MATHS2/6.02%20-%20Image%20and%20Kernal%20of%20linear%20transformations/","title":"Kernal and Image","text":""},{"location":"MATHS2/6.02%20-%20Image%20and%20Kernal%20of%20linear%20transformations/#definations","title":"Definations","text":""},{"location":"MATHS2/6.02%20-%20Image%20and%20Kernal%20of%20linear%20transformations/#kernal","title":"Kernal","text":"<ul> <li>Let \\(f : V \\rightarrow W\\) be a linear transformation.</li> <li>The kernal of \\(f\\) or \\(ker(f):\\) \\(\\(ker(f) = \\{v \\in V | f(v) = 0\\}\\)\\)</li> </ul>"},{"location":"MATHS2/6.02%20-%20Image%20and%20Kernal%20of%20linear%20transformations/#image","title":"Image","text":"<ul> <li>The image of \\(f\\) or \\(im(f) / Im(f):\\) \\(\\(im(f) = \\{w \\in W | \\exists v \\in V, f(v) = w\\}\\)\\)</li> <li>\\(Im(f)\\) is the other name for the \"range of the function \\(f\\)\" which we have studied in Math-1.</li> <li>The image of \\(f\\) is also the subspace for \\(W\\).</li> </ul>"},{"location":"MATHS2/6.02%20-%20Image%20and%20Kernal%20of%20linear%20transformations/#examples","title":"Examples","text":"<ul> <li>Consider \\(f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) defined by \\(f(x, y) = (2x, y)\\).</li> <li>Then the \\(ker(f) = \\lbrace (0,0) \\rbrace\\)</li> <li>\\(Im(f) = \\mathbb{R}^2\\)</li> <li>Consider \\(f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) defined by \\(f(x, y) = (2x, 0)\\).</li> <li>Then the \\(ker(f) = \\lbrace (0,y) | y \\in \\mathbb{R} \\rbrace\\)</li> <li>\\(Im(f) = \\lbrace (x,0) | x \\in \\mathbb{R} \\rbrace\\)</li> </ul>"},{"location":"MATHS2/6.02%20-%20Image%20and%20Kernal%20of%20linear%20transformations/#the-kernal-and-injectivity-of-a-linear-transformation","title":"The kernal and injectivity of a linear transformation","text":"<ul> <li>Function \\(f : V \\rightarrow W\\) is injective if \\(f(v_1) = f(v_2)\\) implies \\(v_1 = v_2\\).</li> <li>Linear transformation \\(f\\) being \\(1-1\\) (or injective) is equivalent to \\(f(v) = 0\\) implies \\(v = 0\\).</li> <li>Rewriting the last part in terms of \\(ker(f)\\), we see that linear transformation is \\(1-1\\) is equivalent to \\(ker(f) = \\lbrace 0 \\rbrace\\).   \\(\\(\\text{ A linear transformation } f \\text{ is injective if and only if } ker(f) = \\lbrace 0 \\rbrace\\)\\)</li> </ul>"},{"location":"MATHS2/6.02%20-%20Image%20and%20Kernal%20of%20linear%20transformations/#the-image-and-surjectivity-of-a-linear-transformation","title":"The image and surjectivity of a linear transformation","text":"<ul> <li>Function \\(f : V \\rightarrow W\\) is surjective if for each \\(w \\in W\\), there exists some \\(v \\in V\\) such that \\(f(v) = w\\).</li> <li>It follows the defination of the function \\(f : V \\rightarrow V\\) being onto (or surjective) is equivalent to \\(Range(f) = W\\).</li> </ul>"},{"location":"MATHS2/6.02%20-%20Image%20and%20Kernal%20of%20linear%20transformations/#kernels-and-null-spaces","title":"Kernels and null spaces","text":"<ul> <li>Let \\(f : V \\rightarrow W\\) be a linear transformation. Let \\(\\beta = v_1, v_2, \\dots, v_n\\) and \\(\\gamma = w_1, w_2, \\dots, w_m\\) be ordered bases for \\(V\\) and \\(W\\) respectively.</li> <li>Let \\(A\\) be the corresponding to \\(f\\) with respect to \\(\\beta\\) and \\(\\gamma\\).   \\(\\(v = \\sum_{j=1}^n c_j v_j \\in V, f(v)=\\sum_{j=1}^nc_j\\sum_{i=1}^m A_{ij}w_i\\)\\)</li> <li>If \\(f(v) = 0\\) for all \\(v \\in V\\), then \\(f\\) is called a zero transformation.</li> <li>All the coefficients \\(c_j\\) are zero, so \\(v = 0\\).</li> <li>Thus \\(v = ker(f)\\).</li> </ul>"},{"location":"MATHS2/6.02%20-%20Image%20and%20Kernal%20of%20linear%20transformations/#bases-for-the-kernal-and-image-of-linear-transformation","title":"Bases for the kernal and image of linear transformation","text":"<ul> <li>Let \\(A\\) be the matrix corresponding to \\(f\\) with respect to \\(\\beta\\) and \\(\\gamma\\).</li> <li>The relation between kernals and null spaces derived earlier actually yields an isomorphism between the null space of \\(A\\) and the kernal of \\(f\\).</li> </ul>"},{"location":"MATHS2/6.02%20-%20Image%20and%20Kernal%20of%20linear%20transformations/#finding-the-basis-for-the-kernal-and-range","title":"Finding the basis for the kernal and range","text":"<ul> <li>If we have the matrix of the linear transformation we can find the basis for the kernal and range:</li> <li>We can convert that matrix into a RREF and then multiply it by the variables from the vector space:<ul> <li>Example 1:</li> <li>Suppose the matrix \\(A =\\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix}\\)</li> <li>The RREF of the matrix is \\(\\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\)</li> <li>Multiply the matrix by the variables \\(x\\) and \\(y\\) we get</li> <li>\\(\\begin{bmatrix}1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\cdot \\begin{bmatrix} x \\\\ y\\end{bmatrix} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}\\)</li> <li>The basis for range of the matrix \\(A\\) is the columns with pivot points. So the basis is \\(\\text{Basis for } R =\\lbrace (1,3) , (2,4) \\rbrace\\)</li> <li>In this case both \\(x\\) and \\(y\\) are equating to \\(0\\) so the \\(ker(A) =\\lbrace 0 \\rbrace\\)</li> <li>Example 2:</li> <li>Linear Transformation from \\(\\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\) with respect to standard ordered basis.</li> <li>Basis for Domain \\(\\lbrace (1,0), (0,1) \\rbrace\\)</li> <li>Basis for Range \\(\\lbrace (1,0,0), (0,1,0), (0,0,1) \\rbrace\\)</li> <li>Matrix \\(A = \\begin{bmatrix} 1 &amp; 2 \\\\ 2 &amp; 4 \\\\ 4 &amp; 8 \\end{bmatrix}\\)</li> <li>\\(A_{rref} = \\begin{bmatrix} 1 &amp; 2 \\\\ 0 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}\\)</li> <li>The basis for range of the matrix \\(A\\) is the columns with pivot points. So the basis is \\(\\text{Basis for } R =\\lbrace (1,3,4) \\rbrace\\)</li> <li>\\(ker(T) = \\text{ Nullspace of } \\begin{pmatrix} 1 &amp; 2 \\\\ 0 &amp; 0 \\\\ 0 &amp; 0\\end{pmatrix} = \\begin{pmatrix} 1 &amp; 2 \\\\ 0 &amp; 0 \\\\ 0 &amp; 0\\end{pmatrix} \\cdot \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\Rightarrow x + 2y = 0 \\Rightarrow x= -2y\\)</li> <li>The kernal is \\(\\lbrace (-2y, y) | y \\in \\mathbb{R} \\rbrace\\) and the basis of the kernal is \\(\\lbrace (-2, 1) \\rbrace\\).</li> <li>Example 3:</li> <li>Linear Transformation from \\(\\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) with respect to \\(\\lbrace (1,1), (1,0) \\rbrace\\).</li> <li>The matix \\(A = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 6 \\end{bmatrix}\\)</li> <li>\\(A_{rref} = \\begin{bmatrix} 1 &amp; 2 \\\\ 0 &amp; 0 \\end{bmatrix}\\)</li> <li>The basis for range of the matrix \\(A\\) is the columns with pivot points. So the basis is \\(\\text{Basis for } R =\\lbrace 1(1,1) + 3(1,0) \\rbrace = \\lbrace (4,1)\\rbrace\\)</li> <li>Nullspace of \\(A\\) is \\(\\begin{pmatrix} 1 &amp; 2 \\\\ 0 &amp; 0 \\end{pmatrix} \\cdot \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\Rightarrow x +2y = 0 \\Rightarrow x = -2y\\)</li> <li>\\(ker(T) = \\lbrace (-2y, y) | y \\in \\mathbb{R} \\rbrace\\).</li> <li>Basis for kernal is \\(\\lbrace -2(1,1) + 1(1,0) \\rbrace \\Rightarrow \\{  (-1,-2)\\}\\)</li> <li>Example 4:</li> <li>Linear Transformation from \\(\\mathbb{R}^3 \\rightarrow \\mathbb{R}^3\\) with respect to \\(\\{ (1, 0, 0) , (0, 1, 0), (0, 0, 1)\\}\\) for the domain and \\(\\{ (1, 1, 1) , (1, 1, 0), (1, 0, 0)\\}\\) for the codomain</li> <li>The Matrix \\(A = \\begin{bmatrix} 1 &amp; 4 &amp; 1 \\\\ 0 &amp; 2 &amp; 2 \\\\ 1 &amp; 6 &amp; 3 \\end{bmatrix}\\)</li> <li>\\(A_{rref} = \\begin{bmatrix} 1 &amp; 0 &amp; -3 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\)</li> <li>Basis for range is \\(\\{ 1(1,1,1) + 0 (1,1,0) + 1(1,0,0), 4(1,1,1) + 2 (1,1,0) + 6(1,0,0) \\} \\Rightarrow \\{ (2,1,1),(12,6,4)\\}\\)</li> <li>B</li> </ul> </li> </ul>"},{"location":"MATHS2/6.02%20-%20Image%20and%20Kernal%20of%20linear%20transformations/#points","title":"Points","text":""},{"location":"MATHS2/6.02%20-%20Image%20and%20Kernal%20of%20linear%20transformations/#properties-of-linear-spaces","title":"Properties of linear spaces","text":"<ul> <li>If the linear transformation is from \\(\\mathbb{R}^m \\rightarrow \\mathbb{R}^n\\), where are \\(m &gt; n\\).</li> <li>Then the transformation can never be \\(1-1\\).</li> <li>This is because we there has to be some overlap while mapping a higher dimension to a lower dimension.</li> <li>If the linear transformation is from \\(\\mathbb{R}^m \\rightarrow \\mathbb{R}^n\\), where are \\(m &lt; n\\).</li> <li>Then the transformation can never be \\(\\text{onto}\\).</li> <li>This is because a smaller dimension cannot fully map to a higher dimension.</li> <li>If the linear transformation is from \\(\\mathbb{R}^m \\rightarrow \\mathbb{R}^n\\), where are \\(m = n\\).</li> <li>Then this transformation can be \\(1-1\\) and \\(\\text{onto}\\).</li> <li>If the \\(\\ker(f) = 0\\) then it can be \\(1-1\\). If the \\(\\ker(f) = 0\\) then the \\(nullity(A) = 0\\).</li> <li>According to the Rank Nullity Theorm \\(rk(A) + nullity(A) = n\\).<ul> <li>\\(rk(A) + 0 = n\\)</li> </ul> </li> <li>Since the rank of the matrix is \\(n\\) then it is onto.</li> <li>TLDR: A linear transformation can be \\(1-1\\) and \\(\\text{onto}\\) if the transformtion is happening in the same dimension and nullity of the transformation is 0 or the rank of the matrix is \\(n\\).</li> <li>We can use these properties to find the properties of the transformation.</li> </ul>"},{"location":"MATHS2/6.02%20-%20Image%20and%20Kernal%20of%20linear%20transformations/#finding-the-linear-transformation","title":"Finding the linear transformation","text":"<ul> <li>We can find the linear transformation if we have the basis of the domain and codomain.</li> <li>We can backtrack from codomain to domain. We can find the matrix of the transformation and then find the transformation.</li> </ul>"},{"location":"MATHS2/7.01%20-%20Equivalence%20and%20similarity%20of%20matricies/","title":"Equivalence of Matricies","text":"<ul> <li>Let \\(A\\) and \\(B\\) be two matrices of order \\(m \\times n\\). We say \\(A\\) is equivalent to \\(B\\) if \\(B = QAP\\) for some invertible \\(n \\times n\\) matrix \\(P\\) and some invertible \\(m \\times m\\) matrix \\(Q\\).</li> </ul>"},{"location":"MATHS2/7.01%20-%20Equivalence%20and%20similarity%20of%20matricies/#properties-of-equivalence","title":"Properties of equivalence","text":"<ul> <li>\\(A\\) can be transformed into \\(B\\) by a sequence of elementary row operations.</li> <li>\\(rank(A) = rank(B)\\)</li> <li>If the rank of \\(A\\) and \\(B\\) are the same then they are equivalent.</li> </ul>"},{"location":"MATHS2/7.01%20-%20Equivalence%20and%20similarity%20of%20matricies/#equivalence-matrices-is-an-equivalence-relation","title":"Equivalence matrices is an equivalence relation","text":"<ul> <li>\\(A\\) is equivalent to itself.</li> <li>\\(A\\) is equivalent to \\(B\\) if and only if \\(B\\) is equivalent to \\(A\\).</li> <li>\\(A\\) is equivalent to \\(B\\) and \\(B\\) is equivalent to \\(C\\) if and only if \\(A\\) is equivalent to \\(C\\).</li> </ul>"},{"location":"MATHS2/7.01%20-%20Equivalence%20and%20similarity%20of%20matricies/#example","title":"Example","text":"<ul> <li>Consider the linear transformation \\(f : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^2,\\) defined as :</li> <li>\\(f(x,y,z) = (x + y, x - z)\\)</li> <li>Consider two ordered bases for \\(\\mathbb{R}^3\\):</li> <li>\\(\\beta_1 = (1,0,0), (0,1,0),(0,0,1)\\) and \\(\\beta_1 = (1,1,0), (0,1,1),(0,0,1)\\)</li> <li>Consider two ordered basis for \\(\\mathbb{R}^2\\):</li> <li>\\(\\gamma_1 = (1,0), (0,1)\\) and \\(\\gamma_2 = (1,0), (1,1)\\)</li> <li>\\(f(1,0,0) = (1,0)\\)</li> <li>\\(f(0,1,0) = (1,1) = 1(1,0) + 1(0,1)\\)</li> <li>\\(f(0,0,1) = (0,1)\\)</li> <li>The matrix of \\(\\beta_1\\) with respect to \\(\\gamma_1\\) is:</li> <li>\\(A = \\begin{bmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 \\end{bmatrix}\\)</li> <li>\\(f(1,1,0) = (2,1) = 1(1,0) + 1(1,1)\\)</li> <li>\\(f(0,1,1) = (1,2) = -1(1,0) + 2(1,1)\\)</li> <li>\\(f(0,0,1) = (0,1) = -1(1,0) + 1(1,1)\\)</li> <li>The matrix of \\(\\beta_2\\) with respect to \\(\\gamma_2\\) is:</li> <li>\\(B = \\begin{bmatrix} 1 &amp; -1 &amp; -1 \\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix}\\)</li> <li>Choose \\(P = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1\\end{bmatrix}\\) and \\(Q = \\begin{bmatrix} 1 &amp; -1 \\\\ 0 &amp; 1 \\end{bmatrix}\\)</li> <li>\\(QAP = \\begin{bmatrix} 1 &amp; -1 &amp; -1 \\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1\\end{bmatrix}  = \\begin{bmatrix} 1 &amp; -1 &amp; -1 \\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix} = B\\)</li> <li>Hence \\(A\\) and \\(B\\) are equivalent.</li> </ul>"},{"location":"MATHS2/7.01%20-%20Equivalence%20and%20similarity%20of%20matricies/#note","title":"Note","text":"<ul> <li>We can get the matrix of \\(P\\) by expressing the ordered basis \\(\\beta_2\\) in terms of the ordered basis \\(\\beta_1\\)</li> <li>We can get the matrix of \\(Q\\) by expressing the ordered basis \\(\\gamma_2\\) in terms of the ordered basis \\(\\gamma_1\\)</li> </ul>"},{"location":"MATHS2/7.01%20-%20Equivalence%20and%20similarity%20of%20matricies/#similarity","title":"Similarity","text":"<ul> <li>An \\(n \\times n\\) matrix \\(A\\) is said to be similar to a matrix \\(B\\) if there exists an invertible \\(n \\times n\\) matrix \\(P\\) such that \\(B = P^{-1}AP \\text{ or } PB = AP\\).</li> <li>Similar matrix have the same trace.</li> <li> \\[\\text{Trace = } tr(A) = \\sum_{i=1}^nA_{ii}\\] </li> <li>\\(A\\) is similar to itself.</li> <li>\\(A\\) is similar to \\(B\\) if and only if \\(B\\) is similar to \\(A\\).</li> <li>\\(A\\) is similar to \\(B\\) and \\(B\\) is similar to \\(C\\) if and only if \\(A\\) is similar to \\(C\\).</li> </ul>"},{"location":"MATHS2/7.01%20-%20Equivalence%20and%20similarity%20of%20matricies/#properties-of-similarity","title":"Properties of similarity","text":"<ul> <li>Suppose \\(A\\) is similar to \\(B\\) matricies.</li> <li>\\(A\\) and \\(B\\) are equivalent.</li> <li>\\(rank(A) = rank(B)\\)</li> <li>\\(det(B) = det(A)\\)</li> <li>\\(det(B) = det(P^{-1}AP) \\Rightarrow det(P^{-1})det(A)det(P) \\Rightarrow  \\frac{1}{det(P)}det(A)det(P) = det(A)\\)</li> <li>Several other invariants of \\(A\\) and \\(B\\) are the same such as the characteristic polynomial, minimal polynomial and eigen values (with multiplicity).</li> </ul>"},{"location":"MATHS2/7.01%20-%20Equivalence%20and%20similarity%20of%20matricies/#example_1","title":"Example","text":"<ul> <li>Consider the linear transformation \\(f : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3 \\text{ where } f(x,y,z) = (-x+y+z,x-y+z,x+y-z)\\)</li> <li>The basis \\(\\beta = \\gamma\\) both are the standard ordered basis for</li> <li>\\(f(1,0,0) = (-1,1,1) = -1(1,0,0) + 1(0,1,0) + 1(0,0,1)\\)</li> <li>\\(f(0,1,0) = (1,-1,1) = 1(1,0,0) - 1(0,1,0) + 1(0,0,1)\\)</li> <li>\\(f(0,0,1) = (1,1,-1) = 1(1,0,0) + 1(0,1,0) - 1(0,0,1)\\)</li> <li>Hence the matrix of \\(f\\) is:<ul> <li>\\(A = \\begin{bmatrix} -1 &amp; 1 &amp; 1 \\\\ 1 &amp; -1 &amp; 1 \\\\ 1 &amp; 1 &amp; -1 \\end{bmatrix}\\)</li> </ul> </li> <li>We get \\(P\\) by expressing the ordered basis \\(\\beta\\) in terms of the ordered basis \\(\\gamma\\).<ul> <li>\\(P = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 \\end{bmatrix}\\)</li> </ul> </li> <li>Consider the linear transformation \\(f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2 \\text{ where } f(x,y) = (2x,y)\\)</li> <li>The basis \\((1,0), (1,1)\\) for \\(\\mathbb{R}^2\\). Then we have the following:</li> <li>\\(f(1,0) = (2,0) = 2(1,0) + 0(1,1)\\)</li> <li>\\(f(1,1)= (2,1) = 1(1,0) + 1(1,1)\\)</li> <li>The matrix of \\(f\\) is:<ul> <li>\\(B = \\begin{bmatrix} 2 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix}\\)</li> </ul> </li> </ul>"},{"location":"MATHS2/7.02%20-%20Affine%20subspace/","title":"Affine Subspace","text":""},{"location":"MATHS2/7.02%20-%20Affine%20subspace/#defination","title":"Defination","text":"<ul> <li>Let \\(V\\) be a vector space. An affine subspace of \\(V\\) is a subset \\(L\\) such that there exists \\(v \\in V\\) and a vector subspace \\(U \\sube V\\) such that   \\(\\(L = v + U := \\{ v + u| u \\in U\\}\\)\\)</li> <li>An affine subspace \\(L\\) is n-dimensional if the corresponding subspace \\(U\\) is n-dimensional.</li> <li>The subspace \\(U\\) corresponding to an affine subspace is unique.</li> <li>However the vector \\(v\\) corresponding to an affine subspace is not unique.</li> <li>Affine subspaces are thus translation of vector subspace \\(V.\\)</li> </ul>"},{"location":"MATHS2/7.02%20-%20Affine%20subspace/#solution-set-of-a-system-of-linear-equations","title":"Solution set of a system of linear equations","text":"<ul> <li>Let \\(Ax = b\\) be a linear system of equations.</li> <li>\\(b = 0:\\) In this case, it is a homogeneous system and as seen     before, the solution set is a subspace of \\(\\mathbb{R}^n\\), namely the null     space \\(A\\).</li> <li>\\(b \\notin \\text{ column space of } A\\): In this case, \\(Ax = b\\) does not have a solution, so the solution set is the empty set.</li> <li>\\(b \\in \\text{ column space of } A\\): In this case, the solution set L is an affine subspace of \\(\\mathbb{R}^n\\). Specifically, it can be described as \\(L =  v + nullspace(A)\\) where v is any solution of the equation \\(Ax = b.\\)</li> </ul>"},{"location":"MATHS2/7.02%20-%20Affine%20subspace/#affine-mapping-of-affine-subspaces","title":"Affine mapping of affine subspaces","text":"<ul> <li>Let \\(L\\) and \\(L'\\) be affine subspace of \\(V\\) and \\(W\\) respectively. Let \\(f : L \\rightarrow L'\\) be a function. Cosider any vector \\(v \\in L\\) and the unique subspace \\(U \\sube V\\) such that \\(L = v + U\\).</li> <li>Note that \\(f(v) \\in L'\\) and hence \\(f(v) + U'\\) where \\(U'\\) is unique subspace of \\(W\\) corresponding to \\(L'\\). Then \\(f\\) is an affine mapping from \\(L\\) to \\(L'\\) if the function \\(g: U \\rightarrow U'\\) defined by \\(g(u) = f(u+v) - f(v)\\) is a linear transformation.</li> <li>For a linear transformation \\(T: U \\rightarrow U'\\), and fixed vectors \\(v \\in L\\) and \\(v' \\in L'\\), an affine mapping \\(f\\) can be obtained by defining \\(f(v+u) = v' + T(u)\\), and in fact every affine mapping is obtained in this way.</li> </ul>"},{"location":"MATHS2/7.02%20-%20Affine%20subspace/#example","title":"Example","text":"<ul> <li>Let \\(T = (2x+3y+2, 4x-5y+3)\\). Then this is an affine mapping from \\(\\mathbb{R}^3 \\rightarrow \\mathbb{R}^2\\)</li> <li>Let \\(T : V \\rightarrow W\\) be a linear transformation and \\(w \\in W\\), then mapping   \\(\\(T': V \\rightarrow W\\)\\) \\(\\(T'(v) = T(v) + w\\)\\)</li> <li>This is a affine mapping from \\(V\\) to \\(W\\).</li> <li>We can find the shift in the mapping by looking at the shift at the \\(0\\) vector</li> </ul>"},{"location":"MATHS2/7.03%20-%20Lengths%20and%20angle/","title":"Lengths of Vectors","text":"<ul> <li>Let us find the length of vector \\((3,4)\\) in \\(\\mathbb{R}^2\\)</li> <li>Using Pythagoras' theorm the length of the vector \\((3,4)\\) is:</li> <li>\\(\\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\\text{ units}\\)</li> </ul>"},{"location":"MATHS2/7.03%20-%20Lengths%20and%20angle/#formula","title":"Formula","text":"<ul> <li>The Length of the vector is the Square root of it's Dot Product with itself.</li> <li>\\(u \\in \\mathbb{R}^n\\) \\(\\(||u|| = \\sqrt{u \\cdot u}\\)\\)</li> </ul>"},{"location":"MATHS2/7.03%20-%20Lengths%20and%20angle/#angle-between-between-two-vectors-in-mathbbr2","title":"Angle between between two vectors in \\(\\mathbb{R}^2\\)","text":""},{"location":"MATHS2/7.03%20-%20Lengths%20and%20angle/#formula_1","title":"Formula","text":"\\[cos(\\theta) = \\frac{u \\cdot v}{||u|| \\cdot ||v|| }$$ $$or$$ $$\\theta = \\cos^{-1}(\\frac{u\\cdot v}{\\sqrt{(v \\cdot v) \\times(u \\cdot u)}})\\] <ul> <li>Let us find the angle between the vectors \\((3,4)\\) and \\((1,5)\\) in \\(\\mathbb{R}^2\\)</li> <li>The angle is measured in degrees or radians \\(( 0 \\text{ and } 2\\pi)\\)</li> <li>It is clockwise from the positive \\(x\\)-axis</li> <li>The angle is often described by computing its trigonometrix function \\((e.g. \\sin, \\cos, \\tan)\\)</li> </ul>"},{"location":"MATHS2/7.03%20-%20Lengths%20and%20angle/#the-dot-product-and-the-angle-between-two-vectors-in-mathbbr2","title":"The dot product and the angle between two vectors in \\(\\mathbb{R}^2\\)","text":"<ul> <li>Let \\(u\\) and \\(v\\) be two vectors in \\(\\mathbb{R}^2\\). Then we can compute the angle \\(\\theta\\) between the vectors \\(u\\) and \\(v\\) using the dot products as:   $$   \\cos(\\theta) = \\frac{u\\cdot v}{\\sqrt{(v \\cdot v) \\times(u   \\cdot u)}}   $$   $$ \\text{or}$$   $$   \\theta = \\cos^{-1}(\\frac{u\\cdot v}{\\sqrt{(v \\cdot v) \\times(u \\cdot u)}})   $$</li> </ul>"},{"location":"MATHS2/7.03%20-%20Lengths%20and%20angle/#the-dot-product-of-two-vectors-in-mathbb3","title":"The dot product of two vectors in \\mathbb{3}$","text":"<ul> <li>Consider the two vectors \\((1, 2, 3)\\) and \\((2, 0, 1)\\) in \\(\\mathbb{R}^3\\). The dot product of these two vectors gives us a scalar as follows:   \\(\\((1,2,3) \\cdot (2,0,1) = 1 \\times 2 + 2 \\times 0 + 3 \\times 1 = 5\\)\\)</li> </ul>"},{"location":"MATHS2/7.03%20-%20Lengths%20and%20angle/#length-of-vector-in-mathbbr3","title":"Length of vector in \\(\\mathbb{R}^3\\)","text":"<ul> <li>Finding the length of the vector \\((4,3,3)\\) in \\(\\mathbb{R}^3\\)</li> <li>Using the formula we get:</li> <li>\\(\\sqrt{4^2 + 3^2 + 3^2} = \\sqrt{16 + 9 + 9} = \\sqrt{34} = 5.83\\text{ units}\\)</li> </ul>"},{"location":"MATHS2/7.03%20-%20Lengths%20and%20angle/#angle-between-two-vectors-in-mathbbr3","title":"Angle between two vectors in \\(\\mathbb{R}^3\\)","text":"<ul> <li>Finding the angle \\(\\theta\\) between \\((1,0,0) \\text{ and } (1,0,1).\\)</li> <li>Using the formula:</li> <li>\\((1,0,0). (1, 0, 1)= 1, (1, 0, 1). (1, 0, 1) = 2,(1, 0, 0) . (1, 0, 0) = 1\\)</li> <li>Hence, \\(\\theta = \\cos^{-1}(\\frac{1}{\\sqrt{2}}) = 45^{\\circ}\\)</li> </ul>"},{"location":"MATHS2/7.04%20-%20Inner%20Product%20and%20Norm/","title":"Inner Product","text":""},{"location":"MATHS2/7.04%20-%20Inner%20Product%20and%20Norm/#definition","title":"Definition","text":"<ul> <li>An inner product on a vector space \\(V\\) is a function:   \\(\\(&lt; .\\ ,\\ . &gt;: V \\times V \\rightarrow \\mathbb{R}\\)\\)</li> <li>It is an inner product when it satisfies all the conditions mentioned below.</li> </ul>"},{"location":"MATHS2/7.04%20-%20Inner%20Product%20and%20Norm/#conditions","title":"Conditions","text":"<ul> <li>\\(&lt;v_1,v_2&gt; = &lt;v_2,v_1&gt;\\)</li> <li>\\(&lt;v_1+v_2,v_3&gt; = &lt;v_1,v_3&gt; + &lt;v_2,v_3&gt;\\)</li> <li>\\(&lt;cv_1,v_2&gt; = c&lt;v_1,v_2&gt; = &lt;v_1,cv_2&gt;\\)</li> <li>\\(&lt;v_1,v_1&gt; \\geq 0\\)</li> <li>\\(&lt;v_1,v_1&gt; = 0 \\iff v_1 = 0\\)</li> <li>A vector space \\(V\\) together with an inner product \\(&lt; .\\ ,\\ . &gt;\\) is called an inner product space.</li> </ul>"},{"location":"MATHS2/7.04%20-%20Inner%20Product%20and%20Norm/#dot-product","title":"Dot Product","text":"<ul> <li>The dot product is a special case of an inner product.</li> <li>\\(u,v \\in \\mathbb{R}^2\\) \\(\\(u \\cdot v = &lt;u,v&gt; = u_1v_1 + u_2v_2 + \\dots + u_nv_n\\)\\)</li> </ul>"},{"location":"MATHS2/7.04%20-%20Inner%20Product%20and%20Norm/#norm","title":"Norm","text":""},{"location":"MATHS2/7.04%20-%20Inner%20Product%20and%20Norm/#defination","title":"Defination","text":"\\[|| \\ \\cdot \\ || : V \\rightarrow \\mathbb{R}$$ $$ x \\rightarrowtail ||x|| = \\sqrt{&lt;x,x&gt;}\\] <ul> <li>It is an inner product when it satisfies all the conditions mentioned below.</li> </ul>"},{"location":"MATHS2/7.04%20-%20Inner%20Product%20and%20Norm/#conditions_1","title":"Conditions","text":"<ul> <li>\\(|| x + y || \\leq ||x|| + ||y||\\)</li> <li>\\(||cx|| = |c| ||x||\\)</li> <li>\\(||x|| \\geq 0\\) and \\(||x|| = 0 \\iff x = 0\\)</li> <li>All inner products satify the conditions of norms</li> </ul>"},{"location":"MATHS2/7.04%20-%20Inner%20Product%20and%20Norm/#length","title":"Length","text":"<ul> <li>The length of the vector \\(x\\) is the norm of the vector \\(x\\).   \\(\\(||u|| = \\sqrt{(x_1^2+ x_2^2 + \\dots + x_n^2)}\\)\\)</li> <li>The length function \\(\\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is the norm of the vector space \\(\\mathbb{R}^n\\).</li> </ul>"},{"location":"MATHS2/7.04%20-%20Inner%20Product%20and%20Norm/#example","title":"Example","text":"<ul> <li>The following is an expample of a norm of \\(\\mathbb{R}^n\\)</li> <li>Defining \\(||u||_1 = |x_1| + |x_2| + \\dots + |x_n| \\text{ for all } u = (x_1,x_2,\\dots,x_n) \\in \\mathbb{R}^n\\)</li> <li>\\(||u||_1\\) satisfies the \\(0\\) condtion of the norm because it can only be \\(0\\) when all the values in \\(u\\) are \\(0\\).</li> <li>\\(||cu||_1 = |cx_1| + |cx_2| + \\dots + |cx_n| = |c|(|x_1| + |x_2| + \\dots + |x_n|) = |c| (|x_1| + |x_2| + \\dots + |x_n|)\\) This also statisfies the scalar multiplication condition of the norm.</li> <li>\\(||u+v||_1 \\leq |x_1 + y_1| + |x_2 + y_2| + \\dots + |x_n + y_n|\\). This also statisfies the triangle inequality condition of the norm.</li> <li>\\(\\therefore ||u||_1\\) is a norm of \\(\\mathbb{R}^n\\)</li> </ul>"},{"location":"MATHS2/8.01%20-%20Orthogonality%20and%20Linear%20Independence/","title":"Orthogonality","text":""},{"location":"MATHS2/8.01%20-%20Orthogonality%20and%20Linear%20Independence/#definition","title":"Definition","text":"<ul> <li>Two vectors \\(u,v \\in V\\) are orthogonal if \\(&lt;u,v&gt; = 0\\) or \\(u \\cdot v = 0\\).</li> <li>It depends on the inner product of the vector space.</li> <li>Different inner products can lead to different definitions of orthogonality.</li> </ul>"},{"location":"MATHS2/8.01%20-%20Orthogonality%20and%20Linear%20Independence/#example","title":"Example","text":"<ul> <li>Consider the \\(\\mathbb{R}^2\\) with inner product</li> <li>\\(&lt;u,v&gt; = x_1y_1 - (x_2y_2 + x_2y_1) + 2x_2y_2\\)</li> <li>Then the vectors \\((1,1)\\) and \\((1,0)\\) are orthogonal.</li> <li>\\(&lt;u,v&gt; = 1 \\times 1 - (1 \\times 0 + 1 \\times 1) + 2 \\times 0 = 0\\)</li> </ul>"},{"location":"MATHS2/8.01%20-%20Orthogonality%20and%20Linear%20Independence/#an-orthogonal-set-of-vectors","title":"An Orthogonal set of vectors","text":"<ul> <li>An orthogonal set of vectors of an inner product space \\(V\\) is a set of vectors whose elements are mutually orthogonal.</li> <li>Explicitly, if \\(S = \\{v_1,v_2,\\dots,v_n\\}\\) is an orthogonal set of vectors, then \\(&lt;v_i,v_j&gt; = 0\\) for all \\(i \\neq j\\).   \\(\\(&lt;v_i,v_j&gt; = 0 \\text{ for all } i,j \\in \\{1,2, \\dots, k\\} \\text{ and } i \\neq j\\)\\)</li> </ul>"},{"location":"MATHS2/8.01%20-%20Orthogonality%20and%20Linear%20Independence/#example_1","title":"Example","text":"<ul> <li>Consider \\(\\mathbb{R}^3\\) with the usual inner product. Then the set \\(S = \\{(4,3, -2), (-3, 2, -3), (-5, 18, 17)\\}\\) is an orthogonal set of vectors.</li> </ul>"},{"location":"MATHS2/8.01%20-%20Orthogonality%20and%20Linear%20Independence/#orthogonality-and-linear-independence","title":"Orthogonality and Linear Independence","text":"<ul> <li>Let \\(V\\) be an orthogonal set of vectors in the inner product space \\(V\\).</li> <li>Then \\(V\\) is linearly independent if and only if \\(V\\) is orthogonal.</li> </ul>"},{"location":"MATHS2/8.01%20-%20Orthogonality%20and%20Linear%20Independence/#proof","title":"Proof","text":"<ul> <li>If \\(V\\) is orthogonal, then \\(&lt;v_i,v_j&gt; = 0\\) for all \\(i \\neq j\\).</li> <li>Thus, \\(v_i\\) is orthogonal to \\(v_j\\) for all \\(i \\neq j\\).</li> <li>Thus, \\(v_i\\) is linearly independent of \\(v_j\\) for all \\(i \\neq j\\).</li> <li>Thus, \\(V\\) is linearly independent.</li> </ul>"},{"location":"MATHS2/8.01%20-%20Orthogonality%20and%20Linear%20Independence/#orthogonal-basis","title":"Orthogonal Basis","text":"<ul> <li>Let V be an inner product space. A basis consisting of mutually orthogonal vectors is called an orthogonal basis.   Since an orthogonal set of vectors is already linearly independent, an orthogonal set is a basis precisely when it is a maximal orthogonal set (i.e. there is no orthogonal set strictly containing this one).</li> <li>If \\(dim(V) = n\\), then   \\(\\(\\text{orthogonal basis = orthogonal set of n vectors.}\\)\\)</li> </ul>"},{"location":"MATHS2/8.01%20-%20Orthogonality%20and%20Linear%20Independence/#example_2","title":"Example","text":"<ul> <li>The standard basis</li> <li>\\(S = \\{(4,3, -2), (-3, 2, -3), (-5, 18, 17)\\}\\)</li> <li>Consider \\(\\mathbb{R}^2\\) with the inner product \\(&lt;(x_1,x_2),(y_1,y_2)&gt; = x_1y_1 - (x_2y_2 + x_2y_1) + 2x_2y_2\\)</li> <li>Then the basis \\(\\{(1,0), (0,1)\\}\\) is an orthogonal basis.</li> </ul>"},{"location":"MATHS2/8.02%20-%20Orthonormal%20Basis/","title":"What is an Orthonormal Set?","text":"<ul> <li>An orthonormal set of vectors of an inner product space \\(V\\) is an orthogonal set of vectors such that the norm of each vector of the set is \\(1\\).</li> <li>Suppose \\(S \\in V\\), then \\(S\\) is an orthonormal set if and only if   \\(\\(\\|v_i\\| = 1 \\text{ for all } i \\in \\{1,2, \\dots, k\\}\\)\\) \\(\\(&lt;v_i,v_j&gt; = 0 \\text{ for all } i,j \\in \\{1,2, \\dots, k\\} \\text{ and } i \\neq j\\)\\)</li> </ul>"},{"location":"MATHS2/8.02%20-%20Orthonormal%20Basis/#what-is-an-orthonormal-basis","title":"What is an Orthonormal Basis?","text":"<ul> <li>An orthonormal basis is an orthonormal set of vectors which forms a basis.</li> <li>Equivalently : An orthonormal basis is an orthogonal basis where the norm of each vector is 1.</li> <li>Equivalently : An orthonormal basis is a maximal orthonormal set.</li> <li>Example : The standard basis w.r.t. the usual inner product forms an orthonormal basis.</li> </ul>"},{"location":"MATHS2/8.02%20-%20Orthonormal%20Basis/#example","title":"Example","text":"<ul> <li>Consider \\(\\mathbb{R}^3\\) with the usual inner product. Then the set \\(S = \\{\\frac{1}{3}(,2, 2), \\frac{1}{3}(-2,-1, -2), \\frac{1}{3}(2, -2, 1)\\}\\) is an orthonormal basis in \\(\\mathbb{R}^3\\).</li> </ul>"},{"location":"MATHS2/8.02%20-%20Orthonormal%20Basis/#obtaining-an-orthonormal-sets-from-an-orthogonal-sets","title":"Obtaining an Orthonormal sets from an Orthogonal sets","text":"<ul> <li>Let V be an inner product space. If \\(S\\) is an orthogonal set of vectors, then we can obtain an orthonormal set of vectors \\(\\beta\\) from \\(S\\) by dividing each vector \\(v_i\\) by its norm.   \\(\\(\\beta = \\{\\frac{v_1}{\\|v_1\\|}, \\frac{v_2}{\\|v_2\\|}, \\dots, \\frac{v_n}{\\|v_n\\|}\\}\\)\\)</li> </ul>"},{"location":"MATHS2/8.02%20-%20Orthonormal%20Basis/#example_1","title":"Example","text":"<ul> <li>Consider \\(\\mathbb{R}^2\\) with the usual inner product. Then the set \\(S = \\{(4,3), (-3, 2), (-5, 18)\\}\\) is an orthogonal set of vectors.</li> <li>Then \\(\\beta = \\{\\frac{1}{\\sqrt{10}},(1,3), \\frac{1}{\\sqrt{10}(-3,1)}\\}\\) is an orthonormal set of vectors.</li> </ul>"},{"location":"MATHS2/8.02%20-%20Orthonormal%20Basis/#why-orthonormal-bases-are-important","title":"Why Orthonormal Bases are Important?","text":"<ul> <li>Suppose \\(S\\) is an orthonormal basis of an inner product space \\(V\\) and let \\(v \\in V\\)</li> <li>Then \\(v\\) can be written as a linear combination of the vectors in \\(S\\).   \\(\\(v = \\sum_{i=1}^n c_i v_i\\)\\)</li> <li>How do we find the coefficients \\(c_i\\)? For any basis, this means writing a system of linear equations and solving it.</li> <li>But since \\(\\gamma\\) is orthonormal, we can use the inner product and compute the coefficients \\(c_i\\) directly.   \\(\\(c_i = &lt;v, v_i&gt;\\)\\)</li> </ul>"},{"location":"MATHS2/8.02%20-%20Orthonormal%20Basis/#example_2","title":"Example","text":"<ul> <li>\\(\\{\\frac{1}{\\sqrt{10}},(1,3), \\frac{1}{\\sqrt{10}(-3,1)}\\) is an orthonormal basis of \\(\\mathbb{R}^2\\). Write \\((2,5)\\) as a linear combination in terms of these basis vectors.</li> <li>\\(c_1 = &lt;(2,5), \\frac{1}{\\sqrt{10}(1,3)}&gt;\\)</li> <li>\\(c_1 = \\frac{1}{\\sqrt{10}}(2\\cdot 1 + 5\\cdot 3) = \\frac{1}{\\sqrt{10}}(2 + 15) = \\frac{17}{\\sqrt{10}}\\)</li> <li>\\(c_2 = &lt;(2,5), \\frac{1}{\\sqrt{10}(-3,1)}&gt;\\)</li> <li>\\(c_2 = \\frac{1}{\\sqrt{10}}(2\\cdot (-3) + 5\\cdot 1) = \\frac{1}{\\sqrt{10}}(-6 + 5) = \\frac{-1}{\\sqrt{10}}\\)</li> </ul>"},{"location":"MATHS2/8.03%20-%20Projections%20using%20inner%20products/","title":"The projection of a vector a subspace","text":""},{"location":"MATHS2/8.03%20-%20Projections%20using%20inner%20products/#shortest-distance-in-mathbbr2","title":"Shortest Distance in \\(\\mathbb{R}^2\\)","text":"<ul> <li>\\(A\\) and \\(B\\) are points in the plane \\(\\mathbb{R}^2\\) and we want to find the nearest point from \\(B\\) on the line passing through \\(A\\) and the origin. Drop a perpendicular from \\(B\\) on to the line.   Let \\(a\\) and \\(b\\) be the vectors corresponding to the points A and B respectively.</li> </ul>"},{"location":"MATHS2/8.03%20-%20Projections%20using%20inner%20products/#defination","title":"Defination","text":"<ul> <li>Let \\(V\\) be an inner product space, \\(v \\in V\\) and \\(W \\subseteq V\\) be a subspace of \\(V\\). Then the projection of \\(v\\) onto \\(W\\) is in the vector in \\(proj_W(v),\\) computed as follows:   \\(\\(proj_W(v) = \\sum_{i=1}^n&lt;v,v_i&gt;v_i\\)\\)</li> <li>Fact : The definition is independent of the chosen orthonormal basis (i.e. the expression on the RHS does not change even if you choose a different orthonormal basis).</li> <li>The projection of \\(v\\) onto \\(W\\) is the vector in \\(W\\) closest to \\(v\\). Note that \"closest\" is in terms of the distance based on the norm   induced by the inner product.</li> </ul>"},{"location":"MATHS2/8.03%20-%20Projections%20using%20inner%20products/#example","title":"Example","text":"<ul> <li>\\(V = \\mathbb{R}^2, W = &lt;(3,1)&gt;,v = (1,3)\\)</li> <li>\\(\\frac{1}{\\sqrt{10}}(1,3)\\)</li> <li>\\(proj_W(v) = &lt;v,\\frac{1}{\\sqrt{10}}(1,3)&gt; \\times\\sqrt{10}(1,3) = &lt;(1,3),(3,1)&gt;\\frac{1}{10}(3,1) = \\frac{3+3}{10}(3,1) = \\frac{6}{10}(3,1)\\)</li> </ul>"},{"location":"MATHS2/8.03%20-%20Projections%20using%20inner%20products/#projection-on-a-vector-and-orthogonal-bases","title":"Projection on a vector and orthogonal bases","text":"<ul> <li>Let \\(V\\) be an inner product space and \\(v, w \\in V\\) is defined as follows:   \\(\\(proj_w(v) = proj_{&lt;w&gt;}(v)\\)\\)   $$ proj*w(v) =  \\frac{w}{||w||} = \\frac{}{||w||^2}w = \\frac{}{}w$$   \\(\\(proj_W(v) = \\sum_{i=1}^n\\frac{&lt;v,v_i&gt;}{&lt;v_i,v_i&gt;}v_i\\)\\)"},{"location":"MATHS2/8.03%20-%20Projections%20using%20inner%20products/#example_1","title":"Example","text":"<ul> <li>Let \\(W\\) be the 2-dimensional subpsace of \\(V = \\mathbb{R}^3\\) spanned by the orthogonal vectors \\(v_1 = (1,2,1)\\) and \\(v_2 = (1, \u20141, 1)\\). What is the projection of \\(v = (-2,2,2)\\) on \\(W\\).</li> <li>\\(proj_{v1}v = \\frac{&lt;v,v_1&gt;}{v_1,v_1}v_1 = \\frac{4}{6} (1,2,1) = \\frac{2}{3}(1,2,1)\\)</li> <li>\\(proj_{v2}v = \\frac{&lt;v,v_2&gt;}{v_2,v_2}v_2 = -\\frac{2}{3} (1,-1,1)\\)</li> <li>\\(proj_Wv = proj_{v1}v + proj_{v2}v = \\frac{2}{3}(1,2,1) - \\frac{2}{3} (1,-1,1) = (0,1,0)\\)</li> </ul>"},{"location":"MATHS2/8.03%20-%20Projections%20using%20inner%20products/#projection-as-a-linear-transformation","title":"Projection as a linear transformation","text":"<ul> <li>Let \\(V\\) be an inner product space and \\(W\\) be a subspace of \\(V\\). Then the projection of \\(v\\) onto \\(W\\) is the unique vector in \\(W\\) that is closest to \\(v\\) in the sense of the norm induced by the inner product.</li> <li>\\(P_w(v)\\) is the linear transformation that maps \\(v\\) to the projection of \\(v\\) onto \\(W\\).   \\(\\(P_w(v) = proj_W(v)\\)\\) \\(\\(P_W(v_1 + v_2) = P_W(v_1) + P_W(v_2) \\ \\And \\ P_W(cv) = cP(v)\\)\\)</li> <li>It follows all the properties of a linear transformation.</li> </ul>"},{"location":"MATHS2/8.03%20-%20Projections%20using%20inner%20products/#properties","title":"Properties","text":"<ul> <li>\\(P_W(v) =v, \\forall v \\in W\\)</li> <li>\\(Image(P_W) = W\\)</li> <li>\\(W^{\\perp} = \\{ v| v \\in V,\\) such that \\(&lt;v,w&gt; = 0 \\ \\forall w \\in W\\}\\) is the null space of \\(P_W\\)</li> <li>\\(P_W^2 = P_W\\)</li> <li>\\(||P_W(v)|| \\leq ||v||\\)</li> </ul>"},{"location":"MATHS2/8.04%20-%20Gram-Schmidt%20Process/","title":"Gram-Schmidt Process","text":""},{"location":"MATHS2/8.04%20-%20Gram-Schmidt%20Process/#definition","title":"Definition","text":"<ul> <li>The Gram-Schmidt process is a method for constructing an orthogonal basis from a given linearly independent set of vectors.</li> </ul>"},{"location":"MATHS2/8.04%20-%20Gram-Schmidt%20Process/#finding-an-orthogonal-basis-using-inner-products-and-projections","title":"Finding an Orthogonal Basis using inner products and projections","text":""},{"location":"MATHS2/8.04%20-%20Gram-Schmidt%20Process/#example-and-intuition","title":"Example and Intuition","text":"<ul> <li>Consider the basis \\(\\beta = \\{(1,2,2), (-1,0,2),(0,0,1)\\}\\) in \\(\\mathbb{R}^3\\).</li> <li>Let \\(v_1 = (1,2,2).\\) We want a vector which is orthogonal to $v_1 a vector in \\(&lt;v_1&gt;\\), so we can use the projection \\(P_{v_1}\\) to \\(v_1\\).</li> <li>Define \\(v_2 = (-1,0,2) - P_{v_1}((-1,0,2))\\)<ul> <li>\\(\\Rightarrow (-1,0,2) - \\frac{&lt;(-1,0,2), (1,2,2)&gt;}{&lt;(1,2,2), (1,2,2)&gt;} (1,2,2)\\)</li> <li>\\(\\Rightarrow (-\\frac{4}{3},-\\frac{2}{3}, \\frac{4}{3})\\)</li> </ul> </li> <li>\\(W^\\perp = \\{ v | &lt;v,w&gt; = 0 \\forall w \\in W\\} = \\text{ Nullspace of } P_W\\)</li> <li>\\(P_W(v) = 0 \\leftrightarrow v \\in W^\\perp\\)</li> <li>We want a vector which is orthogonal to both \\(v_1\\) and \\(v_2\\), a vector in the \\(Span(\\{v_1,v_2\\})^\\perp\\), so we can use the projection \\(P_{Span(\\{v_1,v_2\\})}\\) to \\(Span(\\{v_1,v_2\\})\\).</li> <li>Define \\(v_3 = (0,0,1) - P_{v_1}((0,0,1)) - P_{v_2}((0,0,1))\\)</li> <li>\\(\\Rightarrow (0,0,1) - \\frac{&lt;(0,0,1), (1,2,2)&gt;}{&lt;(1,2,2), (1,2,2)&gt;} (1,2,2) - \\frac{&lt;(0,0,1), (-\\frac{4}{3},-\\frac{2}{3}, \\frac{4}{3})&gt;}{&lt;(-\\frac{4}{3},-\\frac{2}{3}, \\frac{4}{3}), (-\\frac{4}{3},-\\frac{2}{3}, \\frac{4}{3})&gt;} (-\\frac{4}{3},-\\frac{2}{3}, \\frac{4}{3})\\)</li> <li>\\(\\Rightarrow (\\frac{2}{9}, -\\frac{2}{9}, \\frac{1}{9})\\)</li> </ul>"},{"location":"MATHS2/8.04%20-%20Gram-Schmidt%20Process/#finding-an-orthogonal-basis-using-the-gram-schmidt-process","title":"Finding an Orthogonal Basis using the Gram-Schmidt Process","text":""},{"location":"MATHS2/8.04%20-%20Gram-Schmidt%20Process/#algorithm","title":"Algorithm","text":"<ul> <li>Let \\(V\\) be an inner product space with basis \\(\\{ x_1, x_2, \\dots, x_n\\}\\). Define the orthogonal basis \\(\\{ v_1, v_2, \\dots, v_n\\}\\) and the corresponding orthonormal basis \\(\\{ w_1, w_2, \\dots, w_n\\}\\) as follows:</li> <li>\\(v_1 = x_1; w_1 = \\frac{v_1}{||v_1||}\\)</li> <li>\\(v_2 = x_2 - &lt;x_2,w_1&gt; w_1 ; w_2 = \\frac{v_2}{||v_2||}\\)</li> <li>\\(\\vdots\\)</li> <li>\\(v_n = x_n - &lt;x_n,w_1&gt; w_1 - \\dots - &lt;x_n,w_{n-1}&gt; w_{n-1} ; w_n = \\frac{v_n}{||v_n||}\\)</li> </ul>"},{"location":"MATHS2/8.05%20-%20Orthogonal%20Transformation%20and%20rotations/","title":"What are orthogonal transformations","text":"<ul> <li>Let \\(V\\) be an inner product space and \\(T\\) be a linear transformation from \\(V\\) to \\(V\\). \\(T\\) is said to be Orthogonal Transformation if:   \\(\\(&lt;Tv,Tw&gt; = &lt;v,w&gt; \\forall v, w \\in V\\)\\)</li> <li>When \\(V = \\mathbb{R}^n\\) with the usual inner product, a linear transformation \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) is orthogonal if and only if Preserves angles and lengths.</li> <li>It is enough to demand that the linear transformation pre-serves lengths.In that case, angles automatically get preserved (think of triangle congruences).</li> </ul>"},{"location":"MATHS2/8.05%20-%20Orthogonal%20Transformation%20and%20rotations/#finding-the-rotation-matrix-in-mathbbr2","title":"Finding the rotation matrix in \\(\\mathbb{R}^2\\)","text":"<ul> <li>Consider the standard basis \\(\\{ (1,0), (0,1)\\}\\) in \\(\\mathbb{R}^2\\). Rotate the plane by the angle \\(\\theta\\). The vectors obtained after the tell us the matrix corresponding to this linear transformation.   \\(\\(R_\\theta = \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{bmatrix}\\)\\)</li> </ul>"},{"location":"MATHS2/8.05%20-%20Orthogonal%20Transformation%20and%20rotations/#note","title":"Note","text":"<ul> <li>\\(R_\\theta^T = R_{-\\theta}\\)</li> <li>\\(R_\\theta \\times R_\\theta^T = R_\\theta^T \\times R_\\theta = I\\)</li> <li>Further note that since angles and lengths are preserved and the standard basis is orthonormal, the rotated vectors are also orthonormal and therefore yield an orthonormal basis of \\(\\mathbb{R}^2\\).</li> </ul>"},{"location":"MATHS2/8.05%20-%20Orthogonal%20Transformation%20and%20rotations/#finding-the-rotation-matrix-in-mathbbr3","title":"Finding the rotation matrix in \\(\\mathbb{R}^3\\)","text":"<ul> <li> <p>Consider the rotations about the axes in \\(\\mathbb{R}^3\\). Since these clearly preserve angles and distances and are linear transformations, they are orthogonal transformations.</p> </li> <li> <p>Rotations about the axes can be described by considering its effect on the standard basis \\(\\{e_1, e_2, e_3\\}.\\)</p> </li> <li> <p>When considering the rotation about the \\(Z-axis\\), \\(e_3\\) remains unchanged and the \\(X Y-\\text{plane}\\) gets rotated exactly as in the previous case of \\(\\mathbb{R}^2\\). Therefore its matrix is   \\(\\(T_3{\\theta} = \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta &amp; 0 \\\\ \\sin \\theta &amp; \\cos \\theta &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)\\)</p> </li> <li>If we fix the \\(X-axis\\) and rotate the \\(Y Z-\\text{plane}\\), we get   \\(\\(T_2{\\theta} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos \\theta &amp; -\\sin \\theta \\\\ 0 &amp; \\sin \\theta &amp; \\cos \\theta \\end{bmatrix}\\)\\)</li> <li>If we fix the \\(Y-axis\\) and rotate the \\(X Z-\\text{plane}\\), we get   \\(\\(T_1{\\theta} = \\begin{bmatrix} \\cos \\theta &amp; 0 &amp; \\sin \\theta \\\\ 0 &amp; 1 &amp; 0 \\\\ -\\sin \\theta &amp; 0 &amp; \\cos \\theta \\end{bmatrix}\\)\\)</li> </ul>"},{"location":"MATHS2/8.05%20-%20Orthogonal%20Transformation%20and%20rotations/#note_1","title":"Note","text":"<ul> <li>\\(T_i{\\theta}^T = T_i{-\\theta}\\)</li> <li>\\(T_i({\\theta}) \\times T_i({\\theta})^T = T_i({\\theta})^T \\times T_i({\\theta}) = I\\)</li> </ul>"},{"location":"MATHS2/8.05%20-%20Orthogonal%20Transformation%20and%20rotations/#example","title":"Example","text":"<ul> <li>A linear transformation \\(T: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3\\), where</li> <li>\\(T(x_1,x_2,x_3) = \\frac{1}{3}(x_1 - 2x_2 + 2x_3, 2x_1 - x_2 - 2x_3, 2x_1 + 2x_2 + x_3)\\)</li> <li>Then evaluating \\(T\\) on the standard basis \\(\\{e_1, e_2, e_3\\}\\), we get   \\(\\(T(e_1) = \\frac{1}{3}(1, 2, 2)\\)\\) \\(\\(T(e_2) = \\frac{1}{3}(-2, -1, 2)\\)\\) \\(\\(T(e_3) = \\frac{1}{3}(2, -2, 1)\\)\\)</li> <li>Therefore, the matrix corresponding to \\(T\\) is   \\(\\(T = \\frac{1}{3}\\begin{bmatrix} 1 &amp; -2 &amp; 2 \\\\ 2 &amp; -1 &amp; 2 \\\\ 2 &amp; -2 &amp; 1 \\end{bmatrix}\\)\\)   $$T^T = A^T = \\frac{1}{3}\\begin{bmatrix} 1 &amp; 2 &amp; 2 \\ -2 &amp; -1 &amp; -2 \\ 2 &amp; 2 &amp; 1 \\end{bmatrix}.</li> <li>As \\(\\{v_1,v_2,v_3\\}\\) is an othonormal basis set, the linear transformation \\(T\\) is Orthogonal Transformation.</li> <li>Because \\(A^T A = I_3\\)</li> </ul> <p>Yes, now that you have found the projections of the point \\((3,4,5)\\) onto the basis vectors \\(\\mathbf{u}_1=(1,0,-1)\\) and \\(\\mathbf{u}_2=(0,1,-1)\\), you can use them to find the closest point on the plane to the original point.</p> <p>To do this, you can use the fact that the closest point on the plane to a given point is the projection of that point onto the plane. Since the plane is defined by the equation \\(x+y+z=0\\), we can write its normal vector as \\(\\mathbf{n}=(1,1,1)\\).</p> <p>Then, the projection of the point \\((3,4,5)\\) onto the plane is given by:</p> \\[\\operatorname{proj}_{\\mathbf{n}}\\begin{pmatrix}3\\4\\5\\end{pmatrix}=\\left(\\frac{\\begin{pmatrix}3\\4\\5\\end{pmatrix}\\cdot\\mathbf{n}}{|\\mathbf{n}|^2}\\right)\\mathbf{n}=\\left(\\frac{3+4+5}{3}\\right)\\begin{pmatrix}1\\1\\1\\end{pmatrix}=\\begin{pmatrix}4\\4\\4\\end{pmatrix}\\] <p>So the closest point on the plane to the original point \\((3,4,5)\\) is \\((4,4,4)\\).</p> <p>To find the closest distance between the point \\((3,4,5)\\) and the plane, you can calculate the distance between the original point and the closest point on the plane using the distance formula:</p> \\[d=|\\begin{pmatrix}3\\4\\5\\end{pmatrix}-\\begin{pmatrix}4\\4\\4\\end{pmatrix}|=\\sqrt{(3-4)^2+(4-4)^2+(5-4)^2}=\\sqrt{3}\\] <p>So the closest distance between the point \\((3,4,5)\\) and the plane \\(x+y+z=0\\) is \\(\\sqrt{3}\\).</p>"},{"location":"MATHS2/9.01%20-%20Multivariable%20functions/","title":"Scalared valued multivariable functions","text":"<ul> <li>A sclar valued multivariable function is a function \\(f : D \\rightarrow \\mathbb{R}\\) where \\(D\\) is a domain in \\(\\mathbb{R}^n\\), where \\(n &gt; 1\\).</li> </ul>"},{"location":"MATHS2/9.01%20-%20Multivariable%20functions/#example","title":"Example","text":"<ul> <li>Linear transformation</li> <li>Polynomial functions</li> <li>(Arithmetic) combinations or compositions of functions</li> </ul>"},{"location":"MATHS2/9.01%20-%20Multivariable%20functions/#vector-valued-multivariable-functions","title":"Vector valued multivariable functions","text":"<ul> <li> <p>A sclar valued multivariable function is a function \\(f : D  \\rightarrow \\mathbb{R}^m\\) where \\(D\\) is a domain in \\(\\mathbb{R}^n\\), where \\(m,n &gt; 1\\).</p> </li> <li> <p>It can be thought of as a vector of scalar valued multivariable functions.</p> </li> <li>We have seen the example of a linear transformation.</li> </ul>"},{"location":"MATHS2/9.01%20-%20Multivariable%20functions/#multivariable-functions-or-function-of-several-variables","title":"Multivariable functions (or function of several variables)","text":"<ul> <li>A multivariable function or a function of several variables is either a scalar-valued multivariable function or a vector-valued multivariable function.</li> <li>When considering a multivariable functions, we will write \\(f : D \\rightarrow \\mathbb{R}^m\\) where \\(D\\) is a domain in \\(\\mathbb{R}^n\\) where \\(n &gt; 1\\) and with no restriction on \\(m\\) (it can also be 1).</li> <li>Further, if we want to refer to an element in \\(D\\) without bothering about the coordinates, we will use \\(x \\in D\\)</li> </ul>"},{"location":"MATHS2/9.01%20-%20Multivariable%20functions/#example_1","title":"Example","text":"<ul> <li>\\(f(x,y) = 2.5x - 3.5y\\)</li> <li>\\(f(x,y) = 2x^3 - 3y^2 + \\pi\\)</li> <li>\\(f(x,y) = \\sin(x^2 + y^2)\\)</li> <li>\\(f(x,y) = \\frac{1}{2\\pi} e^{\\frac{x^2+y^2}{2}}\\)</li> <li>\\(f(x,y) = 10x^{-2x-5y}\\)</li> </ul>"},{"location":"MATHS2/9.01%20-%20Multivariable%20functions/#arithmetic-operations-on-multivariable-functions","title":"Arithmetic operations on multivariable functions","text":"<ul> <li> <p>Let \\(D \\in \\mathbb{R}^n\\) and \\(f : D \\rightarrow \\mathbb{R}^m\\) be a multivariable function on \\(D\\).</p> </li> <li> <p>Addition</p> </li> <li>The sum function \\(f+g\\) is defined on \\(D\\) by:</li> <li>Then \\((f+g)(x) = f(x) +g(x)\\) for all \\(x \\in D\\).</li> <li>Scalar multiplication</li> <li>The scalar multiplication function \\(k \\cdot f\\) is defined on \\(D\\) by:</li> <li>Then \\((k \\cdot f)(x) = k \\cdot f(x)\\) for all \\(x \\in D\\).</li> <li>Multiplication</li> <li>If \\(m=1\\), the product function \\(fg\\) is defined on \\(D\\) by:</li> <li>\\(fg(x) = f(x)g(x)\\) for all \\(x \\in D\\).</li> <li>Division</li> <li>If \\(m=1\\), the quotient function \\(\\frac{f}{g}\\) is defined on \\(D\\) by:</li> <li>\\(\\frac{f}{g}(x) = \\frac{f(x)}{g(x)}\\) for all \\(x \\in D\\).</li> </ul>"},{"location":"MATHS2/9.01%20-%20Multivariable%20functions/#functions-obtained-by-composition","title":"Functions obtained by composition","text":"<ul> <li>Let \\(D \\subset \\mathbb{R}^n\\) and \\(f : D \\rightarrow \\mathbb{R}^m\\) be a multivariable function.</li> <li>Let \\(g: E \\rightarrow \\mathbb{R}^p\\) be a function on \\(E\\) where \\(Range(f) \\subseteq E\\).</li> <li>Then for each \\(x \\in D, f(x) \\in E\\) and therefore \\(g(f(x))\\) yields a well-defined element in \\(\\mathbb{R}^p\\).</li> <li>Thus, we obtain a multivariable function \\(g \\circ f: D \\rightarrow \\mathbb{R}^p\\) called the composition of \\(g\\) and \\(f\\) defined as follows:   \\(\\(g \\circ f(x) = g(f(x)), x \\in D\\)\\)</li> </ul>"},{"location":"MATHS2/9.01%20-%20Multivariable%20functions/#example_2","title":"Example","text":"<ul> <li>\\(f(x,y) = x^2 + y^2\\) is a function \\(\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\).</li> <li>\\(g(x) = \\sqrt{x}\\)</li> <li>Then \\(g \\circ f(x,y) = \\sqrt{x^2 + y^2}\\) is a function \\(\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\).</li> </ul>"},{"location":"MATHS2/9.01%20-%20Multivariable%20functions/#curves-in-mathbbrm","title":"Curves in \\(\\mathbb{R}^m\\)","text":"<ul> <li>A curves in \\(\\mathbb{R}^m\\) refers to the range of a function \\(f : D \\rightarrow \\mathbb{R}^m\\) where \\(D\\) is a domain in \\(\\mathbb{R}\\)</li> </ul>"},{"location":"MATHS2/9.01%20-%20Multivariable%20functions/#examples","title":"Examples","text":"<ul> <li>Line in \\(\\mathbb{R}^m\\)</li> <li>\\(\\gamma(f)\\) where \\(f\\) is a function of one variable</li> <li>Conics in \\(\\mathbb{R}^2\\)</li> <li>Helix in \\(\\mathbb{R}^3\\)</li> </ul>"},{"location":"MATHS2/9.02%20-%20Partial%20Derivative/","title":"Rate of change w.r.t. a particular variable at a point","text":"<ul> <li>Let \\(f(x_1, x_2, \\ldots, x_n)\\) be a function on domain \\(D\\) in \\(\\mathbb{R}^n\\) containing a point \\(a\\) and an open ball around it.</li> <li>Then the rate of change of \\(f\\) at \\(a\\) w.r.t. \\(x_i\\) is defined as:   \\(\\(\\lim_{h \\rightarrow 0} \\frac{f(a + he_i) - f(a)}{h}\\)\\) \\(\\(a = (a_1, a_2, \\ldots, a_n); e_i = (0, \\ldots, 0, 1, 0, \\ldots, 0)\\)\\)</li> </ul>"},{"location":"MATHS2/9.02%20-%20Partial%20Derivative/#example","title":"Example","text":"<ul> <li>\\(f(x,y) = x + y\\) at \\((0,0)\\)</li> <li>\\(\\lim_{h\\rightarrow 0}\\frac{f((0,0) + h(1,0)) - f(0,0)}{h}\\)</li> <li>\\(\\lim_{h\\rightarrow 0}\\frac{f(h,0) - f(0,0)}{h}\\)</li> <li>\\(\\lim_{h\\rightarrow 0}\\frac{h - 0}{h}\\)</li> <li>\\(\\lim_{h\\rightarrow 0}1 = 1\\)</li> <li>The rate of change of \\(f(x,y,z) = xy + yz + zx \\text { at } (1,2,3)\\) w.r.t. \\(y\\).</li> <li>\\(\\lim_{h\\rightarrow 0}\\frac{f((1,2,3)+h(0,1,0)) - f(1,2,3)}{h}\\)</li> <li>\\(\\lim_{h\\rightarrow 0}\\frac{f(1,2+h,3) - f(1,2,3)}{h}\\)</li> <li>\\(\\lim_{h\\rightarrow 0}\\frac{4h}{h} = 4\\)</li> <li>Th rate of change of \\(f(x,y) = \\sin(xy) \\text { at } (1,0)\\) w.r.t. x</li> <li>\\(\\lim_{h\\rightarrow 0}\\frac{f((1,0)+h(1,0)) - f(1,0)}{h}\\)</li> <li>\\(\\lim_{h\\rightarrow 0}\\frac{\\sin((1+h),0) - \\sin(1,0)}{h}\\)</li> <li>\\(\\lim_{h\\rightarrow 0}\\frac{0-0}{h}=0\\)</li> </ul>"},{"location":"MATHS2/9.02%20-%20Partial%20Derivative/#partial-derivative","title":"Partial Derivative","text":"<ul> <li>Let \\(f(x_1, x_2, \\ldots, x_n)\\) be a function on domain \\(D\\) in \\(\\mathbb{R}^n\\). The partial derivative of \\(f\\) w.r.t. \\(x_1\\) is the function denoted by \\(\\frac{\\partial f}{\\partial x_1}\\) defined on \\(D\\) by:   \\(\\(\\frac{\\partial f}{\\partial x_1}(x_1, x_2, \\ldots, x_n) = \\lim_{h \\rightarrow 0} \\frac{f(x_1 + h, x_2, \\ldots, x_n) - f(x_1, x_2, \\ldots, x_n)}{h}\\)\\)</li> </ul>"},{"location":"MATHS2/9.02%20-%20Partial%20Derivative/#-its-domain-consists-of-those-points-of-d-at-which-the-limits-exist","title":"- Its domain consists of those points of \\(D\\) at which the limits exist.","text":""},{"location":"MATHS2/9.02%20-%20Partial%20Derivative/#example_1","title":"Example","text":"<ul> <li>\\(f(x,y) = x + y\\)</li> <li> \\[\\frac{\\partial f}{\\partial x}(x,y) = \\lim_{h\\rightarrow 0}\\frac{f(x+h,y) - f(x,y)}{h}\\] </li> <li> \\[\\frac{\\partial f}{\\partial x}(x,y) = \\lim_{h\\rightarrow 0}\\frac{x+h+y-x-y}{h} = \\frac{h}{h}=1\\] </li> <li>Let's say we have a function that depends on two variables, x and y, given by:</li> <li>\\(f(x,y) = x^2y + xy^2\\)</li> <li>We can find the partial derivatives of this function with respect to x and y:</li> <li>\\(\\frac{\\partial f}{\\partial x} = 2xy + y^2\\)</li> <li>\\(\\frac{\\partial f}{\\partial y} = x^2 + 2xy\\)</li> <li> <p>Another example could be a function that depends on three variables, x, y, and z, such as:</p> </li> <li> <p>\\(g(x,y,z) = 2xy + z^3 - xz\\)</p> </li> <li>We can find the partial derivatives of this function with respect to x, y, and z:</li> <li>\\(\\frac{\\partial g}{\\partial x} = -z + 2y\\)</li> <li>\\(\\frac{\\partial g}{\\partial y} = 2x\\)</li> <li> <p>\\(\\frac{\\partial g}{\\partial z} = 3z^2 - x\\)</p> </li> <li> <p>A more complicated example could be a function that depends on four variables, w, x, y, and z, such as:</p> </li> <li> <p>\\(h(w,x,y,z) = 3w^2x + yz^2 - 2wxz\\)</p> </li> <li>We can find the partial derivatives of this function with respect to each variable:</li> <li>\\(\\frac{\\partial h}{\\partial w} = 6wx - 2xz\\)</li> <li>\\(\\frac{\\partial h}{\\partial x} = 3w^2 - 2wz\\)</li> <li>\\(\\frac{\\partial h}{\\partial y} = z^2\\)</li> <li> <p>\\(\\frac{\\partial h}{\\partial z} = 2yz - 2wx\\)</p> </li> <li> <p>In each of these examples, the partial derivative with respect to a particular variable is found by differentiating the function with respect to that variable while treating all other variables as constants.</p> </li> </ul>"},{"location":"MATHS2/9.03%20-%20Directional%20Derivatives/","title":"Rate of change in a perticular direction at a point","text":"<ul> <li>Let \\(f(x_1,x_2, \\dots, x_n)\\) be a function defined on a domain \\(D \\subset \\mathbb{R}^n\\) containing a point \\(a\\) and an open ball around it.</li> <li>Suppose instead of in the direction of axes, we are interested in the rate of change of the function \\(f\\) at \\(a\\) in some other direction.</li> <li>We can use the same idea as for partial derivatives and chose a unit vector \\(\\vec{u} = (u_1, u_2, \\dots, u_n)\\) in the direction of interest.   \\(\\(\\lim_{h \\to 0} \\frac{f(a_1 + hu_1, a_2 + hu_2, \\dots, a_n + hu_n) - f(a_1, a_2, \\dots, a_n)}{h}\\)\\)</li> </ul>"},{"location":"MATHS2/9.03%20-%20Directional%20Derivatives/#example","title":"Example","text":"<ul> <li>The rate of change of \\(f(x,y) = x+y\\) at \\((0,0)\\) in the direction of the \\(y=x\\) line.</li> <li>The unit vector is \\(\\vec{u} = (\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}})\\)</li> <li>\\(\\lim_{h \\to 0} \\frac{f(0 + h\\frac{1}{\\sqrt{2}}, 0 + h\\frac{1}{\\sqrt{2}}) - f(0,0)}{h} = \\lim_{h \\to 0} \\frac{h}{\\sqrt{2}} = \\frac{\\sqrt{2}h}{h}\\)</li> <li>The rate of change of \\(f(x,y,z) = xy + yz + zx\\) at \\((1,2,3)\\) in the direction of the vector \\((4,3,0)\\)</li> <li>To get the unit vector we divide the vector by its norm.</li> <li>\\(u = \\frac{v}{||v||} = \\frac{(4,3,0)}{\\sqrt{16+9}} = \\frac{(4,3,0)}{5}\\)</li> <li>\\(lim_{h \\to 0} \\frac{f(1 + h\\frac{4}{5}, 2 + h\\frac{3}{5}, 3) - f(1,2,3)}{h} = \\frac{32}{5}\\)</li> </ul>"},{"location":"MATHS2/9.03%20-%20Directional%20Derivatives/#properties","title":"Properties","text":"<ul> <li>Linearity: Let \\(c \\in \\mathbb{R}\\). If the directional derivative at the point \\(a\\) in the direction of the unit vector \\(u\\) exists for both the functions \\(f(x) \\text{ and } g(x)\\), then it also exists for \\((cf + g)(x)\\) and   \\(\\((cf + g)_u (a) = cf_u(a)+g_u(a)\\)\\)</li> <li>The Product Rule: If the directional derivative at the point \\(a\\) in the direction of the unit vector \\(u\\) exists for both the functions \\(f(x) \\text{ and } g(x)\\), then it also exists for \\((fg)(x)\\) and   \\(\\((fg)_u (a) = f_u(a)g(a) + f(a)g_u(a)\\)\\)</li> <li>The Quotient Rule: If the directional derivative at the point \\(a\\) in the direction of the unit vector \\(u\\) exists for both the functions \\(f(x) \\text{ and } g(x)\\), then it also exists for \\((\\frac{f}{g})(x)\\) and   \\(\\((\\frac{f}{g})_u (a) = \\frac{f_u(a)g(a) - f(a)g_u(a)}{g^2(a)}\\)\\)</li> </ul>"},{"location":"MATHS2/9.03%20-%20Directional%20Derivatives/#example_1","title":"Example","text":"<ul> <li>\\(f(x,y) = x +y\\)</li> <li>\\(\\Rightarrow \\lim_{h \\rightarrow 0} \\frac{f(x+hu_1,y + hu_2) - f(x,y)}{h}\\)</li> <li>\\(\\Rightarrow \\lim_{h \\rightarrow 0} \\frac{(x+hu_1) + (y + hu_2) - (x+y)}{h}\\)</li> </ul>"},{"location":"MATHS2/9.04%20-%20Limits%20for%20scalar-valued%20multivariable%20functions/","title":"Limits of sequences in \\(R^p\\)","text":"<ul> <li>Let \\(\\{ a_n \\}\\) be a sequence in \\(\\mathbb{R}^p\\).</li> <li>\\(a_n = (a_{n1}, a_{n2}, \\dots, a_{np})\\)</li> <li>We say that \\(\\{ a_n\\}\\) has a limit \\(a = (a_1, a_2, \\dots, a_p) \\in \\mathbb{R}^p\\) if as \\(n\\) increases, the sequence in the \\(i^{th}\\) coordinate has a limit \\(a_i\\).</li> <li>A sequence \\(\\{ a_n \\}\\) is convergent if it converges to some point in \\(\\mathbb{R}^p\\).</li> <li>A sequence \\(\\{ a_n \\}\\) is divergent if it does not converge to any point in \\(\\mathbb{R}^p\\).</li> <li>Subsequences of a sequence \\(\\{ a_n \\}\\) are sequences that can be obtained by removing some elements from the original sequence.</li> </ul>"},{"location":"MATHS2/9.04%20-%20Limits%20for%20scalar-valued%20multivariable%20functions/#limits-of-scalar-valued-multivariable-functions-at-a-point","title":"Limits of scalar-valued multivariable functions at a point","text":"<ul> <li>Let \\(f\\) be a scala-valued multivariable function defined on a domain \\(D\\) in \\(\\mathbb{R}^k\\) and \\(a\\) be a point such that there exists a sequence in \\(D\\) which converges to \\(a\\).</li> <li> <p>If tehre exists a real number \\(L\\) such that \\(f(a_n) \\rightarrow L\\) for all sequences \\(a_n\\) such that \\(a_n \\rightarrow a\\), then we say that the limit of \\(f\\) at \\(a\\) exists and is equal to \\(L\\). We write   \\(\\(\\lim_{x \\rightarrow a} f(x) = L\\)\\)</p> </li> <li> <p>\\(\\lim_{x \\rightarrow a} f(x) = L\\) is equivalent to : as \\(x\\) comes closer and closer to \\(a\\) and \\(f(x)\\) eventually comes closer to to \\(L\\).</p> </li> </ul> <p>If there is no such number \\(L\\) then we say that the limit of \\(f\\) at \\(a\\) does not exist.</p>"},{"location":"MATHS2/9.04%20-%20Limits%20for%20scalar-valued%20multivariable%20functions/#rules-for-limits-of-scalar-valued-multivariable-functions","title":"Rules for limits of scalar-valued multivariable functions","text":"<p>\\(\\text{If }\\) \\(\\lim_{x \\rightarrow a} f(x) = F\\), \\(\\lim_{x \\rightarrow a} g(x) = G\\)</p> <ul> <li>\\(\\lim_{x \\rightarrow a} (cf+g)(x) = cF + G\\)</li> <li>\\(\\lim_{x \\rightarrow a} (fg)(x) = FG\\)</li> <li>\\(\\lim_{x \\rightarrow a} (\\frac{f}{g})(x) = \\frac{F}{G}; \\iff G \\neq 0\\)</li> <li>Suppose \\(f\\) is a scalar-valued multivariable function and \\(g\\) is a function of one variable such that the composistion \\(g \\circ f\\) is defined on \\(D\\).</li> <li>\\(\\lim_{x \\rightarrow a} (g \\circ f)(x) = \\lim_{x \\rightarrow a} g(f(x))\\)</li> <li>Sandwich principle: if \\(\\lim_{x \\rightarrow a} f(x) = F\\) and \\(\\lim_{x \\rightarrow a} g(x) = F\\) and \\(f(x) \\leq h(x) \\leq g(x)\\) then \\(\\lim_{x \\rightarrow a} h(x) = F\\)</li> </ul>"},{"location":"MATHS2/9.05%20-%20Limits%20of%20vector-valued%20function%20at%20a%20point/","title":"Limit of a vector valued fucntion at a point","text":"<ul> <li>Let \\(f : D \\rightarrow \\mathbb{R}^m\\) be a vector valued multivariable function defined on the domain \\(D\\) in \\(\\mathbb{R}^k\\) and \\(a\\) be a point such that there exists a sequence in \\(D\\) which converges to \\(a\\).</li> <li>If \\(f_i\\) is the \\(i^{th}\\) component of \\(f\\) then the limit of \\(f\\) at \\(a\\) is defined as scalar valued function from \\(D\\) to \\(\\mathbb{R}\\). Suppose for each \\(i\\) the limit \\(\\lim_{x \\rightarrow a} f_i(x)\\) exists. and equals \\(L_i\\) then:   \\(\\(\\lim_{x \\rightarrow a} f(x) = L\\)\\)</li> <li>This is equivalent to \\(:\\) as \\(x\\) comes closer and closer to \\(a\\), \\(f(x)\\) eventually comes closer and closer to \\(L\\).</li> <li>If for some \\(i\\), the limit \\(f_i\\) at \\(a\\) does not exist, then the limit of \\(f\\) at \\(a\\) does not exist.</li> </ul>"},{"location":"MATHS2/9.05%20-%20Limits%20of%20vector-valued%20function%20at%20a%20point/#limit-of-function-at-a-point-along-a-curve","title":"Limit of function at a point along a curve","text":"<ul> <li>Let \\(f\\) be a scalar-valued mutlivariable function defined on the domain \\(D\\) in \\(\\mathbb{R}^k\\) and \\(a\\) be a point such that there exists a sequence in \\(D\\) which converges to \\(a\\).</li> <li>Let \\(C\\) be a curve passing through the point \\(a\\) belonging to the domain \\(D\\)</li> <li>The limit of \\(f\\) at \\(a\\) along the curve \\(C\\) exists and equals \\(L\\) if for every sequence \\(a_n\\) contained in \\(C\\) which converges to \\(a\\), the sequence \\(f(a_n)\\) converges to \\(L\\).</li> </ul>"},{"location":"MATHS2/9.05%20-%20Limits%20of%20vector-valued%20function%20at%20a%20point/#theorm","title":"Theorm","text":"<ul> <li>The limit of \\(f\\) at \\(a\\) exists and equals \\(L\\) precisely when for every curve \\(C\\) in the domain \\(D\\) passing through \\(a\\) the limit of \\(f\\) at \\(a\\) along \\(C\\) exists and equals \\(L\\).</li> </ul>"},{"location":"MATHS2/9.05%20-%20Limits%20of%20vector-valued%20function%20at%20a%20point/#continuity-of-a-function","title":"Continuity of a function","text":"<ul> <li>Let \\(f\\) be a multivariable function defined on a domain \\(D\\) in \\(\\mathbb{R}^k\\) and \\(a \\in D\\) be a point such that there exists a sequence in \\(D\\) which converges to \\(a\\).</li> <li>Defination \\(:\\) \\(f\\) is conitnuous at \\(a\\) if the limit of \\(f\\) at \\(a\\) exists and \\(\\lim_{x \\rightarrow a} f(x) = f(a)\\). \\(f\\) is continuous at \\(a\\) is equivalent to \\(f(a_n) \\rightarrow f(a)\\) as \\(a_n \\rightarrow a\\).</li> <li>NOTE \\(:\\) continuity means that the limit at \\(a\\) can be obtained by evaluating the function at \\(a\\).</li> </ul>"},{"location":"MATHS2/9.06%20-%20Directional%20Derivatives%20in%20terms%20of%20Gradients/","title":"The Gradient vector / function","text":"<ul> <li>Let \\(f(x_1,x_2, \\dots, x_n)\\) be a function defined on a domain in \\(D\\) in \\(\\mathbb{R}^n\\) containing some open ball around the point \\(a\\).</li> <li>Suppose all the partial derivatives of \\(f\\) exist at \\(a\\). Then the gradient vector of \\(f\\) at \\(a\\) is the vector \\((f_{x_1}(a),f_{x_2}(a),\\dots,f_{x_n}(a))\\) in \\(\\mathbb{R}^n\\). It is denoted by \\(\\nabla f(a)\\).</li> <li>The gradient function of \\(f\\) is a function taking values in \\(\\mathbb{R}^n\\) obtained by associating to every point \\(a\\) its gradient vector \\(\\nabla f(a)\\).</li> <li>The domain of \\(\\nabla f\\)is the set of points in \\(D\\) all of whose partial derivatives exist.</li> </ul>"},{"location":"MATHS2/9.06%20-%20Directional%20Derivatives%20in%20terms%20of%20Gradients/#examples","title":"Examples","text":"<ul> <li>Let \\(f(x,y) = \\sin(xy)\\)</li> <li>Then \\(\\nabla f(a) = (y\\cos(xy),x\\cos(xy))\\)</li> <li>Let \\(f(x,y,z) = x^2+y^2+z^2\\)</li> <li>Then \\(\\nabla f(a) = (2x,2y,2z)\\)</li> </ul>"},{"location":"MATHS2/9.06%20-%20Directional%20Derivatives%20in%20terms%20of%20Gradients/#properties-of-the-gradient","title":"Properties of the Gradient","text":"<ul> <li> <p>Linearity</p> </li> <li> \\[\\nabla (cf+g)(x) = c\\nabla f(x) + \\nabla g(x)\\] </li> <li> <p>Chain Rule</p> </li> <li> \\[\\nabla (fg)(x) = f(x)\\nabla g(x) + g(x)\\nabla f(x)\\] </li> <li> <p>Quotient Rule</p> </li> <li> \\[\\nabla \\frac{f}{g}(x) = \\frac{g(x)\\nabla f(x) - f(x)\\nabla g(x)}{g^2(x)}\\] </li> </ul>"},{"location":"MATHS2/9.06%20-%20Directional%20Derivatives%20in%20terms%20of%20Gradients/#note","title":"Note","text":"<ul> <li>Suppose \\(\\nabla f\\) exists and is continuous on some open vall around the point \\(a\\). Then for every unit vector \\(u\\), the directional derivative \\(f_u(a)\\) exists and equals \\(\\nabla f(a) \\cdot u\\).er</li> </ul>"},{"location":"STATS2/TO%20ASK/","title":"TO ASK","text":"<p>AQ1.1 Q3) How to solve it https://seek.onlinedegree.iitm.ac.in/courses/ns_23t1_ma1004?id=66&amp;type=assignment&amp;tab=courses</p>"},{"location":"STATS2/Books%20%2B%20Link/LINKS/","title":"Links for Topics","text":"<ul> <li>Multiple Discrete Random Variables -  https://www.youtube.com/watch?v=CqYuEwwNUu8</li> <li>Multiple Random Variables: Discrete and Continuous- https://www.youtube.com/watch?v=XhXhI_NaoCc</li> <li>Multiple random variables  https://www.youtube.com/watch?v=1U537aiXJzM</li> <li>Multiple random variables with densities - https://www.youtube.com/watch?v=AR3SoXCvw8I</li> <li>Discrete random variables - part 1/5 (continuous vs discrete) - https://www.youtube.com/watch?v=ajLFqrPTAcY</li> </ul> <p>part 2: https://youtu.be/FrL4Dcoy9MI part 3: https://youtu.be/NXUkzZhrrcA part 4: https://youtu.be/cnJjKX5AHi4 part 5: https://youtu.be/NTWD-EyTkR0</p>"},{"location":"STATS2/WEEK%201/aq_solutions/","title":"Assignment Question Solutions","text":"<p>Warning</p> <ul> <li>Please try to solve the questions on your own first.</li> <li>Thinking that \"I have seen the solutions , I understand everything\" is WRONG. You actually don't understand anything unless you solve questions.</li> <li>For detailed explanations look for posts on discourse or scroll through live lectures.</li> </ul>"},{"location":"STATS2/WEEK%201/aq_solutions/#aq-11","title":"AQ 1.1","text":""},{"location":"STATS2/WEEK%201/aq_solutions/#aq12","title":"AQ1.2","text":""},{"location":"STATS2/WEEK%201/aq_solutions/#aq13","title":"AQ1.3","text":""},{"location":"STATS2/WEEK%201/aq_solutions/#aq14","title":"AQ1.4","text":""},{"location":"STATS2/WEEK%201/aq_solutions/#aq15","title":"AQ1.5","text":""},{"location":"STATS2/WEEK%201/aq_solutions/#aq16","title":"AQ1.6","text":""},{"location":"STATS2/WEEK%201/aq_solutions/#aq17","title":"AQ1.7","text":""},{"location":"STATS2/WEEK%201/Notes/Two%20Discrete%20Random%20Variables/","title":"Discrete Random Variables","text":"<p>Discrete random variables are variables that can take on a countable number of distinct values.  These values are typically integers or whole numbers and are often the result of counting or enumerating something. </p> <p>Example</p> <ul> <li>Coin Toss : Consider a random variable \\(X\\) which represents the number of heads when a coin is tossed \\(2\\) times. We can see that \\(X\\) can only take the values \\(0 , 1 , 2\\). Here \\(X\\) is a discrete random variable.</li> <li>Russian Roulette : Consider a random variable \\(Y\\). \\(Y = 0\\) when a bullet is not fired and \\(Y=1\\) when  a bullet is fired. \\(Y\\) takes the values of \\(0,1\\). Here \\(Y\\) is a discrete random variable.</li> <li>Dice Roll : Consider a random variable \\(Z\\). \\(Z\\) can only take values \\(0,1,2,3,4,5,6\\) and each of these  values has a corresponding probability. Here \\(Z\\) is a discrete random variable.</li> </ul>"},{"location":"STATS2/WEEK%201/Notes/Two%20Discrete%20Random%20Variables/#takeaways-from-this-week","title":"Takeaways from this Week","text":"<ul> <li>You should be able to understand how the joint distribution tables work.</li> <li>Marginal PMF and Conditional Distribution are the most important concept in this week and will be used in the future.</li> </ul>"},{"location":"STATS2/WEEK%201/Notes/Two%20Discrete%20Random%20Variables/#joint-pmf","title":"Joint PMF","text":"<p>A joint PMF (Probability Mass Function) distribution refers to the probability distribution of two or  more random variables occurring together. It gives the probabilities for all possible combinations of values of these variables.</p> <p>Suppose \\(X\\) and \\(Y\\) are discrete random variables defined in the same probability space. Let the range of \\(X\\) and \\(Y\\) be \\(T_X\\) and \\(T_Y\\) , respectively . The joint PMF of \\(X\\) and \\(Y\\) , denoted by \\(f_{XY}\\) , is a function from \\(T_X \\times T_Y\\) to [0,1] defined as</p> \\[f_{XY} = P(X = t_1 \\ and \\ Y=t_2) , t_1 \\in T_X , t_2 \\in T_Y\\] <p>It is usually written in a table or a matrix.</p> <p>Example</p> <p>Coin Toss</p> <p>Let \\(X_i = 1\\) if \\(i^{th}\\) toss is heads and \\(X_i = 0\\) if toss is tales.</p> \\(t_2\\) \\ \\(t_1\\) 0 1 0 1/4 1/4 1 1/4 1/4 <p>Then , \\(f_{XY}(0,0) = P(X_1=0,X_2=0) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}\\) </p> <p>Picking Marbles</p> <p>Consider a bag containing three red marbles (R) and two blue marbles (B).  We randomly select two marbles from the bag without replacement and observe their colors.</p> <p>Joint PMF Distribution would provide all the possibilites of picking 2 marbles from the bag.</p> Marble 1 Marble 2 Probability (P) R R 3/10 R B 3/10 B R 3/10 B B 1/10 <p>Let \\(X,Y\\) represent the color of first and second marbles respectively. Then,</p> <ul> <li>\\(f_{XY}(R,R) = \\frac{3}{5} \\times \\frac{2}{4} = \\frac{3}{10}\\) (Both marbles are red)</li> <li>\\(f_{XY}(R,B) = \\frac{3}{5} \\times \\frac{2}{4} = \\frac{3}{10}\\) (\\(X\\) is red and \\(Y\\) is blue)</li> </ul> <p>Info</p> <p>The sum of all the Joint PMFs will always be 1</p>"},{"location":"STATS2/WEEK%201/Notes/Two%20Discrete%20Random%20Variables/#marginal-pmf-probability-mass-function","title":"Marginal PMF (Probability Mass Function)","text":"<p>The marginal PMF (Probability Mass Function) refers to the probabilities of a single random variable,  independently of the other variables in a joint probability distribution.</p> <p>Suppose \\(X\\) and \\(Y\\) are jointly distributed discrete random variables with joint PMF \\(f_{XY}\\). The PMF of the individual random variables \\(X\\) and \\(Y\\) are called as marginal PMFs. It can be shown that</p> \\[f_X(t)=P(X=t)=\\sum_{t'\\in T_Y} f_{XY}(t,t')\\] \\[f_Y(t)=P(Y=t)=\\sum_{t'\\in T_X} f_{XY}(t,t')\\] <p>where \\(T_X\\) and \\(T_Y\\) are the ranges of \\(X\\) and \\(Y\\) , respectively.</p> <p>Example</p> \\(t_2\\) \\ \\(t_1\\) 0 1 \\(f_{x_{2}}(t_2)\\) 0 1/4 1/4 1/2 1 1/4 1/4 1/2 \\(f_{x_{1}}(t_1)\\) 1/2 1/2 <ul> <li>Adding along the rows and adding along the columns gives us the Marginal PMF.</li> <li> <p>In our case the marignal PMF of \\(X_1\\) will be</p> <ul> <li>\\(f_{X_1}(0) = f_{X_{1}X_2}(0,0) + f_{X_{1}X_2}(0,1) = \\frac{1}{4} + \\frac{1}{4}\\)</li> <li>\\(f_{X_1}(1) = f_{X_{1}X_2}(1,0) + f_{X_{1}X_2}(1,1) = \\frac{1}{4} + \\frac{1}{4}\\)</li> </ul> </li> </ul>"},{"location":"STATS2/WEEK%201/Notes/Two%20Discrete%20Random%20Variables/#conditional-distribution-of-one-random-variable-given-another","title":"Conditional distribution of one random variable given another","text":"<p>Conditional distribution refers to the probability distribution of one random variable given the knowledge  or condition of another random variable. It provides the probabilities for the values of one variable,  taking into account specific conditions or values of another variable.</p> <p>In simpler terms, it allows us to understand how the distribution of one variable changes  or is affected when we consider a specific condition or value of another variable.</p> <p>Suppose \\(X\\) and \\(Y\\) are jointly distributed discrete random variables with joint PMF \\(f_{XY}\\). The conditional PMF of \\(Y\\) given \\(X=t\\) is defined as the PMF:</p> \\[Q(t')=P(Y=t'|X=t)=\\frac{P(Y=t' , X=t)}{P(X=t)} = \\frac{f_{XY}}{f_{X}(t)}\\] <p>Example</p> \\(t_2\\) \\ \\(t_1\\) 0 1 2 \\(f_Y(t_2)\\) 0 1/4 1/8 1/8 1/2 1 1/8 1/8 1/4 1/2 \\(f_X(t_1)\\) 3/8 1/4 3/8 <p>Where , \\(X \\in \\{0,1,2\\}\\) , \\(Y \\in \\{0,1\\}\\) and \\((Y|X=0) \\in \\{0,1\\}\\)</p> <ul> <li>\\(f_{Y|X = 0}(0) = \\frac{f_{XY}(0,0)}{f_X(0)} = \\frac{1/4}{3/8} = \\frac{2}{3}\\)</li> <li>\\(f_{Y|X = 0}(1) = \\frac{f_{XY}(0,1)}{f_X(0)} = \\frac{1/8}{3/8} = \\frac{1}{3}\\)</li> </ul>"},{"location":"STATS2/WEEK%201/Notes/Two%20Discrete%20Random%20Variables/#factoring","title":"Factoring","text":"<p>This is just a formula , mug it up \ud83d\udc80.</p> \\[\\begin{align*} f_{X_1X_2X_3X_4}(t_1,t_2,t_3,t_4) = P(X_4 = t_4 , X_3 = t_3 , X_2 = t_2 , X_1 = t_1) \\\\ \\implies f_{X_4 \\mid X_3 = t_3 , X_2 = t_2 , X_1=t_1}(t_4) \\times f_{X_3 \\mid X_2 = t_2 , X_1=t_1}(t_3) \\times f_{X_2 \\mid X_1=t_1}(t_1) \\times f_{X_1}(t_1) \\end{align*}\\]"},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/","title":"Parameter Estimation 2","text":""},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#bayesian-estimation","title":"Bayesian Estimation","text":"<p>\\(\\(X_1 , X_2 .... X_n \\sim , \\text{parameter } \\Uptheta\\)\\) - Prior distribution of \\(\\Uptheta\\) : \\(\\Uptheta \\sim f(\\Uptheta)\\) - Bayes' rule : posterior \\(\\propto\\) likelihood \\(\\times\\) prior </p> <p>\\(\\(P(\\Uptheta = \\theta | S) = \\frac{P(S|\\Uptheta = \\theta) \\times f_{\\Uptheta}(\\theta)}{P(S)}\\)\\) ![[Pasted image 20230410153658.png]]</p>"},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#choice-of-priors","title":"Choice of Priors","text":""},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#types-of-priors","title":"Types of Priors","text":"<ul> <li> <p>Flat , uninformative: </p> <ul> <li>Nearly flat over the interval in which the parameter takes value </li> <li>This usually reduces to something close to maximum likelihood.</li> </ul> </li> <li> <p>Conjugate Priors: </p> <ul> <li>Pick a prior so that the posterior is in the same class as the prior.</li> </ul> </li> <li> <p>Informative Priors: </p> <ul> <li>This needs some justification from the domain of the problem.</li> <li>Parameterize the prior so that its flatness can be controlled.</li> </ul> </li> </ul>"},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#examples","title":"Examples","text":""},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#bernoulli-distribution","title":"Bernoulli Distribution","text":"<p>\\(\\(X_1 , X_2 .... X_n \\sim \\text{iid } \\text{Bernoulli}(\\mathbf{p})\\)\\) - Prior \\(\\mathbf{p} \\sim \\text{Uniform}[0,1]\\) , continuous distribution  - Samples: \\(x_1 , x_2 , x_3 ..... x_n\\) - \\(w = x_1 + x_2 + ... + x_n\\)  - Posterior density: \\(\\text{Beta}(w + 1 , n -w + 1)\\)     - Posterior Mean: \\(\\frac{w+ 1}{(w+1) + (n-w+1)} = \\frac{w+1}{n+2} =\\frac{x_1 + x_2 + ... +x_n + 1}{n+2}\\)</p>"},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#bernoulli-distribution-with-beta-prior","title":"Bernoulli Distribution with beta prior","text":"<p>\\(\\(X_1 , X_2 .... X_n \\sim \\text{iid } \\text{Bernoulli}(\\mathbf{p})\\)\\) - Prior \\(\\mathbf{p} \\sim \\text{Beta}(\\alpha,\\beta)\\) , continuous distribution  - \\(w = x_1 + x_2 + ... + x_n\\)  - Posterior Density: \\(\\text{Beta}(w + \\alpha , n - w + \\beta)\\)     - Posterior Mean: \\(\\frac{w + \\alpha}{(w + \\alpha) + (n - w + \\beta)} = \\frac{w + \\alpha}{n + \\alpha + \\beta}\\)</p>"},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#observations-for-beta-prior","title":"Observations for Beta Prior","text":"<ul> <li>Prior : \\(\\text{Beta}(\\alpha , \\beta)\\)<ul> <li>\\(\\alpha , \\beta \\geq 0\\)</li> <li>PDF \\(\\propto p^{\\alpha - 1}(1-p)^{\\beta -1} , 0 &lt; p &lt; 1\\)</li> </ul> </li> <li>\\(\\alpha = \\beta = 1\\)<ul> <li>Flat prior </li> <li>Estimate close to but not equal to maximum -likelihood </li> </ul> </li> <li>\\(\\alpha = \\beta = 0\\)<ul> <li>Estimate coincides with Maximum-likelihood.</li> </ul> </li> <li>\\(\\alpha = \\beta\\)<ul> <li>Symmetric Prior </li> </ul> </li> </ul>"},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#normal-sample-with-known-mean-and-known-variance","title":"Normal sample with known mean and known variance","text":"<p>\\(\\(X_1 , X_2 .... X_n \\text{iid Normal}(M ,\\sigma^2)\\)\\) Prior \\(M \\sim Normal(\\mu_{0} , \\sigma_{0}^{2})\\) , continuous distribution  \\(f_M(\\mu) = \\frac{1}{\\sqrt{2 \\pi}\\sigma_0} \\text{exp}(- \\frac{(\\mu - \\mu_0)^2}{2 \\sigma_{0}^{2}})\\)</p> <ul> <li>Posterior Density : Normal </li> <li>Posterior Mean = \\(\\overline{x}\\frac{n \\sigma_{0}^{2}}{n \\sigma_{0}^{2} + \\sigma^2} + \\mu_0 \\frac{\\sigma^2}{n \\sigma_{0}^{2} + \\sigma^2}\\)</li> </ul>"},{"location":"STATS2/WEEK%2011/Hypothesis%20Testing/","title":"Hypothesis Testing","text":"<p>Using samples , decide between a null hypothesis denoted \\(H_0\\) and an alternative hypothesis denoted \\(H_A\\) .</p>"},{"location":"STATS2/WEEK%2011/Hypothesis%20Testing/#acceptance-set-and-test","title":"Acceptance Set and Test","text":"<p>\\(X_1 , X_2 .... X_n \\sim X , H_0 : \\text{null hypothesis , }H_A : \\text{alternative hypothesis.}\\) - Suppose \\(X \\in \\symbfscr{X}\\). Then the samples \\(X_1 , X_2 ... X_n \\in \\symbfscr{X}^n\\) - Subset \\(A \\subseteq \\symbfscr{X}\\)  If \\(X_1 , X_2 ... X_n \\in A\\) we accept \\(H_0\\) otherwise we reject \\(H_0\\)</p>"},{"location":"STATS2/WEEK%2011/Hypothesis%20Testing/#size-and-power-of-test","title":"Size and Power of Test","text":"<p>Type I error: It is also known as the size of a test , denoted as \\(\\alpha\\) - Reject \\(H_0\\) when \\(H_0\\) is true  - \\(\\alpha = P(\\text{Type I error}) = P(\\text{Reject }H_0 | H_0 \\text{ is true})\\) Type II error: It is also known as the power of a test , denoted as \\(1 - \\beta\\)  - Accept \\(H_0\\) when \\(H_A\\) is true  - \\(\\beta = P(\\text{Type II Error}) = P(\\text{Accept } H_0|H_A \\text{ is true})\\)</p>"},{"location":"STATS2/WEEK%2011/Hypothesis%20Testing/#types-of-tests","title":"Types of Tests","text":"<pre><code>$c$ = critical value\n</code></pre>"},{"location":"STATS2/WEEK%2011/Hypothesis%20Testing/#right-tailed-test","title":"Right Tailed Test","text":"<p>\\(H_0 : \\mu = \\mu_0\\) \\(H_A : \\mu &gt; \\mu_0\\) \\(T = \\overline{X}\\) Rejection Region: \\(\\overline{X} &gt; c\\) \\(Z = \\frac{\\overline{X} - \\mu_0}{\\sigma / \\sqrt{n}}\\)</p>"},{"location":"STATS2/WEEK%2011/Hypothesis%20Testing/#left-tailed-test","title":"Left Tailed Test","text":"<p>\\(H_0 : \\mu = \\mu_0\\) \\(H_A : \\mu &lt; \\mu_0\\) \\(T = \\overline{X}\\) Rejection Region: \\(\\overline{X} &lt; c\\)</p>"},{"location":"STATS2/WEEK%2011/Hypothesis%20Testing/#two-tailed-test","title":"Two Tailed Test","text":"<p>\\(H_0 : \\mu = \\mu_0\\) \\(H_A : \\mu \\neq \\mu_0\\) \\(T = \\overline{X}\\) Rejection Region: \\(|\\overline{X} - \\mu| &gt; c\\)</p>"},{"location":"STATS2/WEEK%2012/Different%20Types%20of%20Tests/","title":"Different Types of Tests","text":""},{"location":"STATS2/WEEK%2012/Different%20Types%20of%20Tests/#normal-samples-and-statistics","title":"Normal Samples and Statistics","text":"\\[X_1 , X_2 , X_3 ..... X_n \\sim iid \\text{ Normal}(\\mu , \\sigma^2)$$ - Sample Mean $\\overline{X} = \\frac{1}{n}(X_1 + X_2 + X_3 .... + X_n)$ - Sample Variance $S^2 = \\frac{1}{n-1}((X_1 - \\overline{X})^2 + (X_2 - \\overline{X})^2 + ...... + (X_n - \\overline{X})^2)$ $$\\overline{X} \\sim \\text{ Normal}(\\mu , \\frac{\\sigma^2}{n})$$ $$\\frac{(n-1)}{\\sigma^2}S^2 \\sim \\chi^{2}_{n-1}$$ $$\\frac{\\overline{X} - \\mu}{S / \\sqrt{n}} \\sim t_{n-1}\\]"},{"location":"STATS2/WEEK%2012/Different%20Types%20of%20Tests/#t-test-for-mean-unknown-variance","title":"T-Test for Mean (unknown variance)","text":"<p>\\(\\(X_1 , X_2 ,  ...... ,X_n \\sim \\text{ Normal}(\\mu , \\sigma^2)\\)\\) - Null \\(H_0 : \\mu = \\mu_0, \\text{ Alternative } H_A : \\mu &gt; \\mu_0\\) - \\(T = \\overline{X}\\) , Test : Reject \\(H_0 \\text{ if } T &gt; c\\)</p> <p>Computing Significance Level - Sample Variance \\(S^2 = \\frac{1}{n-1}\\sum^{n}_{i=1}(X_i - \\overline{X})^2\\) - Given , \\(H_0 ,\\frac{T - \\mu_0}{S /  \\sqrt{n}} \\sim t_{n-1}\\) \\(\\(\\alpha = P(T &gt; c | \\mu = \\mu_0) = P(t_{n-1} &gt; \\frac{c - \\mu_0}{S / \\sqrt{n}}) = 1 - F_{t_{n-1}}(\\frac{c- \\mu_0}{S/ \\sqrt{n}})\\)\\)</p>"},{"location":"STATS2/WEEK%2012/Different%20Types%20of%20Tests/#mathbfchi2-test-for-variance","title":"\\(\\mathbf{\\chi^2}\\) test for variance","text":"<p>\\(\\(X_1 , X_2 , ..... iid \\text{ Normal}(\\mu , \\sigma^2)\\)\\) - Null \\(H_0 : \\sigma = \\sigma_0\\) , Alternative \\(H_A : \\sigma &gt; \\sigma_0\\) - \\(S^2 = \\frac{1}{n-1}\\sum^{n}_{i=1}(X - \\overline{X})^2\\) ,  Test: Reject \\(H_0\\) if \\(S &gt;c\\)</p> <p>Computing Significance Level - Given \\(H_0\\) , \\(\\frac{n-1}{\\sigma_0^2}S^2 \\sim \\chi^{2}_{n-1}\\) - \\(\\alpha = P(S &gt; c | H_0) = P(\\frac{n-1}{\\sigma_0^2})S^2 &gt; \\frac{n-1}{\\sigma_0^2})c^2 = 1 - F_{\\chi^{2} _{n-1}}(\\frac{n-1}{\\sigma_0^2}c^2)\\)</p>"},{"location":"STATS2/WEEK%2012/Different%20Types%20of%20Tests/#two-samples-from-normal-distribution","title":"Two Samples from Normal Distribution","text":"<p>\\(\\(X_1 , X_2 , X_3 , ...... X_n \\sim \\text{iid Normal}(\\mu_1 , \\sigma^2_1 )\\)\\) \\(\\(Y_1 , Y_2 , Y_3 , ...... Y_n \\sim \\text{iid Normal}(\\mu_2 , \\sigma^2_2 )\\)\\) - \\(\\overline{X} \\sim \\text{ Normal}(\\mu_1 , \\sigma_1^2 / n_1)\\) , \\(\\overline{Y} \\sim \\text{ Normal}(\\mu_2 , \\sigma_2^2 / n_2)\\) - \\((\\frac{n_1-1}{\\sigma^2_1})S^2_X \\sim \\chi^2_{n_1 - 1}\\) , \\((\\frac{n_2-1}{\\sigma^2_2})S^2_Y \\sim \\chi^2_{n_2 - 1}\\) - \\(\\overline{X} - \\overline{Y} \\sim \\text{Normal}(\\mu_1 - \\mu_2 , \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2})\\) - If \\(\\sigma_1 = \\sigma_2\\)     - \\(\\frac{S^2_X}{S^2_Y} \\sim F(n_1 -1 , n_2 - 1)\\)</p>"},{"location":"STATS2/WEEK%2012/Different%20Types%20of%20Tests/#two-samples-z-test","title":"Two Samples Z-test","text":"<p>\\(\\(X_1 , X_2 , X_3 , ...... X_n \\sim \\text{iid Normal}(\\mu_1 , \\sigma^2_1 )\\)\\) \\(\\(Y_1 , Y_2 , Y_3 , ...... Y_n \\sim \\text{iid Normal}(\\mu_2 , \\sigma^2_2 )\\)\\) - Null \\(H_0 : \\mu = \\mu_0, \\text{ Alternative } H_A : \\mu \\neq \\mu_0\\) - \\(T = \\overline{Y} - \\overline{X}\\) , Test : Reject \\(H_0 \\text{ if } |T| &gt; c\\)</p> <p>Computing Significance Level Given \\(H_0\\) , \\(T \\sim \\text{Normal}(0 , \\sigma_{T}^{2})\\) , where \\(\\sigma_T^2 = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma^2_2}{n_2}\\) \\(\\(\\alpha = P(|T| &gt; c | H_0) = P(|\\text{Normal}(0,1)| &gt; \\frac{c}{\\sigma_{T}})\\)\\)</p>"},{"location":"STATS2/WEEK%2012/Different%20Types%20of%20Tests/#two-sample-f-test","title":"Two Sample F-test","text":"<p>\\(\\(X_1 , X_2 , X_3 , ...... X_n \\sim \\text{iid Normal}(\\mu_1 , \\sigma^2_1 )\\)\\) \\(\\(Y_1 , Y_2 , Y_3 , ...... Y_n \\sim \\text{iid Normal}(\\mu_2 , \\sigma^2_2 )\\)\\) - Null \\(H_0 : \\sigma_1 = \\sigma_2\\) , Alternative \\(H_A : \\sigma_1 \\neq \\sigma_2\\) - T = \\(\\frac{S_X^2}{S_Y^2}\\) , Test : Reject \\(H_0\\) if \\(T &gt; 1 + c_R\\) or \\(T &lt; 1 - c_L\\) - Given \\(H_0\\) , \\(T \\sim F(n_1 -1 , n_2 -1)\\) - \\(\\alpha / 2 = P(T &lt; 1 - c_L | H_0) = P(T &gt; 1 + c_R | H_0)\\)</p>"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/","title":"Extra Content","text":""},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#visualizing-random-variables","title":"Visualizing Random Variables","text":"<p>Sometimes we dont want the usual representation of random variables, that is when we use functions. Functions change the horizontal axis of the graph. \\(f(x) = x- 10\\) is a function which shifts the x axis to the left by 10.</p> <p>Sometimes these functions can either be one to one , which means that each input has a unique output/ no two outputs are the same OR the functions can be many to one , which means outputs for different inputs can be the same/ two or more outputs are same.</p> <p>See This for what changes occurs on different types of functions.</p>"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#many-to-one-functions","title":"Many To One Functions","text":"<p>In the case of many to one functions we add the probabilities when the outputs are the same.</p> <p></p> <p>Info</p> <p>If two variables \\(X\\) and \\(Y\\) are independent then their functions \\(f(X)\\) and \\(g(Y)\\) will also be independent.</p>"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#formulas","title":"Formulas","text":"<p>You are probably better off mugging up these because its not gonna come in future weeks. Also you will be provided with a forumla sheet in the exam with all the formula required for STATS2.</p>"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#two-uniformly-distributed-iid-random-variables","title":"Two uniformly distributed iid random variables","text":""},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#sum","title":"Sum","text":"<p>Given that \\(X,Y \\sim Uniform \\{1,2,3,4.....n\\} , W=X+Y\\) \\(\\implies W \\in \\{2,3,4,5....2n\\}\\)</p> \\[P(W=w) =  \\begin{cases} \\frac{w-1}{n^2}, &amp; 2 \\leq w \\leq n+1 \\\\ \\frac{2n - w + 1}{n^2} &amp; n+2 \\leq w \\leq 2n \\end{cases}\\]"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#maximum","title":"Maximum","text":"<p>Given that \\(X,Y \\sim Uniform \\{1,2,3,4.....n\\} , Z=\\max(X,Y)\\)</p> <p>\\(\\implies Z \\in \\{1,2,3,....n\\}\\)</p> \\[P(Z=z) = \\frac{2z-1}{n^2}\\]"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#sum-of-n-independent-bernoulli-trials","title":"Sum of n independent bernoulli trials","text":"<p>Let \\(X_1 , X_2 , X_3 .... X_n\\) be the results of \\(n\\) i.i.d \\(Bernoulli(p)\\) trials. </p> <p>The sum of the n random variables \\(X_1 , X_2 , X_3 .... X_n\\) is \\(Binomial(n,p)\\)</p>"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#sum-of-2-random-variables-taking-integer-values","title":"Sum of 2 random variables taking integer values","text":"<p>Suppose \\(X\\) and \\(Y\\) take integer values and let their joint PMF be \\(f_{XY}\\). Let \\(Z = X+Y\\)</p> <p>Let \\(z\\) be some integer.</p> \\[\\begin{align}     P(Z=z) &amp;= P(X+Y=z) \\\\     &amp;= \\sum^{\\infty}_{x=- \\infty} P(X=x , Y=z-x) \\\\     &amp;= \\sum^{\\infty}_{x=- \\infty}f_{XY}(x , z-x) \\\\     &amp;= \\sum^{\\infty}_{x=- \\infty}f_{XY}(z-y , y) \\\\ \\end{align}\\]"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#convolution","title":"Convolution","text":"<p>If \\(X\\) and \\(Y\\) are independent,</p> \\[ f_{X+Y}(z) = \\sum^{\\infty}_{x = - \\infty}f_{X}f_{Y}(z-x) \\]"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#two-independent-poisson","title":"Two Independent Poisson","text":""},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#sum_1","title":"Sum","text":"<p>\\(Z = X+Y\\)</p> \\[f_Z(Z) = \\frac{e^{-(\\lambda_1 + \\lambda_2)} \\times (\\lambda_1 + \\lambda_2)^Z}{Z!} \\]"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#conditional-distribution-of-xz","title":"Conditional distribution of X|Z","text":"\\[P(X=k|Z=n) = \\frac{n!}{k!(n-k)!} \\times (\\frac{\\lambda_1}{\\lambda_1 + \\lambda_2})^k \\times (\\frac{\\lambda_2}{\\lambda_1 + \\lambda_2})^{n-k}\\] <p>which is also equals to </p> \\[P(X=k|Z=n) = Binomial(n, \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2})\\] <p>given that \\(X|Z \\sim Binomial(n, \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2})\\)</p>"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#max-of-cdf-of-2-independent-random-variables","title":"Max of CDF of 2 independent random variables","text":"<p>Definition (CDF of a random variable)</p> <p>Cumulative distribution function of a random variable \\(X\\) is a function \\(F_X : \\mathbb{R} \\to [0,1]\\) defined as</p> \\[ F_{X}(x) = P(X \\leq x)\\] <p>Suppose \\(X\\) and \\(Y\\) are independent and \\(Z = \\text{max}(X,Y)\\).</p> \\[\\begin{align}     F_Z(z) &amp;= P(\\text{max}(X,Y) \\leq z) \\\\     &amp;= P((X \\leq z) \\text{and} (Y \\leq z)) \\\\     &amp;= P(X \\leq z)P(Y \\leq z) \\\\     &amp;= F_X(z)F_Y(z) \\end{align}\\]"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#min-of-2-independent-geometric-random-variables","title":"Min of 2 independent Geometric Random Variables","text":""},{"location":"STATS2/WEEK%202/Notes/Notes/","title":"Independent Random Variables","text":""},{"location":"STATS2/WEEK%202/Notes/Notes/#takeaways-from-this-week","title":"Takeaways from this week","text":"<ul> <li>Being able to check whether a given join distribution has indepenedent random variables or not.</li> <li>Memoryless Property </li> </ul>"},{"location":"STATS2/WEEK%202/Notes/Notes/#independence-of-random-variables","title":"Independence of Random Variables","text":"<p>Independent random variables are variables that have no influence or relationship with each other.  In other words, the occurrence or value of one random variable does not affect the occurrence  or value of the other random variable.</p> <p>Formally, two random variables \\(X\\) and \\(Y\\) are considered independent if the probability distribution of one variable is not affected by the other variable.</p> <p>Let \\(X\\) and \\(Y\\) be two random variables defined in a probability space with ranges \\(T_X\\) and \\(T_Y\\) respectively. \\(X\\) and \\(Y\\) are considered independent if :</p> \\[f_{XY}(t_1 , t_2) = f_X(t_1) \\times f_{Y|X=t_1}(t2)\\] <p>where,</p> \\[f_{Y|X=t_1}(t2) = f_Y(t_2)\\] \\[\\therefore f_{XY}(t_1 , t_2) = f_X(t_1) \\times f_Y(t_2)\\] <p>Properties of Independent Variables</p> <ul> <li>Joint PMF is the product of the marginal PMFs when the variables are independent.</li> <li>All the subsets of independent random variables are independent.</li> </ul>"},{"location":"STATS2/WEEK%202/Notes/Notes/#checking-independence-of-random-variables","title":"Checking Independence of Random Variables","text":"<p>For every element in the table of 2 or more random variables. Each entry must be the product of their respective marginal PMFs then only they are considered independent. </p> <p>If for any element \\(f_{XY}(t_1 , t_2) \\neq f_X(t_1)f_Y(t_2)\\) then the variables are considered dependent.</p> <p>Tips To Identify Independent Random Variables</p> <ul> <li>i.i.ds (independent and identically distributed) are one of the examples for independent random variables for any \\(f_{XY}(t_1 , t_2) \\neq 0\\)</li> <li>Finding dependent variables is easier when \\(f_{XY}(t_1)(t_2) =0\\). The logic behind it is for some \\(t_1\\) and \\(t_2\\) \\(f_X(t_1) \\times f_Y(t_2) \\neq 0\\) , if it is 0 then it would mean that either or both of the marginals are 0 which is generally not true.</li> </ul>"},{"location":"STATS2/WEEK%202/Notes/Notes/#geometric-iid","title":"Geometric iid","text":""},{"location":"STATS2/WEEK%202/Notes/Notes/#question-1","title":"Question 1","text":"<p>Let \\(X_1 , X_2 , X_3 ... , X_n\\) be an iid with a Geometric(p) distribution. What is the probability that all of these random variables are larger than some positive integer \\(j\\).</p> \\[X \\sim \\{1,2,3,....\\}\\] \\[P(X=k) = (1-p)^{k-1}p\\] <p>The probability that the random variables are greater than \\(j\\) is :</p> \\[(P(X &gt; j))^n = (1-p)^{jn}\\]"},{"location":"STATS2/WEEK%202/Notes/Notes/#question-2","title":"Question 2","text":"<p>Let \\(X \\sim \\{\\stackrel{\\frac{1}{2}}{0},\\stackrel{\\frac{1}{4}}{1} , \\stackrel{\\frac{1}{8}}{2} , \\stackrel{\\frac{1}{16}}{3} , \\stackrel{\\frac{1}{16}}{4}\\}\\) , and let \\(X_1 ..... X_n\\) be the iid samples with distribution \\(X\\).</p>"},{"location":"STATS2/WEEK%202/Notes/Notes/#what-is-the-probability-that-4-is-missing-in-some-samples","title":"What is the probability that 4 is missing in some samples.","text":"<ul> <li>\\(P(X_1 \\neq 4 , X_2 \\neq 4 , X_3 \\neq 4 , X_4 \\neq 4 .... X_n \\neq 4)\\)</li> <li>\\((P(X \\neq 4))^n\\) as all the probabilities are same for an iid.</li> <li>\\((P(X \\neq 4))^n = (1 - P(X = 4))^n\\)</li> <li>\\((P(X \\neq 4))^n = (1 - \\frac{1}{16})^n = (\\frac{15}{16})^n\\)</li> </ul>"},{"location":"STATS2/WEEK%202/Notes/Notes/#what-is-the-probability-that-4-appears-exactly-once","title":"What is the probability that 4 appears exactly once.","text":"<p>\\(\\implies P( \\text{4 Appears exactly once})=\\)</p> \\[\\begin{align} \\implies P(X_1 = 4 , X_2 \\neq 4 , X_3 \\neq 4 .... X_n \\neq 4) + \\\\  P(X_1 \\neq 4 , X_2 = 4 , X_3 \\neq 4 ... X_n \\neq 4) + ... \\\\ P(X_1 \\neq 4 , X_2 \\neq 4 , X_3 \\neq 4 .... X_{n-1} \\neq 4 , X_n =4) \\end{align}\\] <p>\\(\\implies n \\times P(X \\neq 4)^{n-1} \\times P(X=4)\\) \\(\\implies n \\times (\\frac{15}{16})^{n-1} \\times (\\frac{1}{16})\\)</p>"},{"location":"STATS2/WEEK%202/Notes/Notes/#memoryless-property-of-geometric-distribution","title":"Memoryless Property of Geometric Distribution","text":"<p>The memoryless property of geometric random variables states that the past history or  previous outcomes of the trials do not affect the future outcomes.  In other words, the probability of success in the next trial remains the same,  regardless of how many trials have already occurred.</p> <p>(Just mug up the formula for this one if you dont understand this \ud83d\udc80\ud83e\udd72)</p> \\[P(X \\geq s+t | X \\geq t  ) = P(X \\geq s)\\]"},{"location":"STATS2/WEEK%202/Notes/Notes/#proof","title":"Proof","text":"<p>The PMF of a random variable \\(X\\) is </p> \\[\\begin{align} f(x) &amp;= p(1-p)^x &amp; x &amp;= 0,1,2,.. \\end{align}\\] <p>The probability that \\(X\\) is greater than or equal to \\(x\\) is</p> \\[\\begin{align} P(X \\geq x) &amp;= (1-p)^x &amp; x = 0,1,2,... \\end{align}\\] <p>The conditional probability will be</p> \\[\\begin{align} P(X \\geq s +t | X \\geq t) &amp;= \\frac{P(X \\geq s + t , X \\geq t)}{P(X \\geq t)} \\\\ &amp;= \\frac{P(X \\geq s+t)}{P(X \\geq t)} \\\\ &amp;=  \\frac{(1-p)^{s+t}}{(1-p)^t} \\\\ &amp;= (1-p)^s \\\\ &amp;= P(X \\geq s) \\end{align}\\]"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/","title":"Expectation of Random Variables","text":"<p>Suppose \\(X\\) is a random variables defined in the range of \\(T_X\\) and PMF of \\(X\\) is \\(f_X\\) . The expected value of the random variables \\(X\\) will be $$ E[X] = \\sum_{t \\in T_X} t \\times f_X(t) = \\sum_{t \\in T_X} t \\times P(X =t) $$</p> <pre><code>$E[X]$ has the same unit of $X$\n\n$E[X]$ may or may not belong to range of $X$\n\n```ad-info \ntitle: Expectation Properties\n$E[c \\times X] = c \\times E[X]$ , where $c$ is a constant and $X$ is a random variable.\n\n$E[X + Y] = E[X] + E[Y]$ , where $X$ and $Y$ are 2 random variables.\n$E[X - Y] = E[X] + E[Y]$ , this is right dont worry (to future self)\n\n\n**If X and Y are independent**\n\n$E[XY] = E[X]E[Y]$\n```\n</code></pre>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#types-of-random-variables","title":"Types of Random Variables","text":""},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#uniform-random-variable","title":"Uniform Random Variable","text":"<p>Given that , \\(X \\sim Uniform \\set{a , a+1 , a+2 , ... b}\\) $$ E[X] = \\frac{a+b}{2} $$</p>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#variance","title":"Variance","text":"\\[ Var(X) = \\frac{(b-a)^2}{12} = \\sigma^2 \\]"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#geometric-random-variable","title":"Geometric Random Variable","text":"<p>Given that , \\(X \\sim Geometric(p)\\) $$ E[X] = \\sum^{\\infty}_{t=1} t(1-p)^{t-1}p = \\frac{1}{p} $$</p>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#variance_1","title":"Variance","text":"\\[ Var(X) = \\frac{1-p}{p^2} = \\sigma^2 \\]"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#poisson-random-variable","title":"Poisson Random Variable","text":"<p>Given that , \\(X \\sim Poisson(\\lambda)\\) $$ E[X] = \\sum^{\\infty}_{t = 0} t \\times e^{- \\lambda} \\times \\frac{\\lambda ^ {t}}{t!} = \\lambda $$</p>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#variance_2","title":"Variance","text":"\\[ Var(X) = \\lambda = \\sigma^2 \\]"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#binomial-random-variable","title":"Binomial Random Variable","text":"<p>Given that , \\(X \\sim Binomial(n,p)\\) $$ E[X] = \\sum^{n}_{t=0} t \\times {n \\choose x} p^t (1-p)^{n-t} = np $$</p>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#variance_3","title":"Variance","text":"\\[ Var(X) = np(1-p) = \\sigma^2 \\]"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#bernoulli-random-variable","title":"Bernoulli Random Variable","text":"<p>Given that , \\(X \\sim Bernoulli(p)\\) $$ E[X] = 0(1-p) + p = p $$</p>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#variance_4","title":"Variance","text":"\\[ Var(X) = p(1-p) = \\sigma^2 \\]"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#variance-and-standard-deviation-properties","title":"Variance and Standard Deviation Properties","text":"<p>Let \\(X\\) be a random variable. Let \\(a\\) be a constant real number. $$ Var(X) = E[X^2] - E[X]^2 $$</p> <pre><code>title: Variance Properties \n- $Var(aX) = a^2Var(X)$\n- $SD(aX) = |a|SD(X)$\n- $Var(X + a) = Var(X)$\n- $SD(X + a) = SD(X)$\n\n```ad-info \n**If X and Y are independent**\n\n$Var(X+Y) = Var(X) + Var(Y)$\n```\n</code></pre>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#more-important-formulas","title":"More Important Formulas","text":""},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#standardized-random-variables","title":"Standardized Random Variables","text":"<p>A random variable \\(X\\) is said to be standardized if \\(E[X] = 0 , Var(X) = 1\\)</p> <p>Also, If \\(X\\) is a random variable.  Then, $$ \\frac{X - E[X]}{SD(X)} $$ is a standardized random variable.</p>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#covariance-formula","title":"Covariance Formula","text":"\\[ Cov(X,Y) = E[XY] - E[X] \\times E[Y] \\]"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#correlation-coefficient","title":"Correlation Coefficient","text":"\\[ Cor(X,Y) = \\frac{Cov(X,Y)}{SD(X) \\times SD(Y)} \\]"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#bounds-in-probabilities","title":"Bounds In Probabilities","text":""},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#markovs-inequality","title":"Markov's Inequality","text":"<p>Let \\(X\\) be a random variable. $$ P(X \\geq a) \\leq \\frac{E(X)}{a} $$ <pre><code>A shopkeeper sells mobile phones. The expected demand for mobile phones is\u00a04\u00a0per week. $X$\u00a0is denoting the number of phones sold in a week.\n\n$$\nP(X \\geq 10) \\leq 0.4\n$$\n\nAlso, \n\n$P(X &lt; 10) \\geq 0.6$\n</code></pre></p>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#chebyshevs-inequality","title":"Chebyshev's Inequality","text":"<p>Let \\(X\\) be a random variable with mean \\(\\mu\\) and finite variance \\(\\sigma^2\\) , then for any real number \\(k&gt;0\\) $$ P(|X - \\mu| &lt; k \\sigma) \\geq 1 - \\frac{1}{k^2} $$ Also, $$ P(|X - \\mu| \\geq k \\sigma) \\leq \\frac{1}{k^2} $$</p>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/","title":"Cumulative Distribution Function","text":"<p>The CDF of a random variable \\(X\\) , denoted \\(F_X(X)\\) , is a function from \\(\\mathbb{R} \\to [0,1]\\) is defined as  $$ F_X(X) = P(X \\leq x) $$</p> <pre><code>title: Properties \n\n$F(X)$ is always a non-decreasing funciton taking values between 0 and 1.\n\n$P(a &lt; X \\leq b) = F_X(b) - F_X(a)$\n\nAs $X \\to - \\infty$ , $F_X$ goes to 0.\n\nAs $X \\to  \\infty$ , $F_X$ goes to 1.\n</code></pre>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#cdf-of-standardised-variables","title":"CDF Of Standardised Variables","text":"<p>Let a discrete random variable \\(X\\) have a CDF \\(F_X\\). Assume that \\(Y = \\frac{X - \\mu}{\\sigma}\\) , where \\(\\mu\\) and \\(\\sigma\\) are the mean and standard deviation of \\(X\\)  respectively. If \\(F_Y\\) is the CDF of \\(Y\\) , then  $$ F_Y(y) = F_X(\\mu + Y \\sigma) $$ <pre><code>title: CDF Formula and More Properties \nGiven a child CDF F(x), there exists a random variable $X$ taking values in $\\mathbb{R}$ such that \n\n$$P(X \\leq x) = F(x)$$\n\nProbability of $X$ taking a specific value is always 0.\n\n$P(X = 3) = 0$ for a random variable $X$.\n</code></pre></p>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#continuous-random-variable","title":"Continuous Random Variable","text":"<p>A random variable \\(X\\) with CDF \\(F_X(x)\\) is said to be continuous random variable if \\(F_X(x)\\) is continuous at every \\(x\\). - CDF has no jumps or steps. - So ,\\(P(X =x) = 0\\) for all \\(x\\) - Probability of \\(X\\) falling in an interval will be nonzero \\(\\(P(a &lt; X \\leq b) = F(b) - F(a)\\)\\) $$ \\therefore P(a \\leq X \\leq b) = P(a &lt; X \\leq b) = P(a \\leq X &lt; b) = P(a &lt; X &lt; b) $$ - Graphs of continuous random variables never breaks at any point and does not from one value to another.</p>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#probability-density-functions","title":"Probability Density Functions","text":"<p>They are a much better way to represent random variables on a graph.  $$ \\int^{b}_{a}f(x) dx = F(b) - F(a) = P(a &lt; X &lt; b) $$ Also integral of all the values a variable \\(X\\) takes is always 1. \\(\\implies \\int^{\\infty}_{ - \\infty} f(x) dx = 1\\)</p>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#common-distribution-functions","title":"Common Distribution Functions","text":""},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#uniform","title":"Uniform","text":"<p>\\(X \\sim Uniform[a,b]\\)</p>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#pdf","title":"PDF","text":"\\[ f_X(x) = \\begin{cases} \\frac{1}{b-a} &amp; a &lt; x &lt; b \\\\ 0 &amp; \\text{otherwise} \\end{cases}  \\]"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#cdf","title":"CDF","text":"\\[ F_X(x) = \\begin{cases} 0 &amp; x \\leq a  \\\\  \\frac{x-a}{b-a} &amp; a &lt; x &lt; b \\\\  1 &amp; x \\geq b \\end{cases} \\]"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#exponential","title":"Exponential","text":"<p>\\(X \\sim Exp(\\lambda)\\)</p>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#pdf_1","title":"PDF","text":"\\[ f_X(x) = \\begin{cases} \\lambda \\exp(-\\lambda x) &amp; x&gt;0 \\\\ 0 &amp; otherwise \\end{cases} \\]"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#cdf_1","title":"CDF","text":"\\[ F_X(x) = \\begin{cases} 0 &amp; x \\leq 0 \\\\  1 - \\exp(-\\lambda x) &amp; x &gt;0 \\end{cases} \\]"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#normal","title":"Normal","text":"<p>\\(X \\sim Normal(\\mu , \\sigma^2)\\)</p>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#pdf_2","title":"PDF","text":"\\[ F_X(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp(- \\frac{(x - \\mu)^2}{2 \\sigma^2}) \\]"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#cdf_2","title":"CDF","text":"\\[ F_X(x) = \\int^{x}_{- \\infty} f_X(u)du \\]"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/","title":"CDF of g(X)","text":"<ul> <li>Suppose \\(X\\) is a continuous random variable with CDF \\(F_X\\) and PDF \\(f_X\\)</li> <li>Suppose \\(g : \\mathbb{R} \\to \\mathbb{R}\\) is a function.</li> <li>Then , \\(Y = g(X)\\) is a random variable with CDF \\(F_Y\\) determined as follows. $$ F_Y(y) = P(Y \\leq y) = P(g(X) \\leq y) = P(X \\in \\set{x:g(x) \\leq y}) $$</li> </ul>"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/#monotonic-differentiable-functions","title":"Monotonic , differentiable functions","text":"<p>Suppose \\(X\\) is continuous random variable with PDF \\(f_X\\). Let g(x) be monotonic for \\(x \\in supp(x)\\) with derivative \\(g'(x) = \\frac{dg(x)}{dx}\\). Then, the PDF of \\(Y = g(X)\\) is  $$ f_Y(y) = \\frac{1}{|g'(g^{-1}(y))|}f_X(g^{-1}(y)) $$</p>"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/#translation","title":"Translation","text":"<p>\\(Y = X + a\\) $$ f_Y(y) = f_X(y-a) $$</p>"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/#scaling","title":"Scaling","text":"<p>\\(Y = aX\\) $$ f_Y(y) = \\frac{1}{|a|}f_X(\\frac{y}{a}) $$</p>"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/#affine","title":"Affine","text":"<p>\\(Y = aX + b\\) $$ f_Y(y) = \\frac{1}{|a|}f_X(\\frac{y-b}{a}) $$</p>"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/#affine-transformation-of-normal-distributions","title":"Affine transformation of normal distributions","text":"<ul> <li> <p>Given \\(X \\sim Normal(0,1)\\) $$ f_X(x) = \\frac{1}{\\sqrt{2 \\pi}}\\exp(\\frac{-x^2}{2}) $$ If \\(Y = \\sigma X + \\mu\\) $$ f_Y(y) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}\\exp(\\frac{-(y - \\mu)^2}{2 \\sigma^2}) $$ \\(Y \\sim Normal(\\mu , \\sigma^2)\\)</p> </li> <li> <p>\\(X \\sim Normal(\\mu , \\sigma^2)\\) \\(Y = \\frac{X - \\mu}{\\sigma} \\sim Normal(0,1)\\)</p> </li> </ul>"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/#expectation","title":"Expectation","text":"<p>Let \\(X\\) be a continuous random variable with density \\(f(x)\\). Let \\(g : \\mathbb{R} \\to \\mathbb{R}\\) be a function. The expected value of \\(g(X)\\), denoted \\(E[g(X)]\\) is given by  $$ E[g(X)] = \\int^{\\infty}_{ - \\infty} g(X)f_X(x)dx $$ whenever the above integral exists.</p>"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/#formulas","title":"Formulas","text":"<p>![[Pasted image 20230302183158.png]]</p>"},{"location":"STATS2/WEEK%206/Joint%20Continuous%20Random%20Variables/","title":"Joint Continuous Random Variables","text":"<p>A function \\(f(x,y)\\) is said to be a joint density function if  - \\(f(x,y) \\geq 0\\) , i.e. \\(f\\) is non-negative - \\(\\int \\int^{\\infty}_{ - \\infty} f(x,y)dxdy = 1\\) - Technical : \\(f(x,y)\\) is piecewise continuous in each random variable.</p> <p>support(X,Y) = \\(\\set{(x,y):f_{XY}(x,y) &gt; 0}\\) <pre><code>Let $X$ and $Y$ have joint density \n$$f_{XY} = \\begin{cases}\n1 &amp; 0 &lt; x &lt; 1 , 0 &lt; y &lt; 1\n\\\\\n0 &amp; otherwise\n\\end{cases}$$\n\nThis specific example is also known as uniform unit square.\n</code></pre> To get the probability find the area of the region using integration.</p>"},{"location":"STATS2/WEEK%206/Joint%20Continuous%20Random%20Variables/#2d-uniform-distribution","title":"2D Uniform Distribution","text":"<p>For some (reasonable) region D in \\(\\mathbb{R}^2\\) with total area \\(|D|\\). We say that \\((X,Y) \\sim D\\) if they have the joint density $$ f_{XY} = \\begin{cases} \\frac{1}{|D|} &amp; (x,y) \\in D \\ 0 &amp; otherwise \\end{cases} $$ For any sub region \\(A\\) of \\(D\\) , \\(P((X,Y) \\in A) = \\frac{|A|}{|D|} = \\frac{\\text{Area of A}}{\\text{Area of D}}\\)</p>"},{"location":"STATS2/WEEK%206/Joint%20Continuous%20Random%20Variables/#marginal-density","title":"Marginal Density","text":"<p>Suppose \\((X,Y)\\) have joint density \\(f_{XY}(x,y).\\) Then, - \\(X\\) has the marginal density \\(f_X(x)\\) = \\(\\int^{\\infty}_{y = - \\infty} f_{XY}(x,y)dy\\) - \\(Y\\) has the marginal density \\(f_Y(y)\\) = \\(\\int^{\\infty}_{x = - \\infty} f_{XY}(x,y)dy\\)</p> <p>\\(\\[D = \\{(x, y): [0, 2]\\times [0, -2] \\cup [-1, 0]\\times [0, 1]\\}\\]\\) </p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/","title":"Empirical Distribution and Descriptive Statistics","text":"<p>Let \\(X_1 , X_2 , X_3 .... , X_n \\sim X\\) be iid samples. Let #\\((X_i = t)\\) denote the number of times t occurs in the samples. The emperical distribution is the discrete distribution with PMF  $$ p(t) = \\frac{#(X_i = t)}{n} $$</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#sample-mean","title":"Sample Mean","text":"<p>Let \\(X_1 , X_2 , ...... X_n\\) be iid samples. The sample mean , denoted \\(\\overline{X}\\) , is defined to be the random variable.</p> \\[ \\overline{X} = \\frac{X_1 + X_2 + X_3 + ...... + X_n}{n} \\]"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#expectation","title":"Expectation","text":"<p>\\(E[\\overline{X}] = \\mu\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#variance","title":"Variance","text":"<p>\\(\\text{Var} \\overline{X} = \\frac{\\sigma ^2}{n}\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#sample-variance","title":"Sample Variance","text":"<p>Let \\(X_1 , X_2 , ...... X_n\\) be iid samples. The sample variance, denoted \\(S^2\\) , is defined to be the random variable.</p> <p>\\(\\(S^2 = \\frac{(X_1 - \\overline{X})^2 + (X_2 - \\overline{X})^2 + .... + (X_n - \\overline{X})^2}{n-1}\\)\\) where \\(\\overline{X}\\) is the sample mean.</p> <pre><code>As n increases , sample variance takes values close to distribution variance.\n</code></pre>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#expectation_1","title":"Expectation","text":"<p>Let \\(X_1 , X_2 , ...... X_n\\) be iid samples whose distribution has a finite variance \\(\\sigma^2\\).  The sample variance \\(S^2\\) has expected value given by  \\(\\(E[S^2]=\\sigma^2\\)\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#sample-proportion","title":"Sample Proportion","text":"<p>The sample proportion of \\(A\\) , denoted \\(S(A)\\) , is defined as $$ S(A) = \\frac{# (X_i \\text{for which A is true})}{n} $$</p> <pre><code>As $n$ increases , values of S(A) will be close to P(A).\n</code></pre>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#expectation_2","title":"Expectation","text":"<p>Let \\(X_1 , X_2 , ...... X_n\\) be iid samples whose distribution of \\(X\\). Let \\(A\\) be an event defined using \\(X\\) and let \\(P(A)\\) be the probability of \\(A\\). The sample proportion of \\(A\\) , denoted \\(S(A)\\) ,  has expected value given by $$ E[S(A)] = P(A) $$</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#variance_1","title":"Variance","text":"<p>Variance is given by  $$ Var(S(A)) = \\frac{P(A)(1 - P(A))}{n} $$</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#sum-of-independent-random-variables","title":"Sum of Independent Random Variables","text":"<p>Let \\(X_1 , X_2 , ...... X_n\\) be random variables. Let \\(S = X_1 + X_2 +..... + X_n\\) be their sum. Then, $$ E[S] = E[X_1] + E[X_2]+ ..... E[X_n] $$ If \\(X_1 , X_2 ... X_n\\) are pairwise uncorrelated, then \\(\\(Var(S) = Var(X_1) + Var(X_2) + .... Var(X_n)\\)\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#pairwise-uncorrelated","title":"Pairwise Uncorrelated","text":"\\[ E[X_i X_j] = E[X_i] E[X_j] \\]"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#weak-law-of-large-numbers","title":"Weak Law of Large Numbers","text":"<p>\\(\\(X_1 , X_2 ......X_n \\sim \\text{iid}X\\)\\) Let \\(\\mu = E[X]\\) , \\(\\sigma^2 = Var(X)\\) Sample Mean: \\(\\overline{X} =\\frac{X_1 + X_2 + ... X_n}{n}\\)     Expected value : \\(\\mu\\) , Variance : \\(\\frac{\\sigma^2}{n}\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#weak-law","title":"Weak Law","text":"\\[ P(|\\overline{X} - \\mu|&gt; \\delta) \\leq \\frac{\\sigma^2}{n \\delta^2} \\] <pre><code>- With Probability more than $1 - \\frac{\\sigma^2}{n \\delta^2}$ lies in $[\\mu - \\delta , \\mu + \\delta]$\n\n\n- $\\delta &gt; \\frac{\\sigma}{\\sqrt{n}}$\n</code></pre>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#bernoulli","title":"Bernoulli","text":"<p>For Bernoulli(p) samples  $$ 1 - \\frac{p(1-p)}{n\\delta^2}  $$ sample mean lies in  \\([p - \\delta , p + \\delta]\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#uniform","title":"Uniform","text":"<p>For Uniform{-M, .... M} samples $$ 1 - \\frac{M(M+1)}{3n\\delta^2} $$ sample mean lies in \\([-\\delta , \\delta]\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#normal","title":"Normal","text":"<p>For \\(Normal(0 , \\sigma^2)\\)  $$ 1 - \\frac{\\sigma^2}{n\\delta^2} $$ sample mean lies in \\([-\\delta , \\delta]\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#continuous-uniform","title":"Continuous Uniform","text":"<p>For \\(Uniform[-A , A]\\) samples  $$ 1 - \\frac{A^2}{3n\\delta^2} $$ sample mean lies in \\([-\\delta , \\delta]\\)</p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/","title":"Moment Generating Functions","text":"<p>Let \\(X\\) be a zero-mean random variable. The MGF of \\(X\\) , denoted \\(M_X(\\lambda)\\), is a function from \\(\\mathbb{R} \\to \\mathbb{R}\\) defined as  $$ M_X(\\lambda) = E[e^{\\lambda X}] $$ <pre><code>- **When $X$ is Discrete with PMF $f_X$**\n\n\nX takes the values $\\set{x_1 , x_2 , x_3 ....}$\n$$M_X(\\lambda) = f_X(x_1)e^{\\lambda x_1} + f_X(x_2)e^{\\lambda x_2} + ....$$\n$\\newline$\n\n- **When $X$ is continuous with PDF $f_X$ and support $T_X$**\n$$M_X(\\lambda) = \\int^{}_{x \\in T_X} f_X(x) e^{\\lambda x} dx$$\n</code></pre></p> <pre><code>$\\mathbf{X \\in \\set{\\overset{-1}{1/2} , \\overset{0}{1/4} , \\overset{2}{1/4}}}$\n\n$$M_X(\\lambda) = 0.5e^{-\\lambda} + 0.25 + 0.25 e^{2 \\lambda}$$\n\n$\\newline$\n\n\n$\\mathbf{M_X(\\lambda) = (1/3)e^{3 \\lambda / 2} + (1/6)e^{-3\\lambda} + (1/8)e^{-\\lambda} + (1/8)e^{\\lambda} + 1/4}$\n\n$$X \\sim \\set{\\overset{1/6}{-3} , \\overset{1/8}{-1} , \\overset{1/4}{0} , \\overset{1/8}{1} , \\overset{1/3}{3/2}}$$\n\n$\\newline$\n\n```ad-note \n$\\mathbf{X \\sim Normal(0 , \\sigma^2)}$\n$$M_X(\\lambda) = e^{\\lambda^2 \\sigma^2 / 2}$$\n```\n</code></pre>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#expectation-of-mgf","title":"Expectation Of MGF","text":"<p>\\(E[e^{\\lambda X}] = E[1 + \\lambda X + \\frac{\\lambda^2}{2!}X^2 + \\frac{\\lambda ^3}{3!}X^3 + ....]\\)</p> <p>\\(\\implies 1 + \\lambda E[X] + \\frac{\\lambda^2}{2!}E[X^2] + \\frac{\\lambda^3}{3!} E[X^3]\\)</p> <ul> <li>If \\(\\mathbf{X \\sim \\text{Normal}(0,\\sigma^2) , M_X(\\lambda) = e^{\\lambda^2 \\sigma^2 / 2}}\\) \\(1 + E[X] + \\frac{\\lambda^2}{2!}E[X^2] + \\frac{\\lambda ^3}{3!}E[X^3]\\) \\(\\implies 1 + \\frac{\\lambda^2}{2!}\\sigma^2 + \\frac{\\lambda^4}{4!}3\\sigma^4\\)</li> </ul> <p>\\(\\implies E[X]=0 , E[X^2] = \\sigma^2 , E[X^3] = 0 , E[X^4] = 3\\sigma^4 ...\\)</p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#mgf-of-sample-mean","title":"MGF of Sample Mean","text":"<p>Let \\(X_1 , X_2 .... X_n \\sim iid X , M_X(\\lambda) = \\frac{e^{\\lambda/2} + e^{-\\lambda/2}}{2}\\) - Sample Mean : \\(\\overline{X} = (X_1 + X_2 + ... X_n) / n\\) - \\(M_{X/n}(\\lambda) = \\frac{e^{\\lambda / 2n} + e^{-\\lambda / 2n}}{2}\\)</p> \\[ M_{\\overline{X}}(\\lambda) = {(\\frac{e^{\\frac{\\lambda}{2n}} + e^{\\frac{\\lambda}{2n}}}{2})}^n \\]"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#mgf-convergence-at-mathbf1-sqrtn-scaling","title":"MGF convergence at  \\(\\mathbf{1 / \\sqrt{n}}\\) scaling","text":"<p>Let \\(X_1 , X_2 .... X_n \\sim iid X , M_X(\\lambda) = \\frac{e^{\\lambda/2} + e^{-\\lambda/2}}{2}\\) \\(E[X]=0 , Var(X) = 1/4\\) Consider \\(Y = (X_1 + X_2 ... + X_n) /  \\sqrt{n}\\) \\(M_{X/\\sqrt{n}}(\\lambda) = \\frac{e^{\\lambda / 2\\sqrt{n}} + e^{-\\lambda / 2\\sqrt{n}}}{2}\\)</p> \\[ M_Y(\\lambda) = M_{\\overline{X}}(\\lambda) = {(\\frac{e^{\\frac{\\lambda}{2\\sqrt{n}}} + e^{\\frac{\\lambda}{2\\sqrt{n}}}}{2})}^n \\]"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#using-clt-to-approximate-probability","title":"Using CLT to approximate probability","text":"<p>\\(\\(X_1 , X_2 , .... X_n \\sim \\overset{iid}{X}\\)\\) Let \\(\\mu = E[X] , \\sigma^2 = Var(X)\\) \\(Y = X_1 + X_2 + .... X_n\\)</p> <p>What is \\(P(Y - n\\mu &gt; \\delta n \\mu)\\) ? </p> <p>$$ Z = \\frac{Y - n\\mu}{\\sqrt{n} \\sigma} \\approx \\text{Normal}(0,1) $$ \\(P(Y-n \\mu &gt; \\delta n \\mu) = P(\\frac{Y - n \\mu}{\\sqrt{n} \\sigma} &gt; \\frac{\\delta \\sqrt{n} \\mu}{\\sigma}) \\approx 1 - F(\\frac{\\delta \\sqrt{n} \\mu}{\\sigma})\\)</p> <p>\\(\\begin{align*} F_{Z}(0.2617) = 0.603, F_{Z}(1.6) = 0.9452, F_{Z}(1.5) = 0.933 \\end{align*}\\)</p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#types-of-distributions","title":"Types of Distributions","text":""},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#combination-of-independent-normals","title":"Combination of Independent Normals","text":"<p>Let \\(X_1 , X_2 ... X_n \\sim \\text{independent Normal}\\) Let \\(X_i \\sim \\text{Normal}(\\mu_i ,\\sigma_{i}^{2})\\) Suppose \\(Y = a_1X_1 + a_2X_2 + a_nX_n\\) be a linear combination of independent normals. </p> <p>Then, \\(\\(Y \\sim \\text{Normal}(\\mu , \\sigma^2)\\)\\) where \\(\\mu = E[Y] = a_1\\mu_1 + a_2\\mu_2 + ... + a_n\\mu_n\\) \\(\\sigma^2 = a_{1}^{2}\\sigma_{1}^{2} + a_{2}^{2}\\sigma_{2}^{2} + .... a_{n}^{2}\\sigma_{n}^{2}\\) Therefore Linear combinations of independent normals is normal.</p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#gamma-distribution","title":"Gamma Distribution","text":"<p>\\(X \\sim \\text{Gamma}(\\alpha , \\beta)\\) if PDF \\(f_X(s) \\propto x^{\\alpha -1} e^{-\\beta x} , x&gt;0\\)</p> <pre><code>title: Points to be noted\n- $\\alpha &gt; 0$ and $\\alpha$ is called the shape parameter\n- $\\beta &gt; 0$ and $\\alpha$ is called the rate parameter\n- $\\theta = 1/ \\beta$ and $\\theta$ is called the scale parameter.\n- Sum of n iid $\\text{Exp}(\\beta)$ is $\\text{Gamma}(n, \\beta)$\n- Square of $\\text{Normal}(0 , \\sigma^2)$ is $\\text{Gamma}(\\frac{1}{2} , \\frac{1}{2 \\sigma^2})$\n</code></pre> <p>Mean: \\(\\mathbf{\\frac{\\alpha}{\\beta}}\\) , Variance: \\(\\mathbf{\\frac{\\mathbf{\\alpha}}{\\beta^2}}\\) </p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#cauchy-distribution","title":"Cauchy Distribution","text":"<p>\\(X \\sim \\text{Cauchy}(0, \\alpha^2)\\) if PDF \\(f_X(x) = \\frac{1}{\\pi} \\frac{\\alpha}{\\alpha^2 + (x - \\theta)^2}\\) <pre><code>- $\\theta$ is the location parameter\n- $\\alpha &gt; 0$ and $\\alpha$ is called the scale parameter \n- Suppose $X , Y \\sim \\text{iid} Normal(0,\\sigma^2)$. Then,\n$$\\frac{X}{Y} \\sim \\text{Cauchy}(0,1)$$\n</code></pre> Mean : undefined , Variance : undefined </p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#beta-distribution","title":"Beta Distribution","text":"<p>\\(X \\sim \\text{Beta}(\\alpha , \\beta)\\) if PDF \\(f_X(x) \\propto x^{\\alpha -1}(1- x)^{\\beta -1} , 0 &lt; x &lt; 1\\)</p> <pre><code>- $\\alpha &gt; 0 , \\beta &gt; 0$ and both of them are the shape parameters\n- $\\text{Beta}(\\alpha ,1)$ has PDF $\\propto x^{\\alpha -1}$ which is called the power function distribution \n- Suppose $X \\sim \\text{Gamma}(\\alpha , 1 / \\theta), Y \\sim \\text{Gamma}(\\beta , 1/\\theta)$ , then \n\n$$\n\\frac{X}{X + Y} \\sim \\text{Beta}(\\alpha , \\beta)\n$$\n</code></pre>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#plotted-distributions","title":"Plotted Distributions","text":"<p>![[Pasted image 20230319210918.png]]</p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#sample-mean-distribution","title":"Sample Mean Distribution","text":"<p>\\(X_1 , X_2 , .... X_n \\sim \\text{iid Normal}(\\mu , \\sigma^2)\\) \\(\\overline{X} = \\frac{1}{n}X_1 + ... \\frac{1}{n}X_n\\)</p> <p>Sample mean is a linear combination of iid normal random variables  \\(\\(\\overline{X} \\sim \\text{Normal}(\\mu , \\sigma^2/n)\\)\\) Mean : \\(E[\\overline{X}] = \\mu\\) , Variance : \\(\\text{Var}(\\overline{X}) = \\sigma^2 /n\\)</p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#sum-of-squares-of-normal-samples","title":"Sum of squares of Normal Samples","text":"<p>\\(X_1 , X_2 , .... X_n \\sim \\text{iid Normal}(0 , \\sigma^2)\\) - \\(X_{i}^{2}\\) : \\(\\text{Gamma}(1/2 , 1/2\\sigma^2)\\) , independent - Sum of n independent \\(\\text{Gamma}(\\alpha , \\beta)\\) is \\(\\text{Gamma}(n\\alpha , \\beta)\\) $$ X_{1}^{2} + X_{2}^{2} + X_{3}^{2} + ... + X_{n}^{2} \\sim \\text{Gamma}(\\frac{n}{2} , \\frac{1}{2\\sigma^2}) $$</p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#sample-mean-and-variance-of-normal-samples","title":"Sample Mean and variance of normal samples","text":"<p>Suppose \\(X_1 , X_2 , .... X_n \\sim \\text{iid Normal}(\\mu , \\sigma^2)\\). Then, - \\(\\overline{X} \\sim Normal(\\mu , \\sigma^2 /n)\\) - \\(\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi_{n-1}^{2}\\) , Chi-square with \\(n-1\\) degrees of freedom. - \\(\\overline{X}\\text{ and }S^2\\) are independent.</p>"},{"location":"STATS2/WEEK%209/Parameter%20Estimation/","title":"Parameter Estimation","text":"<p>It is a function defined on the samples taken. </p>"},{"location":"STATS2/WEEK%209/Parameter%20Estimation/#estimation-error","title":"Estimation Error","text":"<p>Let \\(\\theta\\) be the parameter and \\(\\hat{\\theta}\\) be the estimator Error: \\(\\hat{\\theta}(X_1 , X_2 .... X_n) - \\theta\\) is a random variable.</p>"},{"location":"STATS2/WEEK%209/Parameter%20Estimation/#bias","title":"Bias","text":"<p>Let \\(X_1 , X_2 , .... X_n \\sim \\text{iid} X\\) , parameter \\(\\theta\\) The bias of the estimator \\(\\hat{\\theta}\\) for a parameter \\(\\theta\\) , denoted as  \\(\\(\\text{Bias}(\\hat{\\theta} , \\theta) = \\text{E}(\\hat{\\theta}) - \\theta = \\text{Error}\\)\\) <pre><code>title:Unbiased Estimator \nWhen Bias/Error is 0 , then the estimator $\\hat{\\theta}$ is said to be unbiased estimator.\n</code></pre></p>"},{"location":"STATS2/WEEK%209/Parameter%20Estimation/#risk-squared-error","title":"Risk (Squared Error)","text":"<p>Let \\(X_1 , X_2 , .... X_n \\sim \\text{iid} X\\) , parameter \\(\\theta\\) The squared-error or risk of the estimator \\(\\hat{\\theta}\\) for a parameter \\(\\theta\\) , denoted  \\(\\(\\text{Risk}(\\hat{\\theta}, \\theta) = {E[(\\hat{\\theta} - \\theta)^2]}\\)\\) - Since \\(\\text{Error} = \\hat{\\theta} -\\theta\\) , risk is the expected value of squared error and is also called the mean squared error (MSE). - Squared-error risk is the second moment of Error </p>"},{"location":"STATS2/WEEK%209/Parameter%20Estimation/#variance","title":"Variance","text":"<p>Let \\(X_1 , X_2 , .... X_n \\sim \\text{iid} X\\) , parameter \\(\\theta\\) Variance of Estimator: \\(\\(Var(\\hat{\\theta}) = E[(\\hat{\\theta} - E[\\hat{\\theta}])^2]\\)\\) also variance of error is equal to variance of estimator i.e. \\(Var(\\hat{\\theta}) = Var(Error)\\)</p>"},{"location":"STATS2/WEEK%209/Parameter%20Estimation/#bias-variance-tradeoff","title":"Bias Variance Tradeoff","text":"<p>Let \\(X_1 , X_2 , .... X_n \\sim \\text{iid} X\\) , parameter \\(\\theta\\) \\(\\(\\text{Risk}(\\hat{\\theta} , \\theta) = \\text{Bias}(\\hat{\\theta} , \\theta)^2 + Var(\\hat{\\theta})\\)\\) \\(\\(\\text{Risk} = \\text{E}[\\text{Error}]^2 = \\text{Mean}[\\text{Error}]^2 + \\text{Var}[\\text{Error}]\\)\\)</p>"},{"location":"STATS2/WEEK%209/Parameter%20Estimation/#sample-moments","title":"Sample Moments","text":"\\[X_1 , X_2 , ... X_n \\sim \\text{iid} \\ X$$ **Sample Moments:** $$M_k(X_1 , X_2 ...., X_n) = \\frac{1}{n}\\sum^{n}_{i=1}X_{i}^{k}\\]"}]}