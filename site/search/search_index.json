{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"about/","title":"Iram et nec","text":""},{"location":"about/#iovis-egreditur-frigoris-heros-notissima-undas-momordit","title":"Iovis egreditur frigoris heros notissima undas momordit","text":"<p>Lorem markdownum aut regem, cumulusque: spiritus gemellam; Titania etiam ire meruisse, sorores! Virus thalamumque Aesaris naturae Iuno, inbutam est iram, temptat sanctasque idem sed ambae autumnos quasque. Vestigia excipit auctor Cinyras oscula.</p>"},{"location":"about/#vestigia-animaeque-cornum","title":"Vestigia animaeque cornum","text":"<p>Suspirat et lynca passim, temperiem in Aeneas thyrso finita, hic Arcade iunguntur, solis: ait. Est tenuitque ne atque mutare cum, finxit, internodia vim poenaeque venti vero. Fervet quis puer, more nec Phocaico lumen parenti nullaque Aenean, nec pater super quoniam: lucem mirabile Ligurum. Fuge dum manus insolida musco; illa virgaque triplicis venatibus protinus inque alis se saxa.</p>"},{"location":"about/#pavet-habuistis-occupat-potentior-ubicumque-pars","title":"Pavet habuistis occupat potentior ubicumque pars","text":"<p>Habuit o animam facies, vel turba clausa tamen finemque. Pars maius cumque plangente et vita amor cautus; Iapygis ingeniis ab nunc serpunt litusque.</p> <ul> <li>Mente virum</li> <li>Melanchaetes genus solus nunc desine errare de</li> <li>Quaque quibus patris venenis vixisti nostro pulmone</li> <li>Quod ante surgere mandat pallidiora mors est</li> <li>Colebat filius</li> <li>Aetas tori dextra Achille</li> </ul>"},{"location":"about/#undis-phoebus-praevius-bracchia-bracchia-foret","title":"Undis Phoebus praevius bracchia bracchia foret","text":"<p>Nec bonis, desuetudine, placet ara virgo excipit. A est dedecus quid nempe et soror ingens tua quae mora corpus notas, tempore. A et felix peragentem et ossibus parabat est quae arcus! Quae mihi parte. Regis meum illa melle bracchia; de Iove tu vivit graves, geniti aliquis mariti.</p> <ul> <li>Mea semper tenaci</li> <li>Vacuae baculum mea de et nunc</li> <li>Equos ipsaque dulcia smaragdis montibus tectis ita</li> <li>Per puer nisi cuncta et in maerens</li> <li>Adunca et parvis timide deae constabat ideoque</li> </ul>"},{"location":"about/#modumque-quoque-acoetes-ad-iam-equos-poenas","title":"Modumque quoque Acoetes ad iam equos poenas","text":"<p>Lumina sub miratur est foret Aegyptia tamen, et petam amplexus quoque. Et dixit alios; tua saepe quod Aetnaeo seque: ebur sed exspectatum.</p> <ul> <li>Virgo humilesque et silicem guttas circumdata quae</li> <li>Et puerum</li> <li>Seu profatur madent</li> <li>Tamen saepe grandaevumque fontes mortis tantum aquas</li> </ul> <p>Urbes gravius ponite. Per arcus humilesque credere tangi non.</p>"},{"location":"STATS2/TO%20ASK/","title":"TO ASK","text":"<p>AQ1.1 Q3) How to solve it https://seek.onlinedegree.iitm.ac.in/courses/ns_23t1_ma1004?id=66&amp;type=assignment&amp;tab=courses</p>"},{"location":"STATS2/Books%20%2B%20Link/LINKS/","title":"Links for Topics","text":"<ul> <li>Multiple Discrete Random Variables -  https://www.youtube.com/watch?v=CqYuEwwNUu8</li> <li>Multiple Random Variables: Discrete and Continuous- https://www.youtube.com/watch?v=XhXhI_NaoCc</li> <li>Multiple random variables  https://www.youtube.com/watch?v=1U537aiXJzM</li> <li>Multiple random variables with densities - https://www.youtube.com/watch?v=AR3SoXCvw8I</li> <li>Discrete random variables - part 1/5 (continuous vs discrete) - https://www.youtube.com/watch?v=ajLFqrPTAcY</li> </ul> <p>part 2: https://youtu.be/FrL4Dcoy9MI part 3: https://youtu.be/NXUkzZhrrcA part 4: https://youtu.be/cnJjKX5AHi4 part 5: https://youtu.be/NTWD-EyTkR0</p>"},{"location":"STATS2/WEEK%201/aq_solutions/","title":"Assignment Question Solutions","text":"<p>Warning</p> <ul> <li>Please try to solve the questions on your own first.</li> <li>Thinking that \"I have seen the solutions , I understand everything\" is WRONG. You actually don't understand anything unless you solve questions.</li> <li>For detailed explanations look for posts on discourse or scroll through live lectures.</li> </ul>"},{"location":"STATS2/WEEK%201/aq_solutions/#aq-11","title":"AQ 1.1","text":""},{"location":"STATS2/WEEK%201/aq_solutions/#aq12","title":"AQ1.2","text":""},{"location":"STATS2/WEEK%201/aq_solutions/#aq13","title":"AQ1.3","text":""},{"location":"STATS2/WEEK%201/aq_solutions/#aq14","title":"AQ1.4","text":""},{"location":"STATS2/WEEK%201/aq_solutions/#aq15","title":"AQ1.5","text":""},{"location":"STATS2/WEEK%201/aq_solutions/#aq16","title":"AQ1.6","text":""},{"location":"STATS2/WEEK%201/aq_solutions/#aq17","title":"AQ1.7","text":""},{"location":"STATS2/WEEK%201/Notes/Two%20Discrete%20Random%20Variables/","title":"Discrete Random Variables","text":"<p>Discrete random variables are variables that can take on a countable number of distinct values.  These values are typically integers or whole numbers and are often the result of counting or enumerating something. </p> <p>Example</p> <ul> <li>Coin Toss : Consider a random variable \\(X\\) which represents the number of heads when a coin is tossed \\(2\\) times. We can see that \\(X\\) can only take the values \\(0 , 1 , 2\\). Here \\(X\\) is a discrete random variable.</li> <li>Russian Roulette : Consider a random variable \\(Y\\). \\(Y = 0\\) when a bullet is not fired and \\(Y=1\\) when  a bullet is fired. \\(Y\\) takes the values of \\(0,1\\). Here \\(Y\\) is a discrete random variable.</li> <li>Dice Roll : Consider a random variable \\(Z\\). \\(Z\\) can only take values \\(0,1,2,3,4,5,6\\) and each of these  values has a corresponding probability. Here \\(Z\\) is a discrete random variable.</li> </ul>"},{"location":"STATS2/WEEK%201/Notes/Two%20Discrete%20Random%20Variables/#takeaways-from-this-week","title":"Takeaways from this Week","text":"<ul> <li>You should be able to understand how the joint distribution tables work.</li> <li>Marginal PMF and Conditional Distribution are the most important concept in this week and will be used in the future.</li> </ul>"},{"location":"STATS2/WEEK%201/Notes/Two%20Discrete%20Random%20Variables/#joint-pmf","title":"Joint PMF","text":"<p>A joint PMF (Probability Mass Function) distribution refers to the probability distribution of two or  more random variables occurring together. It gives the probabilities for all possible combinations of values of these variables.</p> <p>Suppose \\(X\\) and \\(Y\\) are discrete random variables defined in the same probability space. Let the range of \\(X\\) and \\(Y\\) be \\(T_X\\) and \\(T_Y\\) , respectively . The joint PMF of \\(X\\) and \\(Y\\) , denoted by \\(f_{XY}\\) , is a function from \\(T_X \\times T_Y\\) to [0,1] defined as</p> \\[f_{XY} = P(X = t_1 \\ and \\ Y=t_2) , t_1 \\in T_X , t_2 \\in T_Y\\] <p>It is usually written in a table or a matrix.</p> <p>Example</p> <p>Coin Toss</p> <p>Let \\(X_i = 1\\) if \\(i^{th}\\) toss is heads and \\(X_i = 0\\) if toss is tales.</p> \\(t_2\\) \\ \\(t_1\\) 0 1 0 1/4 1/4 1 1/4 1/4 <p>Then , \\(f_{XY}(0,0) = P(X_1=0,X_2=0) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}\\) </p> <p>Picking Marbles</p> <p>Consider a bag containing three red marbles (R) and two blue marbles (B).  We randomly select two marbles from the bag without replacement and observe their colors.</p> <p>Joint PMF Distribution would provide all the possibilites of picking 2 marbles from the bag.</p> Marble 1 Marble 2 Probability (P) R R 3/10 R B 3/10 B R 3/10 B B 1/10 <p>Let \\(X,Y\\) represent the color of first and second marbles respectively. Then,</p> <ul> <li>\\(f_{XY}(R,R) = \\frac{3}{5} \\times \\frac{2}{4} = \\frac{3}{10}\\) (Both marbles are red)</li> <li>\\(f_{XY}(R,B) = \\frac{3}{5} \\times \\frac{2}{4} = \\frac{3}{10}\\) (\\(X\\) is red and \\(Y\\) is blue)</li> </ul> <p>Info</p> <p>The sum of all the Joint PMFs will always be 1</p>"},{"location":"STATS2/WEEK%201/Notes/Two%20Discrete%20Random%20Variables/#marginal-pmf-probability-mass-function","title":"Marginal PMF (Probability Mass Function)","text":"<p>The marginal PMF (Probability Mass Function) refers to the probabilities of a single random variable,  independently of the other variables in a joint probability distribution.</p> <p>Suppose \\(X\\) and \\(Y\\) are jointly distributed discrete random variables with joint PMF \\(f_{XY}\\). The PMF of the individual random variables \\(X\\) and \\(Y\\) are called as marginal PMFs. It can be shown that</p> \\[f_X(t)=P(X=t)=\\sum_{t'\\in T_Y} f_{XY}(t,t')\\] \\[f_Y(t)=P(Y=t)=\\sum_{t'\\in T_X} f_{XY}(t,t')\\] <p>where \\(T_X\\) and \\(T_Y\\) are the ranges of \\(X\\) and \\(Y\\) , respectively.</p> <p>Example</p> \\(t_2\\) \\ \\(t_1\\) 0 1 \\(f_{x_{2}}(t_2)\\) 0 1/4 1/4 1/2 1 1/4 1/4 1/2 \\(f_{x_{1}}(t_1)\\) 1/2 1/2 <ul> <li>Adding along the rows and adding along the columns gives us the Marginal PMF.</li> <li> <p>In our case the marignal PMF of \\(X_1\\) will be</p> <ul> <li>\\(f_{X_1}(0) = f_{X_{1}X_2}(0,0) + f_{X_{1}X_2}(0,1) = \\frac{1}{4} + \\frac{1}{4}\\)</li> <li>\\(f_{X_1}(1) = f_{X_{1}X_2}(1,0) + f_{X_{1}X_2}(1,1) = \\frac{1}{4} + \\frac{1}{4}\\)</li> </ul> </li> </ul>"},{"location":"STATS2/WEEK%201/Notes/Two%20Discrete%20Random%20Variables/#conditional-distribution-of-one-random-variable-given-another","title":"Conditional distribution of one random variable given another","text":"<p>Conditional distribution refers to the probability distribution of one random variable given the knowledge  or condition of another random variable. It provides the probabilities for the values of one variable,  taking into account specific conditions or values of another variable.</p> <p>In simpler terms, it allows us to understand how the distribution of one variable changes  or is affected when we consider a specific condition or value of another variable.</p> <p>Suppose \\(X\\) and \\(Y\\) are jointly distributed discrete random variables with joint PMF \\(f_{XY}\\). The conditional PMF of \\(Y\\) given \\(X=t\\) is defined as the PMF:</p> \\[Q(t')=P(Y=t'|X=t)=\\frac{P(Y=t' , X=t)}{P(X=t)} = \\frac{f_{XY}}{f_{X}(t)}\\] <p>Example</p> \\(t_2\\) \\ \\(t_1\\) 0 1 2 \\(f_Y(t_2)\\) 0 1/4 1/8 1/8 1/2 1 1/8 1/8 1/4 1/2 \\(f_X(t_1)\\) 3/8 1/4 3/8 <p>Where , \\(X \\in \\{0,1,2\\}\\) , \\(Y \\in \\{0,1\\}\\) and \\((Y|X=0) \\in \\{0,1\\}\\)</p> <ul> <li>\\(f_{Y|X = 0}(0) = \\frac{f_{XY}(0,0)}{f_X(0)} = \\frac{1/4}{3/8} = \\frac{2}{3}\\)</li> <li>\\(f_{Y|X = 0}(1) = \\frac{f_{XY}(0,1)}{f_X(0)} = \\frac{1/8}{3/8} = \\frac{1}{3}\\)</li> </ul>"},{"location":"STATS2/WEEK%201/Notes/Two%20Discrete%20Random%20Variables/#factoring","title":"Factoring","text":"<p>This is just a formula , mug it up \ud83d\udc80.</p> \\[\\begin{align*} f_{X_1X_2X_3X_4}(t_1,t_2,t_3,t_4) = P(X_4 = t_4 , X_3 = t_3 , X_2 = t_2 , X_1 = t_1) \\\\ \\implies f_{X_4 \\mid X_3 = t_3 , X_2 = t_2 , X_1=t_1}(t_4) \\times f_{X_3 \\mid X_2 = t_2 , X_1=t_1}(t_3) \\times f_{X_2 \\mid X_1=t_1}(t_1) \\times f_{X_1}(t_1) \\end{align*}\\]"},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/","title":"Parameter Estimation 2","text":""},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#bayesian-estimation","title":"Bayesian Estimation","text":"<p>\\(\\(X_1 , X_2 .... X_n \\sim , \\text{parameter } \\Uptheta\\)\\) - Prior distribution of \\(\\Uptheta\\) : \\(\\Uptheta \\sim f(\\Uptheta)\\) - Bayes' rule : posterior \\(\\propto\\) likelihood \\(\\times\\) prior </p> <p>\\(\\(P(\\Uptheta = \\theta | S) = \\frac{P(S|\\Uptheta = \\theta) \\times f_{\\Uptheta}(\\theta)}{P(S)}\\)\\) ![[Pasted image 20230410153658.png]]</p>"},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#choice-of-priors","title":"Choice of Priors","text":""},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#types-of-priors","title":"Types of Priors","text":"<ul> <li> <p>Flat , uninformative: </p> <ul> <li>Nearly flat over the interval in which the parameter takes value </li> <li>This usually reduces to something close to maximum likelihood.</li> </ul> </li> <li> <p>Conjugate Priors: </p> <ul> <li>Pick a prior so that the posterior is in the same class as the prior.</li> </ul> </li> <li> <p>Informative Priors: </p> <ul> <li>This needs some justification from the domain of the problem.</li> <li>Parameterize the prior so that its flatness can be controlled.</li> </ul> </li> </ul>"},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#examples","title":"Examples","text":""},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#bernoulli-distribution","title":"Bernoulli Distribution","text":"<p>\\(\\(X_1 , X_2 .... X_n \\sim \\text{iid } \\text{Bernoulli}(\\mathbf{p})\\)\\) - Prior \\(\\mathbf{p} \\sim \\text{Uniform}[0,1]\\) , continuous distribution  - Samples: \\(x_1 , x_2 , x_3 ..... x_n\\) - \\(w = x_1 + x_2 + ... + x_n\\)  - Posterior density: \\(\\text{Beta}(w + 1 , n -w + 1)\\)     - Posterior Mean: \\(\\frac{w+ 1}{(w+1) + (n-w+1)} = \\frac{w+1}{n+2} =\\frac{x_1 + x_2 + ... +x_n + 1}{n+2}\\)</p>"},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#bernoulli-distribution-with-beta-prior","title":"Bernoulli Distribution with beta prior","text":"<p>\\(\\(X_1 , X_2 .... X_n \\sim \\text{iid } \\text{Bernoulli}(\\mathbf{p})\\)\\) - Prior \\(\\mathbf{p} \\sim \\text{Beta}(\\alpha,\\beta)\\) , continuous distribution  - \\(w = x_1 + x_2 + ... + x_n\\)  - Posterior Density: \\(\\text{Beta}(w + \\alpha , n - w + \\beta)\\)     - Posterior Mean: \\(\\frac{w + \\alpha}{(w + \\alpha) + (n - w + \\beta)} = \\frac{w + \\alpha}{n + \\alpha + \\beta}\\)</p>"},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#observations-for-beta-prior","title":"Observations for Beta Prior","text":"<ul> <li>Prior : \\(\\text{Beta}(\\alpha , \\beta)\\)<ul> <li>\\(\\alpha , \\beta \\geq 0\\)</li> <li>PDF \\(\\propto p^{\\alpha - 1}(1-p)^{\\beta -1} , 0 &lt; p &lt; 1\\)</li> </ul> </li> <li>\\(\\alpha = \\beta = 1\\)<ul> <li>Flat prior </li> <li>Estimate close to but not equal to maximum -likelihood </li> </ul> </li> <li>\\(\\alpha = \\beta = 0\\)<ul> <li>Estimate coincides with Maximum-likelihood.</li> </ul> </li> <li>\\(\\alpha = \\beta\\)<ul> <li>Symmetric Prior </li> </ul> </li> </ul>"},{"location":"STATS2/WEEK%2010/Parameter%20Estimation%202/#normal-sample-with-known-mean-and-known-variance","title":"Normal sample with known mean and known variance","text":"<p>\\(\\(X_1 , X_2 .... X_n \\text{iid Normal}(M ,\\sigma^2)\\)\\) Prior \\(M \\sim Normal(\\mu_{0} , \\sigma_{0}^{2})\\) , continuous distribution  \\(f_M(\\mu) = \\frac{1}{\\sqrt{2 \\pi}\\sigma_0} \\text{exp}(- \\frac{(\\mu - \\mu_0)^2}{2 \\sigma_{0}^{2}})\\)</p> <ul> <li>Posterior Density : Normal </li> <li>Posterior Mean = \\(\\overline{x}\\frac{n \\sigma_{0}^{2}}{n \\sigma_{0}^{2} + \\sigma^2} + \\mu_0 \\frac{\\sigma^2}{n \\sigma_{0}^{2} + \\sigma^2}\\)</li> </ul>"},{"location":"STATS2/WEEK%2011/Hypothesis%20Testing/","title":"Hypothesis Testing","text":"<p>Using samples , decide between a null hypothesis denoted \\(H_0\\) and an alternative hypothesis denoted \\(H_A\\) .</p>"},{"location":"STATS2/WEEK%2011/Hypothesis%20Testing/#acceptance-set-and-test","title":"Acceptance Set and Test","text":"<p>\\(X_1 , X_2 .... X_n \\sim X , H_0 : \\text{null hypothesis , }H_A : \\text{alternative hypothesis.}\\) - Suppose \\(X \\in \\symbfscr{X}\\). Then the samples \\(X_1 , X_2 ... X_n \\in \\symbfscr{X}^n\\) - Subset \\(A \\subseteq \\symbfscr{X}\\)  If \\(X_1 , X_2 ... X_n \\in A\\) we accept \\(H_0\\) otherwise we reject \\(H_0\\)</p>"},{"location":"STATS2/WEEK%2011/Hypothesis%20Testing/#size-and-power-of-test","title":"Size and Power of Test","text":"<p>Type I error: It is also known as the size of a test , denoted as \\(\\alpha\\) - Reject \\(H_0\\) when \\(H_0\\) is true  - \\(\\alpha = P(\\text{Type I error}) = P(\\text{Reject }H_0 | H_0 \\text{ is true})\\) Type II error: It is also known as the power of a test , denoted as \\(1 - \\beta\\)  - Accept \\(H_0\\) when \\(H_A\\) is true  - \\(\\beta = P(\\text{Type II Error}) = P(\\text{Accept } H_0|H_A \\text{ is true})\\)</p>"},{"location":"STATS2/WEEK%2011/Hypothesis%20Testing/#types-of-tests","title":"Types of Tests","text":"<pre><code>$c$ = critical value\n</code></pre>"},{"location":"STATS2/WEEK%2011/Hypothesis%20Testing/#right-tailed-test","title":"Right Tailed Test","text":"<p>\\(H_0 : \\mu = \\mu_0\\) \\(H_A : \\mu &gt; \\mu_0\\) \\(T = \\overline{X}\\) Rejection Region: \\(\\overline{X} &gt; c\\) \\(Z = \\frac{\\overline{X} - \\mu_0}{\\sigma / \\sqrt{n}}\\)</p>"},{"location":"STATS2/WEEK%2011/Hypothesis%20Testing/#left-tailed-test","title":"Left Tailed Test","text":"<p>\\(H_0 : \\mu = \\mu_0\\) \\(H_A : \\mu &lt; \\mu_0\\) \\(T = \\overline{X}\\) Rejection Region: \\(\\overline{X} &lt; c\\)</p>"},{"location":"STATS2/WEEK%2011/Hypothesis%20Testing/#two-tailed-test","title":"Two Tailed Test","text":"<p>\\(H_0 : \\mu = \\mu_0\\) \\(H_A : \\mu \\neq \\mu_0\\) \\(T = \\overline{X}\\) Rejection Region: \\(|\\overline{X} - \\mu| &gt; c\\)</p>"},{"location":"STATS2/WEEK%2012/Different%20Types%20of%20Tests/","title":"Different Types of Tests","text":""},{"location":"STATS2/WEEK%2012/Different%20Types%20of%20Tests/#normal-samples-and-statistics","title":"Normal Samples and Statistics","text":"\\[X_1 , X_2 , X_3 ..... X_n \\sim iid \\text{ Normal}(\\mu , \\sigma^2)$$ - Sample Mean $\\overline{X} = \\frac{1}{n}(X_1 + X_2 + X_3 .... + X_n)$ - Sample Variance $S^2 = \\frac{1}{n-1}((X_1 - \\overline{X})^2 + (X_2 - \\overline{X})^2 + ...... + (X_n - \\overline{X})^2)$ $$\\overline{X} \\sim \\text{ Normal}(\\mu , \\frac{\\sigma^2}{n})$$ $$\\frac{(n-1)}{\\sigma^2}S^2 \\sim \\chi^{2}_{n-1}$$ $$\\frac{\\overline{X} - \\mu}{S / \\sqrt{n}} \\sim t_{n-1}\\]"},{"location":"STATS2/WEEK%2012/Different%20Types%20of%20Tests/#t-test-for-mean-unknown-variance","title":"T-Test for Mean (unknown variance)","text":"<p>\\(\\(X_1 , X_2 ,  ...... ,X_n \\sim \\text{ Normal}(\\mu , \\sigma^2)\\)\\) - Null \\(H_0 : \\mu = \\mu_0, \\text{ Alternative } H_A : \\mu &gt; \\mu_0\\) - \\(T = \\overline{X}\\) , Test : Reject \\(H_0 \\text{ if } T &gt; c\\)</p> <p>Computing Significance Level - Sample Variance \\(S^2 = \\frac{1}{n-1}\\sum^{n}_{i=1}(X_i - \\overline{X})^2\\) - Given , \\(H_0 ,\\frac{T - \\mu_0}{S /  \\sqrt{n}} \\sim t_{n-1}\\) \\(\\(\\alpha = P(T &gt; c | \\mu = \\mu_0) = P(t_{n-1} &gt; \\frac{c - \\mu_0}{S / \\sqrt{n}}) = 1 - F_{t_{n-1}}(\\frac{c- \\mu_0}{S/ \\sqrt{n}})\\)\\)</p>"},{"location":"STATS2/WEEK%2012/Different%20Types%20of%20Tests/#mathbfchi2-test-for-variance","title":"\\(\\mathbf{\\chi^2}\\) test for variance","text":"<p>\\(\\(X_1 , X_2 , ..... iid \\text{ Normal}(\\mu , \\sigma^2)\\)\\) - Null \\(H_0 : \\sigma = \\sigma_0\\) , Alternative \\(H_A : \\sigma &gt; \\sigma_0\\) - \\(S^2 = \\frac{1}{n-1}\\sum^{n}_{i=1}(X - \\overline{X})^2\\) ,  Test: Reject \\(H_0\\) if \\(S &gt;c\\)</p> <p>Computing Significance Level - Given \\(H_0\\) , \\(\\frac{n-1}{\\sigma_0^2}S^2 \\sim \\chi^{2}_{n-1}\\) - \\(\\alpha = P(S &gt; c | H_0) = P(\\frac{n-1}{\\sigma_0^2})S^2 &gt; \\frac{n-1}{\\sigma_0^2})c^2 = 1 - F_{\\chi^{2} _{n-1}}(\\frac{n-1}{\\sigma_0^2}c^2)\\)</p>"},{"location":"STATS2/WEEK%2012/Different%20Types%20of%20Tests/#two-samples-from-normal-distribution","title":"Two Samples from Normal Distribution","text":"<p>\\(\\(X_1 , X_2 , X_3 , ...... X_n \\sim \\text{iid Normal}(\\mu_1 , \\sigma^2_1 )\\)\\) \\(\\(Y_1 , Y_2 , Y_3 , ...... Y_n \\sim \\text{iid Normal}(\\mu_2 , \\sigma^2_2 )\\)\\) - \\(\\overline{X} \\sim \\text{ Normal}(\\mu_1 , \\sigma_1^2 / n_1)\\) , \\(\\overline{Y} \\sim \\text{ Normal}(\\mu_2 , \\sigma_2^2 / n_2)\\) - \\((\\frac{n_1-1}{\\sigma^2_1})S^2_X \\sim \\chi^2_{n_1 - 1}\\) , \\((\\frac{n_2-1}{\\sigma^2_2})S^2_Y \\sim \\chi^2_{n_2 - 1}\\) - \\(\\overline{X} - \\overline{Y} \\sim \\text{Normal}(\\mu_1 - \\mu_2 , \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2})\\) - If \\(\\sigma_1 = \\sigma_2\\)     - \\(\\frac{S^2_X}{S^2_Y} \\sim F(n_1 -1 , n_2 - 1)\\)</p>"},{"location":"STATS2/WEEK%2012/Different%20Types%20of%20Tests/#two-samples-z-test","title":"Two Samples Z-test","text":"<p>\\(\\(X_1 , X_2 , X_3 , ...... X_n \\sim \\text{iid Normal}(\\mu_1 , \\sigma^2_1 )\\)\\) \\(\\(Y_1 , Y_2 , Y_3 , ...... Y_n \\sim \\text{iid Normal}(\\mu_2 , \\sigma^2_2 )\\)\\) - Null \\(H_0 : \\mu = \\mu_0, \\text{ Alternative } H_A : \\mu \\neq \\mu_0\\) - \\(T = \\overline{Y} - \\overline{X}\\) , Test : Reject \\(H_0 \\text{ if } |T| &gt; c\\)</p> <p>Computing Significance Level Given \\(H_0\\) , \\(T \\sim \\text{Normal}(0 , \\sigma_{T}^{2})\\) , where \\(\\sigma_T^2 = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma^2_2}{n_2}\\) \\(\\(\\alpha = P(|T| &gt; c | H_0) = P(|\\text{Normal}(0,1)| &gt; \\frac{c}{\\sigma_{T}})\\)\\)</p>"},{"location":"STATS2/WEEK%2012/Different%20Types%20of%20Tests/#two-sample-f-test","title":"Two Sample F-test","text":"<p>\\(\\(X_1 , X_2 , X_3 , ...... X_n \\sim \\text{iid Normal}(\\mu_1 , \\sigma^2_1 )\\)\\) \\(\\(Y_1 , Y_2 , Y_3 , ...... Y_n \\sim \\text{iid Normal}(\\mu_2 , \\sigma^2_2 )\\)\\) - Null \\(H_0 : \\sigma_1 = \\sigma_2\\) , Alternative \\(H_A : \\sigma_1 \\neq \\sigma_2\\) - T = \\(\\frac{S_X^2}{S_Y^2}\\) , Test : Reject \\(H_0\\) if \\(T &gt; 1 + c_R\\) or \\(T &lt; 1 - c_L\\) - Given \\(H_0\\) , \\(T \\sim F(n_1 -1 , n_2 -1)\\) - \\(\\alpha / 2 = P(T &lt; 1 - c_L | H_0) = P(T &gt; 1 + c_R | H_0)\\)</p>"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/","title":"Extra Content","text":""},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#visualizing-random-variables","title":"Visualizing Random Variables","text":"<p>Sometimes we dont want the usual representation of random variables, that is when we use functions. Functions change the horizontal axis of the graph. \\(f(x) = x- 10\\) is a function which shifts the x axis to the left by 10.</p> <p>Sometimes these functions can either be one to one , which means that each input has a unique output/ no two outputs are the same OR the functions can be many to one , which means outputs for different inputs can be the same/ two or more outputs are same.</p> <p>See This for what changes occurs on different types of functions.</p>"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#many-to-one-functions","title":"Many To One Functions","text":"<p>In the case of many to one functions we add the probabilities when the outputs are the same.</p> <p></p> <p>Info</p> <p>If two variables \\(X\\) and \\(Y\\) are independent then their functions \\(f(X)\\) and \\(g(Y)\\) will also be independent.</p>"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#formulas","title":"Formulas","text":"<p>You are probably better off mugging up these because its not gonna come in future weeks. Also you will be provided with a forumla sheet in the exam with all the formula required for STATS2.</p>"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#two-uniformly-distributed-iid-random-variables","title":"Two uniformly distributed iid random variables","text":""},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#sum","title":"Sum","text":"<p>Given that \\(X,Y \\sim Uniform \\{1,2,3,4.....n\\} , W=X+Y\\) \\(\\implies W \\in \\{2,3,4,5....2n\\}\\)</p> \\[P(W=w) =  \\begin{cases} \\frac{w-1}{n^2}, &amp; 2 \\leq w \\leq n+1 \\\\ \\frac{2n - w + 1}{n^2} &amp; n+2 \\leq w \\leq 2n \\end{cases}\\]"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#maximum","title":"Maximum","text":"<p>Given that \\(X,Y \\sim Uniform \\{1,2,3,4.....n\\} , Z=\\max(X,Y)\\)</p> <p>\\(\\implies Z \\in \\{1,2,3,....n\\}\\)</p> \\[P(Z=z) = \\frac{2z-1}{n^2}\\]"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#sum-of-n-independent-bernoulli-trials","title":"Sum of n independent bernoulli trials","text":"<p>Let \\(X_1 , X_2 , X_3 .... X_n\\) be the results of \\(n\\) i.i.d \\(Bernoulli(p)\\) trials. </p> <p>The sum of the n random variables \\(X_1 , X_2 , X_3 .... X_n\\) is \\(Binomial(n,p)\\)</p>"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#sum-of-2-random-variables-taking-integer-values","title":"Sum of 2 random variables taking integer values","text":"<p>Suppose \\(X\\) and \\(Y\\) take integer values and let their joint PMF be \\(f_{XY}\\). Let \\(Z = X+Y\\)</p> <p>Let \\(z\\) be some integer.</p> \\[\\begin{align}     P(Z=z) &amp;= P(X+Y=z) \\\\     &amp;= \\sum^{\\infty}_{x=- \\infty} P(X=x , Y=z-x) \\\\     &amp;= \\sum^{\\infty}_{x=- \\infty}f_{XY}(x , z-x) \\\\     &amp;= \\sum^{\\infty}_{x=- \\infty}f_{XY}(z-y , y) \\\\ \\end{align}\\]"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#convolution","title":"Convolution","text":"<p>If \\(X\\) and \\(Y\\) are independent,</p> \\[ f_{X+Y}(z) = \\sum^{\\infty}_{x = - \\infty}f_{X}f_{Y}(z-x) \\]"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#two-independent-poisson","title":"Two Independent Poisson","text":""},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#sum_1","title":"Sum","text":"<p>\\(Z = X+Y\\)</p> \\[f_Z(Z) = \\frac{e^{-(\\lambda_1 + \\lambda_2)} \\times (\\lambda_1 + \\lambda_2)^Z}{Z!} \\]"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#conditional-distribution-of-xz","title":"Conditional distribution of X|Z","text":"\\[P(X=k|Z=n) = \\frac{n!}{k!(n-k)!} \\times (\\frac{\\lambda_1}{\\lambda_1 + \\lambda_2})^k \\times (\\frac{\\lambda_2}{\\lambda_1 + \\lambda_2})^{n-k}\\] <p>which is also equals to </p> \\[P(X=k|Z=n) = Binomial(n, \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2})\\] <p>given that \\(X|Z \\sim Binomial(n, \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2})\\)</p>"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#max-of-cdf-of-2-independent-random-variables","title":"Max of CDF of 2 independent random variables","text":"<p>Definition (CDF of a random variable)</p> <p>Cumulative distribution function of a random variable \\(X\\) is a function \\(F_X : \\mathbb{R} \\to [0,1]\\) defined as</p> \\[ F_{X}(x) = P(X \\leq x)\\] <p>Suppose \\(X\\) and \\(Y\\) are independent and \\(Z = \\text{max}(X,Y)\\).</p> \\[\\begin{align}     F_Z(z) &amp;= P(\\text{max}(X,Y) \\leq z) \\\\     &amp;= P((X \\leq z) \\text{and} (Y \\leq z)) \\\\     &amp;= P(X \\leq z)P(Y \\leq z) \\\\     &amp;= F_X(z)F_Y(z) \\end{align}\\]"},{"location":"STATS2/WEEK%202/Notes/Extra%20Content/#min-of-2-independent-geometric-random-variables","title":"Min of 2 independent Geometric Random Variables","text":""},{"location":"STATS2/WEEK%202/Notes/Notes/","title":"Independent Random Variables","text":""},{"location":"STATS2/WEEK%202/Notes/Notes/#takeaways-from-this-week","title":"Takeaways from this week","text":"<ul> <li>Being able to check whether a given join distribution has indepenedent random variables or not.</li> <li>Memoryless Property </li> </ul>"},{"location":"STATS2/WEEK%202/Notes/Notes/#independence-of-random-variables","title":"Independence of Random Variables","text":"<p>Let \\(X\\) and \\(Y\\) be two random variables defined in a probability space with ranges \\(T_X\\) and \\(T_Y\\) respectively. \\(X\\) and \\(Y\\) are considered independent if :</p> \\[f_{XY}(t_1 , t_2) = f_X(t_1) \\times f_{Y|X=t_1}(t2)\\] <p>where,</p> \\[f_{Y|X=t_1}(t2) = f_Y(t_2)\\] \\[\\therefore f_{XY}(t_1 , t_2) = f_X(t_1) \\times f_Y(t_2)\\] <p>Info</p> <ul> <li>Joint PMF is the product of the marginal PMFs when the variables are independent.</li> <li>All the subsets of independent random variables are independent.</li> </ul>"},{"location":"STATS2/WEEK%202/Notes/Notes/#checking-independence-of-random-variables","title":"Checking Independence of Random Variables","text":"<ul> <li>For every element in the table of 2 or more random variables. Each entry must be the product of their respective marginal PMFs then only they are considered independent. </li> <li>If for any element \\(f_{XY}(t_1 , t_2) \\neq f_X(t_1)f_Y(t_2)\\) then the variables are considered dependent.</li> </ul> <p>Tip</p> <ul> <li>i.i.ds (independent and identically distributed) are one of the examples for independent random variables for any \\(f_{XY}(t_1 , t_2) \\neq 0\\)</li> <li>Finding dependent variables is easier when \\(f_{XY}(t_1)(t_2) =0\\). The logic behind it is for some \\(t_1\\) and \\(t_2\\) \\(f_X(t_1) \\times f_Y(t_2) \\neq 0\\) , if it is 0 then it would mean that either or both of the marginals are 0 which is generally not true.</li> </ul>"},{"location":"STATS2/WEEK%202/Notes/Notes/#geometric-iid","title":"Geometric iid","text":""},{"location":"STATS2/WEEK%202/Notes/Notes/#question-1","title":"Question 1","text":"<p>Let \\(X_1 , X_2 , X_3 ... , X_n\\) be an iid with a Geometric(p) distribution. What is the probability that all of these random variables are larger than some positive integer \\(j\\).</p> \\[X \\sim \\{1,2,3,....\\}\\] \\[P(X=k) = (1-p)^{k-1}p\\] <p>The probability that the random variables are greater than \\(j\\) is :</p> \\[(P(X &gt; j))^n = (1-p)^{jn}\\]"},{"location":"STATS2/WEEK%202/Notes/Notes/#question-2","title":"Question 2","text":"<p>Let \\(X \\sim \\{\\stackrel{\\frac{1}{2}}{0},\\stackrel{\\frac{1}{4}}{1} , \\stackrel{\\frac{1}{8}}{2} , \\stackrel{\\frac{1}{16}}{3} , \\stackrel{\\frac{1}{16}}{4}\\}\\) , and let \\(X_1 ..... X_n\\) be the iid samples with distribution \\(X\\).</p>"},{"location":"STATS2/WEEK%202/Notes/Notes/#what-is-the-probability-that-4-is-missing-in-some-samples","title":"What is the probability that 4 is missing in some samples.","text":"<ul> <li>\\(P(X_1 \\neq 4 , X_2 \\neq 4 , X_3 \\neq 4 , X_4 \\neq 4 .... X_n \\neq 4)\\)</li> <li>\\((P(X \\neq 4))^n\\) as all the probabilities are same for an iid.</li> <li>\\((P(X \\neq 4))^n = (1 - P(X = 4))^n\\)</li> <li>\\((P(X \\neq 4))^n = (1 - \\frac{1}{16})^n = (\\frac{15}{16})^n\\)</li> </ul>"},{"location":"STATS2/WEEK%202/Notes/Notes/#what-is-the-probability-that-4-appears-exactly-once","title":"What is the probability that 4 appears exactly once.","text":"<p>\\(\\implies P( \\text{4 Appears exactly once})=\\)</p> \\[\\begin{align} \\implies P(X_1 = 4 , X_2 \\neq 4 , X_3 \\neq 4 .... X_n \\neq 4) + \\\\  P(X_1 \\neq 4 , X_2 = 4 , X_3 \\neq 4 ... X_n \\neq 4) + ... \\\\ P(X_1 \\neq 4 , X_2 \\neq 4 , X_3 \\neq 4 .... X_{n-1} \\neq 4 , X_n =4) \\end{align}\\] <p>\\(\\implies n \\times P(X \\neq 4)^{n-1} \\times P(X=4)\\) \\(\\implies n \\times (\\frac{15}{16})^{n-1} \\times (\\frac{1}{16})\\)</p>"},{"location":"STATS2/WEEK%202/Notes/Notes/#memoryless-property","title":"Memoryless property","text":"<p>Just mug up the formula for this one.\ud83d\udc80\ud83e\udd72</p> \\[P(X &gt; k+m | X&gt;m ) = P(X&gt;k)\\] <p>For a detailed proof see the lecture.</p>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/","title":"Expectation of Random Variables","text":"<p>Suppose \\(X\\) is a random variables defined in the range of \\(T_X\\) and PMF of \\(X\\) is \\(f_X\\) . The expected value of the random variables \\(X\\) will be $$ E[X] = \\sum_{t \\in T_X} t \\times f_X(t) = \\sum_{t \\in T_X} t \\times P(X =t) $$</p> <pre><code>$E[X]$ has the same unit of $X$\n\n$E[X]$ may or may not belong to range of $X$\n\n```ad-info \ntitle: Expectation Properties\n$E[c \\times X] = c \\times E[X]$ , where $c$ is a constant and $X$ is a random variable.\n\n$E[X + Y] = E[X] + E[Y]$ , where $X$ and $Y$ are 2 random variables.\n$E[X - Y] = E[X] + E[Y]$ , this is right dont worry (to future self)\n\n\n**If X and Y are independent**\n\n$E[XY] = E[X]E[Y]$\n```\n</code></pre>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#types-of-random-variables","title":"Types of Random Variables","text":""},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#uniform-random-variable","title":"Uniform Random Variable","text":"<p>Given that , \\(X \\sim Uniform \\set{a , a+1 , a+2 , ... b}\\) $$ E[X] = \\frac{a+b}{2} $$</p>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#variance","title":"Variance","text":"\\[ Var(X) = \\frac{(b-a)^2}{12} = \\sigma^2 \\]"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#geometric-random-variable","title":"Geometric Random Variable","text":"<p>Given that , \\(X \\sim Geometric(p)\\) $$ E[X] = \\sum^{\\infty}_{t=1} t(1-p)^{t-1}p = \\frac{1}{p} $$</p>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#variance_1","title":"Variance","text":"\\[ Var(X) = \\frac{1-p}{p^2} = \\sigma^2 \\]"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#poisson-random-variable","title":"Poisson Random Variable","text":"<p>Given that , \\(X \\sim Poisson(\\lambda)\\) $$ E[X] = \\sum^{\\infty}_{t = 0} t \\times e^{- \\lambda} \\times \\frac{\\lambda ^ {t}}{t!} = \\lambda $$</p>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#variance_2","title":"Variance","text":"\\[ Var(X) = \\lambda = \\sigma^2 \\]"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#binomial-random-variable","title":"Binomial Random Variable","text":"<p>Given that , \\(X \\sim Binomial(n,p)\\) $$ E[X] = \\sum^{n}_{t=0} t \\times {n \\choose x} p^t (1-p)^{n-t} = np $$</p>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#variance_3","title":"Variance","text":"\\[ Var(X) = np(1-p) = \\sigma^2 \\]"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#bernoulli-random-variable","title":"Bernoulli Random Variable","text":"<p>Given that , \\(X \\sim Bernoulli(p)\\) $$ E[X] = 0(1-p) + p = p $$</p>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#variance_4","title":"Variance","text":"\\[ Var(X) = p(1-p) = \\sigma^2 \\]"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#variance-and-standard-deviation-properties","title":"Variance and Standard Deviation Properties","text":"<p>Let \\(X\\) be a random variable. Let \\(a\\) be a constant real number. $$ Var(X) = E[X^2] - E[X]^2 $$</p> <pre><code>title: Variance Properties \n- $Var(aX) = a^2Var(X)$\n- $SD(aX) = |a|SD(X)$\n- $Var(X + a) = Var(X)$\n- $SD(X + a) = SD(X)$\n\n```ad-info \n**If X and Y are independent**\n\n$Var(X+Y) = Var(X) + Var(Y)$\n```\n</code></pre>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#more-important-formulas","title":"More Important Formulas","text":""},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#standardized-random-variables","title":"Standardized Random Variables","text":"<p>A random variable \\(X\\) is said to be standardized if \\(E[X] = 0 , Var(X) = 1\\)</p> <p>Also, If \\(X\\) is a random variable.  Then, $$ \\frac{X - E[X]}{SD(X)} $$ is a standardized random variable.</p>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#covariance-formula","title":"Covariance Formula","text":"\\[ Cov(X,Y) = E[XY] - E[X] \\times E[Y] \\]"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#correlation-coefficient","title":"Correlation Coefficient","text":"\\[ Cor(X,Y) = \\frac{Cov(X,Y)}{SD(X) \\times SD(Y)} \\]"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#bounds-in-probabilities","title":"Bounds In Probabilities","text":""},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#markovs-inequality","title":"Markov's Inequality","text":"<p>Let \\(X\\) be a random variable. $$ P(X \\geq a) \\leq \\frac{E(X)}{a} $$ <pre><code>A shopkeeper sells mobile phones. The expected demand for mobile phones is\u00a04\u00a0per week. $X$\u00a0is denoting the number of phones sold in a week.\n\n$$\nP(X \\geq 10) \\leq 0.4\n$$\n\nAlso, \n\n$P(X &lt; 10) \\geq 0.6$\n</code></pre></p>"},{"location":"STATS2/WEEK%203/Expectation%20of%20Random%20Variables/#chebyshevs-inequality","title":"Chebyshev's Inequality","text":"<p>Let \\(X\\) be a random variable with mean \\(\\mu\\) and finite variance \\(\\sigma^2\\) , then for any real number \\(k&gt;0\\) $$ P(|X - \\mu| &lt; k \\sigma) \\geq 1 - \\frac{1}{k^2} $$ Also, $$ P(|X - \\mu| \\geq k \\sigma) \\leq \\frac{1}{k^2} $$</p>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/","title":"Cumulative Distribution Function","text":"<p>The CDF of a random variable \\(X\\) , denoted \\(F_X(X)\\) , is a function from \\(\\mathbb{R} \\to [0,1]\\) is defined as  $$ F_X(X) = P(X \\leq x) $$</p> <pre><code>title: Properties \n\n$F(X)$ is always a non-decreasing funciton taking values between 0 and 1.\n\n$P(a &lt; X \\leq b) = F_X(b) - F_X(a)$\n\nAs $X \\to - \\infty$ , $F_X$ goes to 0.\n\nAs $X \\to  \\infty$ , $F_X$ goes to 1.\n</code></pre>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#cdf-of-standardised-variables","title":"CDF Of Standardised Variables","text":"<p>Let a discrete random variable \\(X\\) have a CDF \\(F_X\\). Assume that \\(Y = \\frac{X - \\mu}{\\sigma}\\) , where \\(\\mu\\) and \\(\\sigma\\) are the mean and standard deviation of \\(X\\)  respectively. If \\(F_Y\\) is the CDF of \\(Y\\) , then  $$ F_Y(y) = F_X(\\mu + Y \\sigma) $$ <pre><code>title: CDF Formula and More Properties \nGiven a child CDF F(x), there exists a random variable $X$ taking values in $\\mathbb{R}$ such that \n\n$$P(X \\leq x) = F(x)$$\n\nProbability of $X$ taking a specific value is always 0.\n\n$P(X = 3) = 0$ for a random variable $X$.\n</code></pre></p>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#continuous-random-variable","title":"Continuous Random Variable","text":"<p>A random variable \\(X\\) with CDF \\(F_X(x)\\) is said to be continuous random variable if \\(F_X(x)\\) is continuous at every \\(x\\). - CDF has no jumps or steps. - So ,\\(P(X =x) = 0\\) for all \\(x\\) - Probability of \\(X\\) falling in an interval will be nonzero \\(\\(P(a &lt; X \\leq b) = F(b) - F(a)\\)\\) $$ \\therefore P(a \\leq X \\leq b) = P(a &lt; X \\leq b) = P(a \\leq X &lt; b) = P(a &lt; X &lt; b) $$ - Graphs of continuous random variables never breaks at any point and does not from one value to another.</p>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#probability-density-functions","title":"Probability Density Functions","text":"<p>They are a much better way to represent random variables on a graph.  $$ \\int^{b}_{a}f(x) dx = F(b) - F(a) = P(a &lt; X &lt; b) $$ Also integral of all the values a variable \\(X\\) takes is always 1. \\(\\implies \\int^{\\infty}_{ - \\infty} f(x) dx = 1\\)</p>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#common-distribution-functions","title":"Common Distribution Functions","text":""},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#uniform","title":"Uniform","text":"<p>\\(X \\sim Uniform[a,b]\\)</p>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#pdf","title":"PDF","text":"\\[ f_X(x) = \\begin{cases} \\frac{1}{b-a} &amp; a &lt; x &lt; b \\\\ 0 &amp; \\text{otherwise} \\end{cases}  \\]"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#cdf","title":"CDF","text":"\\[ F_X(x) = \\begin{cases} 0 &amp; x \\leq a  \\\\  \\frac{x-a}{b-a} &amp; a &lt; x &lt; b \\\\  1 &amp; x \\geq b \\end{cases} \\]"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#exponential","title":"Exponential","text":"<p>\\(X \\sim Exp(\\lambda)\\)</p>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#pdf_1","title":"PDF","text":"\\[ f_X(x) = \\begin{cases} \\lambda \\exp(-\\lambda x) &amp; x&gt;0 \\\\ 0 &amp; otherwise \\end{cases} \\]"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#cdf_1","title":"CDF","text":"\\[ F_X(x) = \\begin{cases} 0 &amp; x \\leq 0 \\\\  1 - \\exp(-\\lambda x) &amp; x &gt;0 \\end{cases} \\]"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#normal","title":"Normal","text":"<p>\\(X \\sim Normal(\\mu , \\sigma^2)\\)</p>"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#pdf_2","title":"PDF","text":"\\[ F_X(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp(- \\frac{(x - \\mu)^2}{2 \\sigma^2}) \\]"},{"location":"STATS2/WEEK%204/Continuous%20Random%20Variables/#cdf_2","title":"CDF","text":"\\[ F_X(x) = \\int^{x}_{- \\infty} f_X(u)du \\]"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/","title":"CDF of g(X)","text":"<ul> <li>Suppose \\(X\\) is a continuous random variable with CDF \\(F_X\\) and PDF \\(f_X\\)</li> <li>Suppose \\(g : \\mathbb{R} \\to \\mathbb{R}\\) is a function.</li> <li>Then , \\(Y = g(X)\\) is a random variable with CDF \\(F_Y\\) determined as follows. $$ F_Y(y) = P(Y \\leq y) = P(g(X) \\leq y) = P(X \\in \\set{x:g(x) \\leq y}) $$</li> </ul>"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/#monotonic-differentiable-functions","title":"Monotonic , differentiable functions","text":"<p>Suppose \\(X\\) is continuous random variable with PDF \\(f_X\\). Let g(x) be monotonic for \\(x \\in supp(x)\\) with derivative \\(g'(x) = \\frac{dg(x)}{dx}\\). Then, the PDF of \\(Y = g(X)\\) is  $$ f_Y(y) = \\frac{1}{|g'(g^{-1}(y))|}f_X(g^{-1}(y)) $$</p>"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/#translation","title":"Translation","text":"<p>\\(Y = X + a\\) $$ f_Y(y) = f_X(y-a) $$</p>"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/#scaling","title":"Scaling","text":"<p>\\(Y = aX\\) $$ f_Y(y) = \\frac{1}{|a|}f_X(\\frac{y}{a}) $$</p>"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/#affine","title":"Affine","text":"<p>\\(Y = aX + b\\) $$ f_Y(y) = \\frac{1}{|a|}f_X(\\frac{y-b}{a}) $$</p>"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/#affine-transformation-of-normal-distributions","title":"Affine transformation of normal distributions","text":"<ul> <li> <p>Given \\(X \\sim Normal(0,1)\\) $$ f_X(x) = \\frac{1}{\\sqrt{2 \\pi}}\\exp(\\frac{-x^2}{2}) $$ If \\(Y = \\sigma X + \\mu\\) $$ f_Y(y) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}\\exp(\\frac{-(y - \\mu)^2}{2 \\sigma^2}) $$ \\(Y \\sim Normal(\\mu , \\sigma^2)\\)</p> </li> <li> <p>\\(X \\sim Normal(\\mu , \\sigma^2)\\) \\(Y = \\frac{X - \\mu}{\\sigma} \\sim Normal(0,1)\\)</p> </li> </ul>"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/#expectation","title":"Expectation","text":"<p>Let \\(X\\) be a continuous random variable with density \\(f(x)\\). Let \\(g : \\mathbb{R} \\to \\mathbb{R}\\) be a function. The expected value of \\(g(X)\\), denoted \\(E[g(X)]\\) is given by  $$ E[g(X)] = \\int^{\\infty}_{ - \\infty} g(X)f_X(x)dx $$ whenever the above integral exists.</p>"},{"location":"STATS2/WEEK%205/Functions%20of%20Continuous%20Random%20Variables/#formulas","title":"Formulas","text":"<p>![[Pasted image 20230302183158.png]]</p>"},{"location":"STATS2/WEEK%206/Joint%20Continuous%20Random%20Variables/","title":"Joint Continuous Random Variables","text":"<p>A function \\(f(x,y)\\) is said to be a joint density function if  - \\(f(x,y) \\geq 0\\) , i.e. \\(f\\) is non-negative - \\(\\int \\int^{\\infty}_{ - \\infty} f(x,y)dxdy = 1\\) - Technical : \\(f(x,y)\\) is piecewise continuous in each random variable.</p> <p>support(X,Y) = \\(\\set{(x,y):f_{XY}(x,y) &gt; 0}\\) <pre><code>Let $X$ and $Y$ have joint density \n$$f_{XY} = \\begin{cases}\n1 &amp; 0 &lt; x &lt; 1 , 0 &lt; y &lt; 1\n\\\\\n0 &amp; otherwise\n\\end{cases}$$\n\nThis specific example is also known as uniform unit square.\n</code></pre> To get the probability find the area of the region using integration.</p>"},{"location":"STATS2/WEEK%206/Joint%20Continuous%20Random%20Variables/#2d-uniform-distribution","title":"2D Uniform Distribution","text":"<p>For some (reasonable) region D in \\(\\mathbb{R}^2\\) with total area \\(|D|\\). We say that \\((X,Y) \\sim D\\) if they have the joint density $$ f_{XY} = \\begin{cases} \\frac{1}{|D|} &amp; (x,y) \\in D \\ 0 &amp; otherwise \\end{cases} $$ For any sub region \\(A\\) of \\(D\\) , \\(P((X,Y) \\in A) = \\frac{|A|}{|D|} = \\frac{\\text{Area of A}}{\\text{Area of D}}\\)</p>"},{"location":"STATS2/WEEK%206/Joint%20Continuous%20Random%20Variables/#marginal-density","title":"Marginal Density","text":"<p>Suppose \\((X,Y)\\) have joint density \\(f_{XY}(x,y).\\) Then, - \\(X\\) has the marginal density \\(f_X(x)\\) = \\(\\int^{\\infty}_{y = - \\infty} f_{XY}(x,y)dy\\) - \\(Y\\) has the marginal density \\(f_Y(y)\\) = \\(\\int^{\\infty}_{x = - \\infty} f_{XY}(x,y)dy\\)</p> <p>\\(\\[D = \\{(x, y): [0, 2]\\times [0, -2] \\cup [-1, 0]\\times [0, 1]\\}\\]\\) </p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/","title":"Empirical Distribution and Descriptive Statistics","text":"<p>Let \\(X_1 , X_2 , X_3 .... , X_n \\sim X\\) be iid samples. Let #\\((X_i = t)\\) denote the number of times t occurs in the samples. The emperical distribution is the discrete distribution with PMF  $$ p(t) = \\frac{#(X_i = t)}{n} $$</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#sample-mean","title":"Sample Mean","text":"<p>Let \\(X_1 , X_2 , ...... X_n\\) be iid samples. The sample mean , denoted \\(\\overline{X}\\) , is defined to be the random variable.</p> \\[ \\overline{X} = \\frac{X_1 + X_2 + X_3 + ...... + X_n}{n} \\]"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#expectation","title":"Expectation","text":"<p>\\(E[\\overline{X}] = \\mu\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#variance","title":"Variance","text":"<p>\\(\\text{Var} \\overline{X} = \\frac{\\sigma ^2}{n}\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#sample-variance","title":"Sample Variance","text":"<p>Let \\(X_1 , X_2 , ...... X_n\\) be iid samples. The sample variance, denoted \\(S^2\\) , is defined to be the random variable.</p> <p>\\(\\(S^2 = \\frac{(X_1 - \\overline{X})^2 + (X_2 - \\overline{X})^2 + .... + (X_n - \\overline{X})^2}{n-1}\\)\\) where \\(\\overline{X}\\) is the sample mean.</p> <pre><code>As n increases , sample variance takes values close to distribution variance.\n</code></pre>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#expectation_1","title":"Expectation","text":"<p>Let \\(X_1 , X_2 , ...... X_n\\) be iid samples whose distribution has a finite variance \\(\\sigma^2\\).  The sample variance \\(S^2\\) has expected value given by  \\(\\(E[S^2]=\\sigma^2\\)\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#sample-proportion","title":"Sample Proportion","text":"<p>The sample proportion of \\(A\\) , denoted \\(S(A)\\) , is defined as $$ S(A) = \\frac{# (X_i \\text{for which A is true})}{n} $$</p> <pre><code>As $n$ increases , values of S(A) will be close to P(A).\n</code></pre>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#expectation_2","title":"Expectation","text":"<p>Let \\(X_1 , X_2 , ...... X_n\\) be iid samples whose distribution of \\(X\\). Let \\(A\\) be an event defined using \\(X\\) and let \\(P(A)\\) be the probability of \\(A\\). The sample proportion of \\(A\\) , denoted \\(S(A)\\) ,  has expected value given by $$ E[S(A)] = P(A) $$</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#variance_1","title":"Variance","text":"<p>Variance is given by  $$ Var(S(A)) = \\frac{P(A)(1 - P(A))}{n} $$</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#sum-of-independent-random-variables","title":"Sum of Independent Random Variables","text":"<p>Let \\(X_1 , X_2 , ...... X_n\\) be random variables. Let \\(S = X_1 + X_2 +..... + X_n\\) be their sum. Then, $$ E[S] = E[X_1] + E[X_2]+ ..... E[X_n] $$ If \\(X_1 , X_2 ... X_n\\) are pairwise uncorrelated, then \\(\\(Var(S) = Var(X_1) + Var(X_2) + .... Var(X_n)\\)\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#pairwise-uncorrelated","title":"Pairwise Uncorrelated","text":"\\[ E[X_i X_j] = E[X_i] E[X_j] \\]"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#weak-law-of-large-numbers","title":"Weak Law of Large Numbers","text":"<p>\\(\\(X_1 , X_2 ......X_n \\sim \\text{iid}X\\)\\) Let \\(\\mu = E[X]\\) , \\(\\sigma^2 = Var(X)\\) Sample Mean: \\(\\overline{X} =\\frac{X_1 + X_2 + ... X_n}{n}\\)     Expected value : \\(\\mu\\) , Variance : \\(\\frac{\\sigma^2}{n}\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#weak-law","title":"Weak Law","text":"\\[ P(|\\overline{X} - \\mu|&gt; \\delta) \\leq \\frac{\\sigma^2}{n \\delta^2} \\] <pre><code>- With Probability more than $1 - \\frac{\\sigma^2}{n \\delta^2}$ lies in $[\\mu - \\delta , \\mu + \\delta]$\n\n\n- $\\delta &gt; \\frac{\\sigma}{\\sqrt{n}}$\n</code></pre>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#bernoulli","title":"Bernoulli","text":"<p>For Bernoulli(p) samples  $$ 1 - \\frac{p(1-p)}{n\\delta^2}  $$ sample mean lies in  \\([p - \\delta , p + \\delta]\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#uniform","title":"Uniform","text":"<p>For Uniform{-M, .... M} samples $$ 1 - \\frac{M(M+1)}{3n\\delta^2} $$ sample mean lies in \\([-\\delta , \\delta]\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#normal","title":"Normal","text":"<p>For \\(Normal(0 , \\sigma^2)\\)  $$ 1 - \\frac{\\sigma^2}{n\\delta^2} $$ sample mean lies in \\([-\\delta , \\delta]\\)</p>"},{"location":"STATS2/WEEK%207/Empirical%20Distribution%20and%20Descriptive%20Statistics/#continuous-uniform","title":"Continuous Uniform","text":"<p>For \\(Uniform[-A , A]\\) samples  $$ 1 - \\frac{A^2}{3n\\delta^2} $$ sample mean lies in \\([-\\delta , \\delta]\\)</p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/","title":"Moment Generating Functions","text":"<p>Let \\(X\\) be a zero-mean random variable. The MGF of \\(X\\) , denoted \\(M_X(\\lambda)\\), is a function from \\(\\mathbb{R} \\to \\mathbb{R}\\) defined as  $$ M_X(\\lambda) = E[e^{\\lambda X}] $$ <pre><code>- **When $X$ is Discrete with PMF $f_X$**\n\n\nX takes the values $\\set{x_1 , x_2 , x_3 ....}$\n$$M_X(\\lambda) = f_X(x_1)e^{\\lambda x_1} + f_X(x_2)e^{\\lambda x_2} + ....$$\n$\\newline$\n\n- **When $X$ is continuous with PDF $f_X$ and support $T_X$**\n$$M_X(\\lambda) = \\int^{}_{x \\in T_X} f_X(x) e^{\\lambda x} dx$$\n</code></pre></p> <pre><code>$\\mathbf{X \\in \\set{\\overset{-1}{1/2} , \\overset{0}{1/4} , \\overset{2}{1/4}}}$\n\n$$M_X(\\lambda) = 0.5e^{-\\lambda} + 0.25 + 0.25 e^{2 \\lambda}$$\n\n$\\newline$\n\n\n$\\mathbf{M_X(\\lambda) = (1/3)e^{3 \\lambda / 2} + (1/6)e^{-3\\lambda} + (1/8)e^{-\\lambda} + (1/8)e^{\\lambda} + 1/4}$\n\n$$X \\sim \\set{\\overset{1/6}{-3} , \\overset{1/8}{-1} , \\overset{1/4}{0} , \\overset{1/8}{1} , \\overset{1/3}{3/2}}$$\n\n$\\newline$\n\n```ad-note \n$\\mathbf{X \\sim Normal(0 , \\sigma^2)}$\n$$M_X(\\lambda) = e^{\\lambda^2 \\sigma^2 / 2}$$\n```\n</code></pre>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#expectation-of-mgf","title":"Expectation Of MGF","text":"<p>\\(E[e^{\\lambda X}] = E[1 + \\lambda X + \\frac{\\lambda^2}{2!}X^2 + \\frac{\\lambda ^3}{3!}X^3 + ....]\\)</p> <p>\\(\\implies 1 + \\lambda E[X] + \\frac{\\lambda^2}{2!}E[X^2] + \\frac{\\lambda^3}{3!} E[X^3]\\)</p> <ul> <li>If \\(\\mathbf{X \\sim \\text{Normal}(0,\\sigma^2) , M_X(\\lambda) = e^{\\lambda^2 \\sigma^2 / 2}}\\) \\(1 + E[X] + \\frac{\\lambda^2}{2!}E[X^2] + \\frac{\\lambda ^3}{3!}E[X^3]\\) \\(\\implies 1 + \\frac{\\lambda^2}{2!}\\sigma^2 + \\frac{\\lambda^4}{4!}3\\sigma^4\\)</li> </ul> <p>\\(\\implies E[X]=0 , E[X^2] = \\sigma^2 , E[X^3] = 0 , E[X^4] = 3\\sigma^4 ...\\)</p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#mgf-of-sample-mean","title":"MGF of Sample Mean","text":"<p>Let \\(X_1 , X_2 .... X_n \\sim iid X , M_X(\\lambda) = \\frac{e^{\\lambda/2} + e^{-\\lambda/2}}{2}\\) - Sample Mean : \\(\\overline{X} = (X_1 + X_2 + ... X_n) / n\\) - \\(M_{X/n}(\\lambda) = \\frac{e^{\\lambda / 2n} + e^{-\\lambda / 2n}}{2}\\)</p> \\[ M_{\\overline{X}}(\\lambda) = {(\\frac{e^{\\frac{\\lambda}{2n}} + e^{\\frac{\\lambda}{2n}}}{2})}^n \\]"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#mgf-convergence-at-mathbf1-sqrtn-scaling","title":"MGF convergence at  \\(\\mathbf{1 / \\sqrt{n}}\\) scaling","text":"<p>Let \\(X_1 , X_2 .... X_n \\sim iid X , M_X(\\lambda) = \\frac{e^{\\lambda/2} + e^{-\\lambda/2}}{2}\\) \\(E[X]=0 , Var(X) = 1/4\\) Consider \\(Y = (X_1 + X_2 ... + X_n) /  \\sqrt{n}\\) \\(M_{X/\\sqrt{n}}(\\lambda) = \\frac{e^{\\lambda / 2\\sqrt{n}} + e^{-\\lambda / 2\\sqrt{n}}}{2}\\)</p> \\[ M_Y(\\lambda) = M_{\\overline{X}}(\\lambda) = {(\\frac{e^{\\frac{\\lambda}{2\\sqrt{n}}} + e^{\\frac{\\lambda}{2\\sqrt{n}}}}{2})}^n \\]"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#using-clt-to-approximate-probability","title":"Using CLT to approximate probability","text":"<p>\\(\\(X_1 , X_2 , .... X_n \\sim \\overset{iid}{X}\\)\\) Let \\(\\mu = E[X] , \\sigma^2 = Var(X)\\) \\(Y = X_1 + X_2 + .... X_n\\)</p> <p>What is \\(P(Y - n\\mu &gt; \\delta n \\mu)\\) ? </p> <p>$$ Z = \\frac{Y - n\\mu}{\\sqrt{n} \\sigma} \\approx \\text{Normal}(0,1) $$ \\(P(Y-n \\mu &gt; \\delta n \\mu) = P(\\frac{Y - n \\mu}{\\sqrt{n} \\sigma} &gt; \\frac{\\delta \\sqrt{n} \\mu}{\\sigma}) \\approx 1 - F(\\frac{\\delta \\sqrt{n} \\mu}{\\sigma})\\)</p> <p>\\(\\begin{align*} F_{Z}(0.2617) = 0.603, F_{Z}(1.6) = 0.9452, F_{Z}(1.5) = 0.933 \\end{align*}\\)</p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#types-of-distributions","title":"Types of Distributions","text":""},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#combination-of-independent-normals","title":"Combination of Independent Normals","text":"<p>Let \\(X_1 , X_2 ... X_n \\sim \\text{independent Normal}\\) Let \\(X_i \\sim \\text{Normal}(\\mu_i ,\\sigma_{i}^{2})\\) Suppose \\(Y = a_1X_1 + a_2X_2 + a_nX_n\\) be a linear combination of independent normals. </p> <p>Then, \\(\\(Y \\sim \\text{Normal}(\\mu , \\sigma^2)\\)\\) where \\(\\mu = E[Y] = a_1\\mu_1 + a_2\\mu_2 + ... + a_n\\mu_n\\) \\(\\sigma^2 = a_{1}^{2}\\sigma_{1}^{2} + a_{2}^{2}\\sigma_{2}^{2} + .... a_{n}^{2}\\sigma_{n}^{2}\\) Therefore Linear combinations of independent normals is normal.</p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#gamma-distribution","title":"Gamma Distribution","text":"<p>\\(X \\sim \\text{Gamma}(\\alpha , \\beta)\\) if PDF \\(f_X(s) \\propto x^{\\alpha -1} e^{-\\beta x} , x&gt;0\\)</p> <pre><code>title: Points to be noted\n- $\\alpha &gt; 0$ and $\\alpha$ is called the shape parameter\n- $\\beta &gt; 0$ and $\\alpha$ is called the rate parameter\n- $\\theta = 1/ \\beta$ and $\\theta$ is called the scale parameter.\n- Sum of n iid $\\text{Exp}(\\beta)$ is $\\text{Gamma}(n, \\beta)$\n- Square of $\\text{Normal}(0 , \\sigma^2)$ is $\\text{Gamma}(\\frac{1}{2} , \\frac{1}{2 \\sigma^2})$\n</code></pre> <p>Mean: \\(\\mathbf{\\frac{\\alpha}{\\beta}}\\) , Variance: \\(\\mathbf{\\frac{\\mathbf{\\alpha}}{\\beta^2}}\\) </p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#cauchy-distribution","title":"Cauchy Distribution","text":"<p>\\(X \\sim \\text{Cauchy}(0, \\alpha^2)\\) if PDF \\(f_X(x) = \\frac{1}{\\pi} \\frac{\\alpha}{\\alpha^2 + (x - \\theta)^2}\\) <pre><code>- $\\theta$ is the location parameter\n- $\\alpha &gt; 0$ and $\\alpha$ is called the scale parameter \n- Suppose $X , Y \\sim \\text{iid} Normal(0,\\sigma^2)$. Then,\n$$\\frac{X}{Y} \\sim \\text{Cauchy}(0,1)$$\n</code></pre> Mean : undefined , Variance : undefined </p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#beta-distribution","title":"Beta Distribution","text":"<p>\\(X \\sim \\text{Beta}(\\alpha , \\beta)\\) if PDF \\(f_X(x) \\propto x^{\\alpha -1}(1- x)^{\\beta -1} , 0 &lt; x &lt; 1\\)</p> <pre><code>- $\\alpha &gt; 0 , \\beta &gt; 0$ and both of them are the shape parameters\n- $\\text{Beta}(\\alpha ,1)$ has PDF $\\propto x^{\\alpha -1}$ which is called the power function distribution \n- Suppose $X \\sim \\text{Gamma}(\\alpha , 1 / \\theta), Y \\sim \\text{Gamma}(\\beta , 1/\\theta)$ , then \n\n$$\n\\frac{X}{X + Y} \\sim \\text{Beta}(\\alpha , \\beta)\n$$\n</code></pre>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#plotted-distributions","title":"Plotted Distributions","text":"<p>![[Pasted image 20230319210918.png]]</p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#sample-mean-distribution","title":"Sample Mean Distribution","text":"<p>\\(X_1 , X_2 , .... X_n \\sim \\text{iid Normal}(\\mu , \\sigma^2)\\) \\(\\overline{X} = \\frac{1}{n}X_1 + ... \\frac{1}{n}X_n\\)</p> <p>Sample mean is a linear combination of iid normal random variables  \\(\\(\\overline{X} \\sim \\text{Normal}(\\mu , \\sigma^2/n)\\)\\) Mean : \\(E[\\overline{X}] = \\mu\\) , Variance : \\(\\text{Var}(\\overline{X}) = \\sigma^2 /n\\)</p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#sum-of-squares-of-normal-samples","title":"Sum of squares of Normal Samples","text":"<p>\\(X_1 , X_2 , .... X_n \\sim \\text{iid Normal}(0 , \\sigma^2)\\) - \\(X_{i}^{2}\\) : \\(\\text{Gamma}(1/2 , 1/2\\sigma^2)\\) , independent - Sum of n independent \\(\\text{Gamma}(\\alpha , \\beta)\\) is \\(\\text{Gamma}(n\\alpha , \\beta)\\) $$ X_{1}^{2} + X_{2}^{2} + X_{3}^{2} + ... + X_{n}^{2} \\sim \\text{Gamma}(\\frac{n}{2} , \\frac{1}{2\\sigma^2}) $$</p>"},{"location":"STATS2/WEEK%208/Moment%20Generating%20Functions/#sample-mean-and-variance-of-normal-samples","title":"Sample Mean and variance of normal samples","text":"<p>Suppose \\(X_1 , X_2 , .... X_n \\sim \\text{iid Normal}(\\mu , \\sigma^2)\\). Then, - \\(\\overline{X} \\sim Normal(\\mu , \\sigma^2 /n)\\) - \\(\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi_{n-1}^{2}\\) , Chi-square with \\(n-1\\) degrees of freedom. - \\(\\overline{X}\\text{ and }S^2\\) are independent.</p>"},{"location":"STATS2/WEEK%209/Parameter%20Estimation/","title":"Parameter Estimation","text":"<p>It is a function defined on the samples taken. </p>"},{"location":"STATS2/WEEK%209/Parameter%20Estimation/#estimation-error","title":"Estimation Error","text":"<p>Let \\(\\theta\\) be the parameter and \\(\\hat{\\theta}\\) be the estimator Error: \\(\\hat{\\theta}(X_1 , X_2 .... X_n) - \\theta\\) is a random variable.</p>"},{"location":"STATS2/WEEK%209/Parameter%20Estimation/#bias","title":"Bias","text":"<p>Let \\(X_1 , X_2 , .... X_n \\sim \\text{iid} X\\) , parameter \\(\\theta\\) The bias of the estimator \\(\\hat{\\theta}\\) for a parameter \\(\\theta\\) , denoted as  \\(\\(\\text{Bias}(\\hat{\\theta} , \\theta) = \\text{E}(\\hat{\\theta}) - \\theta = \\text{Error}\\)\\) <pre><code>title:Unbiased Estimator \nWhen Bias/Error is 0 , then the estimator $\\hat{\\theta}$ is said to be unbiased estimator.\n</code></pre></p>"},{"location":"STATS2/WEEK%209/Parameter%20Estimation/#risk-squared-error","title":"Risk (Squared Error)","text":"<p>Let \\(X_1 , X_2 , .... X_n \\sim \\text{iid} X\\) , parameter \\(\\theta\\) The squared-error or risk of the estimator \\(\\hat{\\theta}\\) for a parameter \\(\\theta\\) , denoted  \\(\\(\\text{Risk}(\\hat{\\theta}, \\theta) = {E[(\\hat{\\theta} - \\theta)^2]}\\)\\) - Since \\(\\text{Error} = \\hat{\\theta} -\\theta\\) , risk is the expected value of squared error and is also called the mean squared error (MSE). - Squared-error risk is the second moment of Error </p>"},{"location":"STATS2/WEEK%209/Parameter%20Estimation/#variance","title":"Variance","text":"<p>Let \\(X_1 , X_2 , .... X_n \\sim \\text{iid} X\\) , parameter \\(\\theta\\) Variance of Estimator: \\(\\(Var(\\hat{\\theta}) = E[(\\hat{\\theta} - E[\\hat{\\theta}])^2]\\)\\) also variance of error is equal to variance of estimator i.e. \\(Var(\\hat{\\theta}) = Var(Error)\\)</p>"},{"location":"STATS2/WEEK%209/Parameter%20Estimation/#bias-variance-tradeoff","title":"Bias Variance Tradeoff","text":"<p>Let \\(X_1 , X_2 , .... X_n \\sim \\text{iid} X\\) , parameter \\(\\theta\\) \\(\\(\\text{Risk}(\\hat{\\theta} , \\theta) = \\text{Bias}(\\hat{\\theta} , \\theta)^2 + Var(\\hat{\\theta})\\)\\) \\(\\(\\text{Risk} = \\text{E}[\\text{Error}]^2 = \\text{Mean}[\\text{Error}]^2 + \\text{Var}[\\text{Error}]\\)\\)</p>"},{"location":"STATS2/WEEK%209/Parameter%20Estimation/#sample-moments","title":"Sample Moments","text":"\\[X_1 , X_2 , ... X_n \\sim \\text{iid} \\ X$$ **Sample Moments:** $$M_k(X_1 , X_2 ...., X_n) = \\frac{1}{n}\\sum^{n}_{i=1}X_{i}^{k}\\]"}]}